{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22804297-eafe-4e53-84d8-7b838c2063d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "def analyze_real_images(image_folder):\n",
    "    \"\"\"Analyze the characteristics of your real laser-matter images\"\"\"\n",
    "    image_paths = sorted(Path(image_folder).glob(\"*.tif\"))  \n",
    "    images = []\n",
    "    \n",
    "    for path in image_paths:\n",
    "        img = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)  # Or RGB if needed\n",
    "        images.append(img)\n",
    "    \n",
    "    images = np.array(images)  # Shape: (n_time, height, width)\n",
    "    \n",
    "    print(f\"Total images: {len(images)}\")\n",
    "    print(f\"Image shape: {images[0].shape}\")\n",
    "    print(f\"Time dimension: {len(images)}\")\n",
    "    \n",
    "    # Analyze statistics\n",
    "    print(\"\\n--- Statistics ---\")\n",
    "    print(f\"Min intensity: {images.min()}\")\n",
    "    print(f\"Max intensity: {images.max()}\")\n",
    "    print(f\"Mean intensity: {images.mean():.2f}\")\n",
    "    print(f\"Std intensity: {images.std():.2f}\")\n",
    "    \n",
    "    # Check if images are registered\n",
    "    print(\"\\n--- Registration Check ---\")\n",
    "    diff_means = []\n",
    "    for i in range(len(images)-1):\n",
    "        diff = np.abs(images[i+1] - images[i])\n",
    "        diff_means.append(diff.mean())\n",
    "    \n",
    "    print(f\"Mean absolute difference between frames: {np.mean(diff_means):.2f}\")\n",
    "\n",
    "    \n",
    "    # Visualize sample frames\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < len(images):\n",
    "            im = ax.imshow(images[i],  cmap='gray')\n",
    "            ax.set_title(f'Frame {i}')\n",
    "            plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Visualize differences\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axes[0].imshow(images[31],  cmap='gray')\n",
    "    axes[0].set_title('Frame 31')\n",
    "    axes[1].imshow(images[32],  cmap='gray')\n",
    "    axes[1].set_title('Frame 32')\n",
    "    axes[2].imshow(np.abs(images[32] - images[31]),  cmap='gray')\n",
    "    axes[2].set_title('Absolute Difference')\n",
    "    \n",
    "    return images\n",
    "\n",
    "# Run analysis'\n",
    "images = analyze_real_images(\"C:/Users/Sinjini/Documents/Semester 3/MLDM Project/Real_Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe7dd2e-f7c9-4c0c-b3d7-bebb30dab6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_mse = []\n",
    "for i in range(len(images)-1):\n",
    "    diff = images[i+1].astype(np.float32) - images[i].astype(np.float32)\n",
    "    diff_mse.append(np.mean(diff**2))\n",
    "print(\"Mean squared difference between frames:\", np.mean(diff_mse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d95524-91ef-4974-be85-c3ff00ac9850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# images already loaded: shape (T, H, W), uint8 or similar\n",
    "\n",
    "ref = images[0].astype(np.uint8)   # reference frame\n",
    "mean_disp = []\n",
    "\n",
    "for i in range(1, len(images)):\n",
    "    cur = images[i].astype(np.uint8)\n",
    "\n",
    "    flow = cv2.calcOpticalFlowFarneback(\n",
    "        ref, cur, None,\n",
    "        0.5, 3, 15, 3, 5, 1.2, 0\n",
    "    )\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    mean_disp.append(mag.mean())\n",
    "\n",
    "frames = np.arange(1, len(images))\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(frames, mean_disp, marker='o')\n",
    "plt.xlabel(\"Frame index\")\n",
    "plt.ylabel(\"Mean displacement from frame 0 (pixels)\")\n",
    "plt.title(\"Average displacement vs frame\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73d4505-d8fa-4827-8a47-e3c788398b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "def analyze_and_register(image_folder):\n",
    "    # 1. Load Images\n",
    "    image_paths = sorted(Path(image_folder).glob(\"*.tif\"))\n",
    "    images = [cv2.imread(str(p), cv2.IMREAD_GRAYSCALE) for p in image_paths]\n",
    "    images = np.array(images)\n",
    "    \n",
    "    # 2. Setup Reference (Frame 0)\n",
    "    ref_img = images[0]\n",
    "    height, width = ref_img.shape\n",
    "    \n",
    "    # Initialize storage for motion data\n",
    "    translations_x = [0]\n",
    "    translations_y = [0]\n",
    "    rotations = [0] # In degrees\n",
    "    \n",
    "    registered_images = [ref_img] # List to store corrected images\n",
    "    \n",
    "    # Initialize ORB detector (Robust feature detection)\n",
    "    orb = cv2.ORB_create(nfeatures=2000)\n",
    "    \n",
    "    # Find keypoints in the clean Frame 0\n",
    "    kp1, des1 = orb.detectAndCompute(ref_img, None)\n",
    "    \n",
    "    print(f\"Processing {len(images)} frames...\")\n",
    "    \n",
    "    # 3. Loop through frames to Calculate & Correct\n",
    "    for i in range(1, len(images)):\n",
    "        curr_img = images[i]\n",
    "        \n",
    "        # Detect features in current frame\n",
    "        kp2, des2 = orb.detectAndCompute(curr_img, None)\n",
    "        \n",
    "        # Match features\n",
    "        matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "        matches = matcher.match(des1, des2)\n",
    "        \n",
    "        # Extract location of good matches\n",
    "        src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "        \n",
    "        # --- DIAGNOSIS STEP ---\n",
    "        # Estimate a \"Partial Affine\" transform (Rotation + Translation + Scale)\n",
    "        # RANSAC will ignore the chaotic laser pixels\n",
    "        matrix, inliers = cv2.estimateAffinePartial2D(dst_pts, src_pts, method=cv2.RANSAC)\n",
    "        \n",
    "        if matrix is None:\n",
    "            # Fallback if detection fails (rare): assume no movement\n",
    "            matrix = np.array([[1, 0, 0], [0, 1, 0]], dtype=np.float32)\n",
    "            \n",
    "        # Extract components for analysis\n",
    "        # Matrix is [[cos, -sin, tx], [sin, cos, ty]]\n",
    "        tx = matrix[0, 2]\n",
    "        ty = matrix[1, 2]\n",
    "        \n",
    "        # Calculate rotation angle from cosine/sine\n",
    "        # arctan2(sin, cos)\n",
    "        angle_rad = np.arctan2(matrix[1, 0], matrix[0, 0])\n",
    "        angle_deg = np.degrees(angle_rad)\n",
    "        \n",
    "        translations_x.append(tx)\n",
    "        translations_y.append(ty)\n",
    "        rotations.append(angle_deg)\n",
    "        \n",
    "        # --- CORRECTION STEP ---\n",
    "        # Apply the matrix to warp the current image back to match Frame 0\n",
    "        #registered_img = cv2.warpAffine(curr_img, matrix, (width, height))\n",
    "        #registered_images.append(registered_img)\n",
    "\n",
    "    # 4. Visualization of the Diagnosis\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
    "    \n",
    "    # Plot Translation\n",
    "    ax1.plot(translations_x, label='X Shift (pixels)', marker='.')\n",
    "    ax1.plot(translations_y, label='Y Shift (pixels)', marker='.')\n",
    "    ax1.set_title(\"Diagnosis: Detected Translation\")\n",
    "    ax1.set_ylabel(\"Pixels\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot Rotation\n",
    "    ax2.plot(rotations, color='r', marker='.')\n",
    "    ax2.set_title(\"Diagnosis: Detected Rotation\")\n",
    "    ax2.set_ylabel(\"Degrees\")\n",
    "    ax2.set_xlabel(\"Frame Index\")\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    #return np.array(registered_images)\n",
    "\n",
    "# --- Run the Code ---\n",
    "# Replace with your actual path\n",
    "folder_path = \"C:/Users/Sinjini/Documents/Semester 3/MLDM Project/Real_Images\"\n",
    "#clean_images = analyze_and_register(folder_path)\n",
    "analyze_and_register(folder_path)\n",
    "#print(f\"Registered data shape: {clean_images.shape}\")\n",
    "\n",
    "'''\n",
    "# Optional: Visualize Before vs After for Frame 15 (or any active frame)\n",
    "#import matplotlib.pyplot as plt\n",
    "raw_images = np.array([cv2.imread(str(p), cv2.IMREAD_GRAYSCALE) for p in sorted(Path(folder_path).glob(\"*.tif\"))])\n",
    "\n",
    "\n",
    "frame_idx = 37 # pick a frame where the movement is high\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(raw_images[frame_idx], cmap='gray')\n",
    "plt.title(f\"Original Frame {frame_idx}\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(clean_images[frame_idx], cmap='gray')\n",
    "plt.title(f\"Registered Frame {frame_idx}\")\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c86993-ed43-40d1-8595-93949f1236a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Assuming 'images' is your (51, H, W) numpy array of grayscale images\n",
    "# We will check Frame 15 (arbitrary choice)\n",
    "frame_idx = 0\n",
    "raw_frame = images[frame_idx]\n",
    "\n",
    "# 1. Function to calculate 2nd spatial derivative (u_xx) using finite difference\n",
    "def calculate_uxx(img):\n",
    "    # The u_xx kernel: [1, -2, 1]\n",
    "    uxx = cv2.filter2D(img.astype(np.float32), -1, np.array([[1, -2, 1]], dtype=np.float32))\n",
    "    return uxx\n",
    "\n",
    "# --- RAW DATA DIAGNOSIS ---\n",
    "raw_uxx = calculate_uxx(raw_frame)\n",
    "raw_uxx_variance = np.var(raw_uxx)\n",
    "print(f\"RAW u_xx Variance: {raw_uxx_variance:.2f}\")\n",
    "\n",
    "# --- SMOOTHED DATA DIAGNOSIS ---\n",
    "# Apply a mild Gaussian filter (simulating denoising)\n",
    "smoothed_frame = gaussian_filter(raw_frame, sigma=1)\n",
    "smoothed_uxx = calculate_uxx(smoothed_frame)\n",
    "smoothed_uxx_variance = np.var(smoothed_uxx)\n",
    "print(f\"SMOOTHED u_xx Variance: {smoothed_uxx_variance:.2f}\")\n",
    "\n",
    "# --- VISUAL PROOF ---\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Display the magnitude of the derivatives\n",
    "axes[0].imshow(np.abs(raw_uxx), cmap='hot', vmax=np.percentile(np.abs(raw_uxx), 98))\n",
    "axes[0].set_title(f'Raw |u_xx| (Variance: {raw_uxx_variance:.0f})')\n",
    "axes[1].imshow(np.abs(smoothed_uxx), cmap='hot', vmax=np.percentile(np.abs(smoothed_uxx), 98))\n",
    "axes[1].set_title(f'Smoothed |u_xx| (Variance: {smoothed_uxx_variance:.0f})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e2e7ac-d682-4b3e-9778-64cb735710fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# 'images' is your loaded (51, H, W) numpy array\n",
    "\n",
    "for i in range(len(images)):\n",
    "    frame_i = images[i]\n",
    "    \n",
    "    # 1. Estimate the pure noise component.\n",
    "    # A simple way to estimate noise is by subtracting a heavily blurred version \n",
    "    # from the original image. The difference should isolate the high-frequency noise.\n",
    "    blurred_frame = cv2.GaussianBlur(frame_i, (5, 5), 0)\n",
    "    noise_estimate = frame_i.astype(np.float32) - blurred_frame.astype(np.float32)\n",
    "    \n",
    "    # 2. Flatten and normalize the noise array\n",
    "    noise_flat = noise_estimate.flatten()\n",
    "    \n",
    "    # 3. Plot the histogram of the noise\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.hist(noise_flat, bins=50, density=True, color='gray', alpha=0.7)\n",
    "    \n",
    "    # Fit a Gaussian curve for visual comparison\n",
    "    from scipy.stats import norm\n",
    "    xmin, xmax = plt.xlim()\n",
    "    x = np.linspace(xmin, xmax, 100)\n",
    "    mu, std = norm.fit(noise_flat)\n",
    "    p = norm.pdf(x, mu, std)\n",
    "    \n",
    "    plt.plot(x, p, 'r', linewidth=2, label=f'Gaussian Fit (μ={mu:.2f}, σ={std:.2f})')\n",
    "    plt.title(f\"Histogram of Estimated Noise in Frame {i}\")\n",
    "    plt.xlabel(\"Pixel Value Residual (Noise)\")\n",
    "    plt.ylabel(\"Frequency (Normalized)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a44a5c-da9d-488a-a15a-8612596f59ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "def poisson_check(images, frame_index=15, block_size=10):\n",
    "    \"\"\"\n",
    "    Performs the Variance vs. Mean analysis to check for Poisson noise.\n",
    "    \n",
    "    Args:\n",
    "        images (np.ndarray): The (T, H, W) stack of grayscale images.\n",
    "        frame_index (int): Index of the brightest frame to analyze (e.g., 40).\n",
    "        block_size (int): Size of the square block to analyze (e.g., 10x10 pixels).\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Use an active, bright frame to capture potential shot noise\n",
    "    active_frame = images[frame_index].astype(np.float32)\n",
    "    \n",
    "    # 2. Initialize lists to store mean and variance of each block\n",
    "    means = []\n",
    "    variances = []\n",
    "    \n",
    "    height, width = active_frame.shape\n",
    "    \n",
    "    # 3. Loop through the image in non-overlapping blocks\n",
    "    for y in range(0, height, block_size):\n",
    "        for x in range(0, width, block_size):\n",
    "            # Define the block\n",
    "            block = active_frame[y:y+block_size, x:x+block_size]\n",
    "            \n",
    "            # Ensure the block is full size (avoids edge effects if image size isn't divisible)\n",
    "            if block.shape[0] == block_size and block.shape[1] == block_size:\n",
    "                \n",
    "                # Calculate mean and variance for the block\n",
    "                block_mean = np.mean(block)\n",
    "                block_variance = np.var(block)\n",
    "                \n",
    "                means.append(block_mean)\n",
    "                variances.append(block_variance)\n",
    "\n",
    "    # 4. Create the Diagnostic Plot\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.scatter(means, variances, s=15, alpha=0.6, color='blue')\n",
    "    \n",
    "    # Calculate and plot a linear trend line for easy visualization\n",
    "    coefficients = np.polyfit(means, variances, 1)\n",
    "    poly_fn = np.poly1d(coefficients)\n",
    "    plt.plot(means, poly_fn(means), \"r-\", \n",
    "             label=f'Linear Fit (Slope: {coefficients[0]:.3f})')\n",
    "    \n",
    "    plt.title(f\"Noise Variance vs. Signal Mean (Frame {frame_index})\")\n",
    "    plt.xlabel(\"Local Mean Intensity (Signal)\")\n",
    "    plt.ylabel(\"Local Variance (Noise Power)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return coefficients[0] # Return the slope of the fit\n",
    "\n",
    "# --- EXECUTE THE CHECK ---\n",
    "\n",
    "# You need to run this command:\n",
    "slope = poisson_check(images) \n",
    "print(f\"Slope of the fit: {slope:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a9c37e-b65a-4216-9e6d-ebe4c8c52aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559b4ffb-5ee6-4046-869b-26ee81541663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2613b2be-1a7e-41d6-97fe-bddaf54696bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.restoration import denoise_tv_chambolle\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_images_corrected(images_raw):\n",
    "\n",
    "    #CORRECTED denoising pipeline with gentler TV weight.\n",
    "    \n",
    "    T, H, W = images_raw.shape\n",
    "    images_clean = np.zeros_like(images_raw, dtype=np.float32)\n",
    "    \n",
    "    for t in range(T):\n",
    "        img = images_raw[t].astype(np.float32)\n",
    "        \n",
    "        # Step 1: Anscombe transform\n",
    "        img_anscombe = 2.0 * np.sqrt(img + 3.0/8.0)\n",
    "        \n",
    "        # Step 2: MUCH GENTLER TV denoising\n",
    "        # Try these weights in order: 0.01, 0.005, 0.001\n",
    "        img_denoised = denoise_tv_chambolle(img_anscombe, weight=0.01)\n",
    "        \n",
    "        # Step 3: Inverse Anscombe\n",
    "        img_clean = (img_denoised / 2.0)**2 - 3.0/8.0\n",
    "        img_clean = np.clip(img_clean, 0, 255)\n",
    "        \n",
    "        images_clean[t] = img_clean\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    #images_clean = (images_clean - images_clean.min()) / (images_clean.max() - images_clean.min())\n",
    "    \n",
    "    return images_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32f9f54-dc83-4d62-800e-b581e200eabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images_gaussian(images_raw):\n",
    "    T, H, W = images_raw.shape\n",
    "    images_clean = np.zeros_like(images_raw, dtype=np.float32)\n",
    "    \n",
    "    for t in range(T):\n",
    "        img = images_raw[t].astype(np.float32)\n",
    "        \n",
    "        # Anscombe\n",
    "        img_anscombe = 2.0 * np.sqrt(img + 3.0/8.0)\n",
    "        \n",
    "        # Gaussian smoothing (no TV)\n",
    "        img_denoised = gaussian_filter(img_anscombe, sigma=1.0)\n",
    "        \n",
    "        # Inverse Anscombe\n",
    "        img_clean = (img_denoised / 2.0)**2 - 3.0/8.0\n",
    "        img_clean = np.clip(img_clean, 0, 255)\n",
    "        \n",
    "        images_clean[t] = img_clean\n",
    "    \n",
    "    #images_clean = (images_clean - images_clean.min()) / (images_clean.max() - images_clean.min())\n",
    "    return images_clean\n",
    "\n",
    "\n",
    "images_clean = preprocess_images_gaussian(images)\n",
    "\n",
    "#images_clean = preprocess_images_corrected(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cd54da-775d-41e8-97e7-b350df717727",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(images_clean[0:2])):\n",
    "    frame_i = images_clean[i]\n",
    "    \n",
    "    # 1. Estimate the pure noise component.\n",
    "    # A simple way to estimate noise is by subtracting a heavily blurred version \n",
    "    # from the original image. The difference should isolate the high-frequency noise.\n",
    "    blurred_frame = cv2.GaussianBlur(frame_i, (5, 5), 0)\n",
    "    noise_estimate = frame_i.astype(np.float32) - blurred_frame.astype(np.float32)\n",
    "    \n",
    "    # 2. Flatten and normalize the noise array\n",
    "    noise_flat = noise_estimate.flatten()\n",
    "    \n",
    "    # 3. Plot the histogram of the noise\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.hist(noise_flat, bins=50, density=True, color='gray', alpha=0.7)\n",
    "    \n",
    "    # Fit a Gaussian curve for visual comparison\n",
    "    from scipy.stats import norm\n",
    "    xmin, xmax = plt.xlim()\n",
    "    x = np.linspace(xmin, xmax, 100)\n",
    "    mu, std = norm.fit(noise_flat)\n",
    "    p = norm.pdf(x, mu, std)\n",
    "    \n",
    "    plt.plot(x, p, 'r', linewidth=2, label=f'Gaussian Fit (μ={mu:.2f}, σ={std:.2f})')\n",
    "    plt.title(f\"Histogram of Estimated Noise in Frame {i}\")\n",
    "    plt.xlabel(\"Pixel Value Residual (Noise)\")\n",
    "    plt.ylabel(\"Frequency (Normalized)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d15e63-4c0f-4b4b-8860-bb69d3bff312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_check(images, frame_index=15, block_size=10):\n",
    "    \"\"\"\n",
    "    Performs the Variance vs. Mean analysis to check for Poisson noise.\n",
    "    \n",
    "    Args:\n",
    "        images (np.ndarray): The (T, H, W) stack of grayscale images.\n",
    "        frame_index (int): Index of the brightest frame to analyze (e.g., 40).\n",
    "        block_size (int): Size of the square block to analyze (e.g., 10x10 pixels).\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Use an active, bright frame to capture potential shot noise\n",
    "    active_frame = images[frame_index].astype(np.float32)\n",
    "    \n",
    "    # 2. Initialize lists to store mean and variance of each block\n",
    "    means = []\n",
    "    variances = []\n",
    "    \n",
    "    height, width = active_frame.shape\n",
    "    \n",
    "    # 3. Loop through the image in non-overlapping blocks\n",
    "    for y in range(0, height, block_size):\n",
    "        for x in range(0, width, block_size):\n",
    "            # Define the block\n",
    "            block = active_frame[y:y+block_size, x:x+block_size]\n",
    "            \n",
    "            # Ensure the block is full size (avoids edge effects if image size isn't divisible)\n",
    "            if block.shape[0] == block_size and block.shape[1] == block_size:\n",
    "                \n",
    "                # Calculate mean and variance for the block\n",
    "                block_mean = np.mean(block)\n",
    "                block_variance = np.var(block)\n",
    "                \n",
    "                means.append(block_mean)\n",
    "                variances.append(block_variance)\n",
    "\n",
    "    # 4. Create the Diagnostic Plot\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.scatter(means, variances, s=15, alpha=0.6, color='blue')\n",
    "    \n",
    "    # Calculate and plot a linear trend line for easy visualization\n",
    "    coefficients = np.polyfit(means, variances, 1)\n",
    "    poly_fn = np.poly1d(coefficients)\n",
    "    plt.plot(means, poly_fn(means), \"r-\", \n",
    "             label=f'Linear Fit (Slope: {coefficients[0]:.3f})')\n",
    "    \n",
    "    plt.title(f\"Noise Variance vs. Signal Mean (Frame {frame_index})\")\n",
    "    plt.xlabel(\"Local Mean Intensity (Signal)\")\n",
    "    plt.ylabel(\"Local Variance (Noise Power)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return coefficients[0] # Return the slope of the fit\n",
    "\n",
    "# --- EXECUTE THE CHECK ---\n",
    "\n",
    "# You need to run this command:\n",
    "slope = poisson_check(images_clean) \n",
    "print(f\"Slope of the fit: {slope:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ba8708-739e-4539-b226-5eb4b39b1e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Assuming 'images' is your (51, H, W) numpy array of grayscale images\n",
    "# We will check Frame 15 (arbitrary choice)\n",
    "frame_idx = 45\n",
    "raw_frame = images_clean[frame_idx]\n",
    "\n",
    "# 1. Function to calculate 2nd spatial derivative (u_xx) using finite difference\n",
    "def calculate_uxx(img):\n",
    "    # The u_xx kernel: [1, -2, 1]\n",
    "    uxx = cv2.filter2D(img.astype(np.float32), -1, np.array([[1, -2, 1]], dtype=np.float32))\n",
    "    return uxx\n",
    "\n",
    "# --- RAW DATA DIAGNOSIS ---\n",
    "raw_uxx = calculate_uxx(raw_frame)\n",
    "raw_uxx_variance = np.var(raw_uxx)\n",
    "print(f\"RAW u_xx Variance: {raw_uxx_variance:.2f}\")\n",
    "\n",
    "# --- SMOOTHED DATA DIAGNOSIS ---\n",
    "# Apply a mild Gaussian filter (simulating denoising)\n",
    "smoothed_frame = gaussian_filter(raw_frame, sigma=1)\n",
    "smoothed_uxx = calculate_uxx(smoothed_frame)\n",
    "smoothed_uxx_variance = np.var(smoothed_uxx)\n",
    "print(f\"SMOOTHED u_xx Variance: {smoothed_uxx_variance:.2f}\")\n",
    "\n",
    "# --- VISUAL PROOF ---\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Display the magnitude of the derivatives\n",
    "axes[0].imshow(np.abs(raw_uxx), cmap='hot', vmax=np.percentile(np.abs(raw_uxx), 98))\n",
    "axes[0].set_title(f'Raw |u_xx| (Variance: {raw_uxx_variance:.0f})')\n",
    "axes[1].imshow(np.abs(smoothed_uxx), cmap='hot', vmax=np.percentile(np.abs(smoothed_uxx), 98))\n",
    "axes[1].set_title(f'Smoothed |u_xx| (Variance: {smoothed_uxx_variance:.0f})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05e33e5-7f83-4c22-9eca-ad3a2559e579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display frames 0, 25, 50 side by side\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Raw images\n",
    "axes[0, 0].imshow(images[0], cmap='gray')\n",
    "axes[0, 0].set_title('Raw Frame 0')\n",
    "axes[0, 1].imshow(images[25], cmap='gray')\n",
    "axes[0, 1].set_title('Raw Frame 25')\n",
    "axes[0, 2].imshow(images[50], cmap='gray')\n",
    "axes[0, 2].set_title('Raw Frame 50')\n",
    "\n",
    "# Clean images\n",
    "axes[1, 0].imshow(images_clean[0], cmap='gray')\n",
    "axes[1, 0].set_title('Clean Frame 0')\n",
    "axes[1, 1].imshow(images_clean[25], cmap='gray')\n",
    "axes[1, 1].set_title('Clean Frame 25')\n",
    "axes[1, 2].imshow(images_clean[50], cmap='gray')\n",
    "axes[1, 2].set_title('Clean Frame 50')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('visual_check.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"Raw images:\")\n",
    "print(f\"  Frame 0:  mean={images[0].mean():.2f}, std={images[0].std():.2f}\")\n",
    "print(f\"  Frame 25: mean={images[25].mean():.2f}, std={images[25].std():.2f}\")\n",
    "print(f\"  Frame 50: mean={images[50].mean():.2f}, std={images[50].std():.2f}\")\n",
    "\n",
    "print(\"\\nCleaned images:\")\n",
    "print(f\"  Frame 0:  mean={images_clean[0].mean():.2f}, std={images_clean[0].std():.2f}\")\n",
    "print(f\"  Frame 25: mean={images_clean[25].mean():.2f}, std={images_clean[25].std():.2f}\")\n",
    "print(f\"  Frame 50: mean={images_clean[50].mean():.2f}, std={images_clean[50].std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c78c7a9-f40f-4698-acb6-93768ffcb6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.restoration import denoise_tv_chambolle\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_images_optimized_tv(images_raw):\n",
    "    \"\"\"\n",
    "    Optimized denoising pipeline running TV filter on the Anscombe-transformed data.\n",
    "    The weight is increased to effectively suppress the noise in the transformed domain.\n",
    "    \"\"\"\n",
    "    \n",
    "    T, H, W = images_raw.shape\n",
    "    images_clean = np.zeros_like(images_raw, dtype=np.float32)\n",
    "    \n",
    "    # We must operate on float data\n",
    "    images_raw = images_raw.astype(np.float32)\n",
    "    \n",
    "    # --- Recommended weights to try in order: 0.15, 0.20, 0.25 ---\n",
    "    # We start with 0.15, which is a good balance for typical image processing tasks.\n",
    "    TV_WEIGHT = 0.15 \n",
    "    \n",
    "    print(f\"Applying Anscombe + TV Denoising with Weight={TV_WEIGHT}...\")\n",
    "\n",
    "    for t in range(T):\n",
    "        img = images_raw[t]\n",
    "        \n",
    "        # Step 1: Anscombe transform (Variance Stabilization)\n",
    "        # Transformed data is roughly in the range [1, 35]\n",
    "        img_anscombe = 2.0 * np.sqrt(img + 3.0/8.0)\n",
    "        \n",
    "        # Step 2: TV denoising \n",
    "        # The weight is now strong enough to filter noise in the [1, 35] domain.\n",
    "        img_denoised = denoise_tv_chambolle(\n",
    "            img_anscombe, \n",
    "            weight=TV_WEIGHT, \n",
    "            #multichannel=False\n",
    "        )\n",
    "        \n",
    "        # Step 3: Inverse Anscombe (Scale Restoration)\n",
    "        img_clean = (img_denoised / 2.0)**2 - 3.0/8.0\n",
    "        img_clean = np.clip(img_clean, 0, 255)\n",
    "        \n",
    "        images_clean[t] = img_clean\n",
    "        \n",
    "    # DO NOT NORMALIZE HERE. The data must remain in its physical [0, 255] range \n",
    "    # for the derivatives to be physically meaningful.\n",
    "    print(\"Denoising complete. Data is in [0, 255] range.\")\n",
    "    return images_clean\n",
    "\n",
    "\n",
    "images_more_clean = preprocess_images_optimized_tv(images_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de5896e-b243-4880-bab0-b73ed26470de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "\n",
    "frame_idx = 0\n",
    "raw_frame = images_more_clean[frame_idx]\n",
    "\n",
    "# 1. Function to calculate 2nd spatial derivative (u_xx) using finite difference\n",
    "def calculate_uxx(img):\n",
    "    # The u_xx kernel: [1, -2, 1]\n",
    "    uxx = cv2.filter2D(img.astype(np.float32), -1, np.array([[1, -2, 1]], dtype=np.float32))\n",
    "    return uxx\n",
    "\n",
    "# --- RAW DATA DIAGNOSIS ---\n",
    "raw_uxx = calculate_uxx(raw_frame)\n",
    "raw_uxx_variance = np.var(raw_uxx)\n",
    "print(f\"RAW u_xx Variance: {raw_uxx_variance:.2f}\")\n",
    "\n",
    "# --- SMOOTHED DATA DIAGNOSIS ---\n",
    "# Apply a mild Gaussian filter (simulating denoising)\n",
    "smoothed_frame = gaussian_filter(raw_frame, sigma=1)\n",
    "smoothed_uxx = calculate_uxx(smoothed_frame)\n",
    "smoothed_uxx_variance = np.var(smoothed_uxx)\n",
    "print(f\"SMOOTHED u_xx Variance: {smoothed_uxx_variance:.2f}\")\n",
    "\n",
    "# --- VISUAL PROOF ---\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Display the magnitude of the derivatives\n",
    "axes[0].imshow(np.abs(raw_uxx), cmap='hot', vmax=np.percentile(np.abs(raw_uxx), 98))\n",
    "axes[0].set_title(f'Raw |u_xx| (Variance: {raw_uxx_variance:.0f})')\n",
    "axes[1].imshow(np.abs(smoothed_uxx), cmap='hot', vmax=np.percentile(np.abs(smoothed_uxx), 98))\n",
    "axes[1].set_title(f'Smoothed |u_xx| (Variance: {smoothed_uxx_variance:.0f})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89fe9ab-cd79-445f-a786-3dc324326945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_derivatives(images_clean, dx=1.0, dy=1.0, dt=1.0):\n",
    "    \"\"\"\n",
    "    Compute spatial and temporal derivatives for SINDy.\n",
    "    \n",
    "    Args:\n",
    "        images_clean: (T, H, W) denoised image stack in [0, 255]\n",
    "        dx, dy: spatial grid spacing (use 1.0 if unknown)\n",
    "        dt: time step between frames (use 1.0 if unknown, can rescale later)\n",
    "    \n",
    "    Returns:\n",
    "        u_t, u_x, u_y, u_xx, u_yy: derivative arrays\n",
    "    \"\"\"\n",
    "    T, H, W = images_clean.shape\n",
    "    \n",
    "    # 1. Temporal derivative (∂u/∂t) - forward difference\n",
    "    u_t = np.zeros_like(images_clean)\n",
    "    u_t[:-1] = (images_clean[1:] - images_clean[:-1]) / dt\n",
    "    # Note: last frame has u_t = 0 (can't compute forward difference)\n",
    "    \n",
    "    # 2. First spatial derivatives (∂u/∂x, ∂u/∂y) - central difference\n",
    "    u_x = np.zeros_like(images_clean)\n",
    "    u_y = np.zeros_like(images_clean)\n",
    "    \n",
    "    u_x[:, :, 1:-1] = (images_clean[:, :, 2:] - images_clean[:, :, :-2]) / (2*dx)\n",
    "    u_y[:, 1:-1, :] = (images_clean[:, 2:, :] - images_clean[:, :-2, :]) / (2*dy)\n",
    "    \n",
    "    # 3. Second spatial derivatives (∂²u/∂x², ∂²u/∂y²) - central difference\n",
    "    u_xx = np.zeros_like(images_clean)\n",
    "    u_yy = np.zeros_like(images_clean)\n",
    "    \n",
    "    u_xx[:, :, 1:-1] = (images_clean[:, :, 2:] - 2*images_clean[:, :, 1:-1] + \n",
    "                         images_clean[:, :, :-2]) / (dx**2)\n",
    "    u_yy[:, 1:-1, :] = (images_clean[:, 2:, :] - 2*images_clean[:, 1:-1, :] + \n",
    "                         images_clean[:, :-2, :]) / (dy**2)\n",
    "    \n",
    "    return u_t, u_x, u_y, u_xx, u_yy\n",
    "\n",
    "\n",
    "# Compute derivatives\n",
    "print(\"=\"*60)\n",
    "print(\"COMPUTING DERIVATIVES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "u_t, u_x, u_y, u_xx, u_yy = compute_derivatives(\n",
    "    images_more_clean, \n",
    "    dx=1.0,  # pixel spacing (or physical spacing if known)\n",
    "    dy=1.0, \n",
    "    dt=1.0   # time step (or physical time if known)\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Derivatives computed successfully!\")\n",
    "print(f\"\\nShapes:\")\n",
    "print(f\"  u_t:  {u_t.shape}\")\n",
    "print(f\"  u_x:  {u_x.shape}\")\n",
    "print(f\"  u_xx: {u_xx.shape}\")\n",
    "\n",
    "print(f\"\\nRanges (checking for reasonable values):\")\n",
    "print(f\"  u_t:  [{u_t.min():.3f}, {u_t.max():.3f}]\")\n",
    "print(f\"  u_x:  [{u_x.min():.3f}, {u_x.max():.3f}]\")\n",
    "print(f\"  u_y:  [{u_y.min():.3f}, {u_y.max():.3f}]\")\n",
    "print(f\"  u_xx: [{u_xx.min():.3f}, {u_xx.max():.3f}]\")\n",
    "print(f\"  u_yy: [{u_yy.min():.3f}, {u_yy.max():.3f}]\")\n",
    "\n",
    "# Visual check of derivatives\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "frame_idx = 25  # Check active frame\n",
    "\n",
    "axes[0, 0].imshow(images_more_clean[frame_idx], cmap='gray')\n",
    "axes[0, 0].set_title('Original (Frame 25)')\n",
    "\n",
    "axes[0, 1].imshow(u_t[frame_idx], cmap='RdBu', vmin=-20, vmax=20)\n",
    "axes[0, 1].set_title('∂u/∂t (Temporal)')\n",
    "\n",
    "axes[0, 2].imshow(u_x[frame_idx], cmap='RdBu', vmin=-10, vmax=10)\n",
    "axes[0, 2].set_title('∂u/∂x (Spatial)')\n",
    "\n",
    "axes[1, 0].imshow(u_y[frame_idx], cmap='RdBu', vmin=-10, vmax=10)\n",
    "axes[1, 0].set_title('∂u/∂y (Spatial)')\n",
    "\n",
    "axes[1, 1].imshow(u_xx[frame_idx], cmap='RdBu', vmin=-5, vmax=5)\n",
    "axes[1, 1].set_title('∂²u/∂x² (Laplacian X)')\n",
    "\n",
    "axes[1, 2].imshow(u_yy[frame_idx], cmap='RdBu', vmin=-5, vmax=5)\n",
    "axes[1, 2].set_title('∂²u/∂y² (Laplacian Y)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('derivatives_visualization.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Derivative visualization saved to 'derivatives_visualization.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa864a5-8a0c-4efd-a1b9-d0597d0a6480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_derivatives(u_t, u_xx, u_yy, frame_idx=25):\n",
    "    \"\"\"\n",
    "    Visual inspection of derivative quality.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    axes[0].imshow(u_t[frame_idx], cmap='RdBu', vmin=-0.1, vmax=0.1)\n",
    "    axes[0].set_title('∂u/∂t')\n",
    "    \n",
    "    axes[1].imshow(u_xx[frame_idx], cmap='RdBu', vmin=-0.1, vmax=0.1)\n",
    "    axes[1].set_title('∂²u/∂x²')\n",
    "    \n",
    "    axes[2].imshow(u_yy[frame_idx], cmap='RdBu', vmin=-0.1, vmax=0.1)\n",
    "    axes[2].set_title('∂²u/∂y²')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('derivative_quality_check.png', dpi=150)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"u_t range: [{u_t.min():.4f}, {u_t.max():.4f}]\")\n",
    "    print(f\"u_xx range: [{u_xx.min():.4f}, {u_xx.max():.4f}]\")\n",
    "\n",
    "\n",
    "validate_derivatives(u_t, u_xx, u_yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e44040-c7af-4aca-b268-07a7959a217e",
   "metadata": {},
   "source": [
    "## The time derivatives are noisy. So we need to do 3D denoising (denoising of the time component as well). So far, we only did denoising of x and y components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed483f82-aad4-4310-aacb-85c4dd9e6628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "def preprocess_images_spatiotemporal(images_raw):\n",
    "    \"\"\"\n",
    "    Denoising with BOTH spatial AND temporal smoothing.\n",
    "    \"\"\"\n",
    "    T, H, W = images_raw.shape\n",
    "    \n",
    "    # Step 1: Anscombe transform (frame-by-frame)\n",
    "    images_anscombe = np.zeros_like(images_raw, dtype=np.float32)\n",
    "    for t in range(T):\n",
    "        img = images_raw[t].astype(np.float32)\n",
    "        images_anscombe[t] = 2.0 * np.sqrt(img + 3.0/8.0)\n",
    "    \n",
    "    # Step 2: 3D Gaussian smoothing (spatial + temporal)\n",
    "    # sigma = (temporal_sigma, spatial_sigma_y, spatial_sigma_x)\n",
    "    images_denoised = gaussian_filter(\n",
    "        images_anscombe, \n",
    "        sigma=(1.0, 1.0, 1.0)  # Smooth in time and space\n",
    "    )\n",
    "    \n",
    "    # Step 3: Inverse Anscombe transform (frame-by-frame)\n",
    "    images_clean = np.zeros_like(images_raw, dtype=np.float32)\n",
    "    for t in range(T):\n",
    "        img_clean = (images_denoised[t] / 2.0)**2 - 3.0/8.0\n",
    "        img_clean = np.clip(img_clean, 0, 255)\n",
    "        images_clean[t] = img_clean\n",
    "    \n",
    "    return images_clean\n",
    "\n",
    "\n",
    "# Apply 3D denoising\n",
    "print(\"Applying spatiotemporal denoising...\")\n",
    "images_clean_3d = preprocess_images_spatiotemporal(images)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780c08b1-9977-406b-82cf-9ea70db50ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_idx = 35\n",
    "raw_frame = images_clean_3d[frame_idx]\n",
    "\n",
    "# 1. Function to calculate 2nd spatial derivative (u_xx) using finite difference\n",
    "def calculate_uxx(img):\n",
    "    # The u_xx kernel: [1, -2, 1]\n",
    "    uxx = cv2.filter2D(img.astype(np.float32), -1, np.array([[1, -2, 1]], dtype=np.float32))\n",
    "    return uxx\n",
    "\n",
    "# --- RAW DATA DIAGNOSIS ---\n",
    "raw_uxx = calculate_uxx(raw_frame)\n",
    "raw_uxx_variance = np.var(raw_uxx)\n",
    "print(f\"RAW u_xx Variance: {raw_uxx_variance:.2f}\")\n",
    "\n",
    "# --- SMOOTHED DATA DIAGNOSIS ---\n",
    "# Apply a mild Gaussian filter (simulating denoising)\n",
    "smoothed_frame = gaussian_filter(raw_frame, sigma=1)\n",
    "smoothed_uxx = calculate_uxx(smoothed_frame)\n",
    "smoothed_uxx_variance = np.var(smoothed_uxx)\n",
    "print(f\"SMOOTHED u_xx Variance: {smoothed_uxx_variance:.2f}\")\n",
    "\n",
    "# --- VISUAL PROOF ---\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Display the magnitude of the derivatives\n",
    "axes[0].imshow(np.abs(raw_uxx), cmap='hot', vmax=np.percentile(np.abs(raw_uxx), 98))\n",
    "axes[0].set_title(f'Raw |u_xx| (Variance: {raw_uxx_variance:.0f})')\n",
    "axes[1].imshow(np.abs(smoothed_uxx), cmap='hot', vmax=np.percentile(np.abs(smoothed_uxx), 98))\n",
    "axes[1].set_title(f'Smoothed |u_xx| (Variance: {smoothed_uxx_variance:.0f})')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0216fee-768a-4078-aa0a-5101b3cd0d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Recompute derivatives\n",
    "u_t, u_x, u_y, u_xx, u_yy = compute_derivatives(\n",
    "    images_clean_3d, \n",
    "    dx=1.0, dy=1.0, dt=1.0\n",
    ")\n",
    "\n",
    "print(f\"\\nNew u_t range: [{u_t.min():.3f}, {u_t.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39e0036-7c2a-4fe5-9498-f26f4e1ec200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_derivatives(images_clean, dx=1.0, dy=1.0, dt=1.0):\n",
    "    \"\"\"\n",
    "    Compute spatial and temporal derivatives for SINDy.\n",
    "    \n",
    "    Args:\n",
    "        images_clean: (T, H, W) denoised image stack in [0, 255]\n",
    "        dx, dy: spatial grid spacing (use 1.0 if unknown)\n",
    "        dt: time step between frames (use 1.0 if unknown, can rescale later)\n",
    "    \n",
    "    Returns:\n",
    "        u_t, u_x, u_y, u_xx, u_yy: derivative arrays\n",
    "    \"\"\"\n",
    "    T, H, W = images_clean.shape\n",
    "    \n",
    "    # 1. Temporal derivative (∂u/∂t) - forward difference\n",
    "    u_t = np.zeros_like(images_clean)\n",
    "    u_t[:-1] = (images_clean[1:] - images_clean[:-1]) / dt\n",
    "    # Note: last frame has u_t = 0 (can't compute forward difference)\n",
    "    \n",
    "    # 2. First spatial derivatives (∂u/∂x, ∂u/∂y) - central difference\n",
    "    u_x = np.zeros_like(images_clean)\n",
    "    u_y = np.zeros_like(images_clean)\n",
    "    \n",
    "    u_x[:, :, 1:-1] = (images_clean[:, :, 2:] - images_clean[:, :, :-2]) / (2*dx)\n",
    "    u_y[:, 1:-1, :] = (images_clean[:, 2:, :] - images_clean[:, :-2, :]) / (2*dy)\n",
    "    \n",
    "    # 3. Second spatial derivatives (∂²u/∂x², ∂²u/∂y²) - central difference\n",
    "    u_xx = np.zeros_like(images_clean)\n",
    "    u_yy = np.zeros_like(images_clean)\n",
    "    \n",
    "    u_xx[:, :, 1:-1] = (images_clean[:, :, 2:] - 2*images_clean[:, :, 1:-1] + \n",
    "                         images_clean[:, :, :-2]) / (dx**2)\n",
    "    u_yy[:, 1:-1, :] = (images_clean[:, 2:, :] - 2*images_clean[:, 1:-1, :] + \n",
    "                         images_clean[:, :-2, :]) / (dy**2)\n",
    "    \n",
    "    return u_t, u_x, u_y, u_xx, u_yy\n",
    "\n",
    "\n",
    "# Compute derivatives\n",
    "print(\"=\"*60)\n",
    "print(\"COMPUTING DERIVATIVES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "u_t, u_x, u_y, u_xx, u_yy = compute_derivatives(\n",
    "    images_clean_3d, \n",
    "    dx=1.0,  # pixel spacing (or physical spacing if known)\n",
    "    dy=1.0, \n",
    "    dt=1.0   # time step (or physical time if known)\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Derivatives computed successfully!\")\n",
    "print(f\"\\nShapes:\")\n",
    "print(f\"  u_t:  {u_t.shape}\")\n",
    "print(f\"  u_x:  {u_x.shape}\")\n",
    "print(f\"  u_xx: {u_xx.shape}\")\n",
    "\n",
    "print(f\"\\nRanges (checking for reasonable values):\")\n",
    "print(f\"  u_t:  [{u_t.min():.3f}, {u_t.max():.3f}]\")\n",
    "print(f\"  u_x:  [{u_x.min():.3f}, {u_x.max():.3f}]\")\n",
    "print(f\"  u_y:  [{u_y.min():.3f}, {u_y.max():.3f}]\")\n",
    "print(f\"  u_xx: [{u_xx.min():.3f}, {u_xx.max():.3f}]\")\n",
    "print(f\"  u_yy: [{u_yy.min():.3f}, {u_yy.max():.3f}]\")\n",
    "\n",
    "# Visual check of derivatives\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "frame_idx = 25  # Check active frame\n",
    "\n",
    "axes[0, 0].imshow(images_clean_3d[frame_idx], cmap='gray')\n",
    "axes[0, 0].set_title('Original (Frame 25)')\n",
    "\n",
    "axes[0, 1].imshow(u_t[frame_idx], cmap='RdBu', vmin=-20, vmax=20)\n",
    "axes[0, 1].set_title('∂u/∂t (Temporal)')\n",
    "\n",
    "axes[0, 2].imshow(u_x[frame_idx], cmap='RdBu', vmin=-10, vmax=10)\n",
    "axes[0, 2].set_title('∂u/∂x (Spatial)')\n",
    "\n",
    "axes[1, 0].imshow(u_y[frame_idx], cmap='RdBu', vmin=-10, vmax=10)\n",
    "axes[1, 0].set_title('∂u/∂y (Spatial)')\n",
    "\n",
    "axes[1, 1].imshow(u_xx[frame_idx], cmap='RdBu', vmin=-5, vmax=5)\n",
    "axes[1, 1].set_title('∂²u/∂x² (Laplacian X)')\n",
    "\n",
    "axes[1, 2].imshow(u_yy[frame_idx], cmap='RdBu', vmin=-5, vmax=5)\n",
    "axes[1, 2].set_title('∂²u/∂y² (Laplacian Y)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('derivatives_visualization.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Derivative visualization saved to 'derivatives_visualization.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6451002b-680c-4c6c-aff8-73b1984d1e2f",
   "metadata": {},
   "source": [
    "The time derivatives are still noisy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e571fcb0-342a-4f4d-a985-e8e912b49b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(images_clean_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0902162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import convolve\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# ============================================================================\n",
    "# ASSUME: images_more_clean is already loaded and preprocessed\n",
    "# Shape: (n_time, height, width) with intensity values [0, 255]\n",
    "# ============================================================================\n",
    "\n",
    "def apply_sindy_to_real_images(images, dt=1.0, dx=1.0, dy=1.0, threshold=0.1, \n",
    "                               subsample_frames=None, subsample_spatial=4):\n",
    "    \"\"\"\n",
    "    Apply SINDy and Weak-SINDy to real experimental images\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    images : ndarray, shape (n_time, height, width)\n",
    "        Preprocessed image sequence\n",
    "    dt : float\n",
    "        Time between frames (if unknown, use 1.0 and interpret results relatively)\n",
    "    dx, dy : float\n",
    "        Spatial resolution (if unknown, use 1.0 and interpret results in pixel units)\n",
    "    threshold : float\n",
    "        Sparsity threshold for SINDy\n",
    "    subsample_frames : int or None\n",
    "        Use only every Nth frame (None = use all frames)\n",
    "    subsample_spatial : int\n",
    "        Downsample spatially by this factor (e.g., 4 means use 1/16 of pixels)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"APPLYING SINDY TO REAL IMAGES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    n_frames, height, width = images.shape\n",
    "    print(f\"\\nOriginal data shape: {n_frames} frames, {height}x{width} pixels\")\n",
    "    \n",
    "    # Subsample frames if specified\n",
    "    if subsample_frames is not None and subsample_frames > 1:\n",
    "        images = images[::subsample_frames]\n",
    "        dt = dt * subsample_frames\n",
    "        print(f\"Subsampling frames: using every {subsample_frames}th frame\")\n",
    "    \n",
    "    # Subsample spatially to reduce memory\n",
    "    if subsample_spatial > 1:\n",
    "        images = images[:, ::subsample_spatial, ::subsample_spatial]\n",
    "        dx = dx * subsample_spatial\n",
    "        dy = dy * subsample_spatial\n",
    "        print(f\"Downsampling spatially by factor {subsample_spatial}\")\n",
    "    \n",
    "    n_frames, height, width = images.shape\n",
    "    total_samples = n_frames * height * width\n",
    "    memory_estimate = total_samples * 9 * 8 / (1024**3)  # 9 features, 8 bytes per float\n",
    "    \n",
    "    print(f\"\\nProcessing data shape: {n_frames} frames, {height}x{width} pixels\")\n",
    "    print(f\"Total samples: {total_samples:,}\")\n",
    "    print(f\"Estimated memory for library: {memory_estimate:.2f} GB\")\n",
    "    \n",
    "    if memory_estimate > 8:\n",
    "        print(\"\\n⚠️  WARNING: This will require a lot of memory!\")\n",
    "        print(\"   Consider increasing subsample_spatial or subsample_frames\")\n",
    "        response = input(\"   Continue anyway? (y/n): \")\n",
    "        if response.lower() != 'y':\n",
    "            return None\n",
    "    \n",
    "    print(f\"Time step: {dt}\")\n",
    "    print(f\"Spatial resolution: dx={dx}, dy={dy}\")\n",
    "    \n",
    "    # Normalize to [0, 1] for numerical stability\n",
    "    u_max = images.max()\n",
    "    u_min = images.min()\n",
    "    u_data = (images - u_min) / (u_max - u_min)\n",
    "    print(f\"\\nNormalized data to [0, 1] range\")\n",
    "    print(f\"Original range: [{u_min:.2f}, {u_max:.2f}]\")\n",
    "    \n",
    "    # --- Spatial Derivative Functions ---\n",
    "    def get_laplacian(f):\n",
    "        \"\"\"Compute Laplacian using finite differences\"\"\"\n",
    "        return (np.roll(f, -1, axis=0) + np.roll(f, 1, axis=0) +\n",
    "                np.roll(f, -1, axis=1) + np.roll(f, 1, axis=1) - 4*f) / (dx**2)\n",
    "    \n",
    "    def get_gradients(f):\n",
    "        \"\"\"Compute gradients using central differences\"\"\"\n",
    "        gx = (np.roll(f, -1, axis=0) - np.roll(f, 1, axis=0)) / (2*dx)\n",
    "        gy = (np.roll(f, -1, axis=1) - np.roll(f, 1, axis=1)) / (2*dy)\n",
    "        return gx, gy\n",
    "    \n",
    "    # --- Build Library ---\n",
    "    def build_library(u_data):\n",
    "        \"\"\"Build library of candidate functions\"\"\"\n",
    "        n_frames, nx, ny = u_data.shape\n",
    "        n_samples = n_frames * nx * ny\n",
    "        \n",
    "        u_flat = u_data.reshape(n_samples)\n",
    "        \n",
    "        lib_features = []\n",
    "        feature_names = []\n",
    "        \n",
    "        print(\"\\nBuilding function library...\")\n",
    "        \n",
    "        # Polynomial terms\n",
    "        lib_features.append(np.ones(n_samples))\n",
    "        feature_names.append('1')\n",
    "        \n",
    "        lib_features.append(u_flat)\n",
    "        feature_names.append('u')\n",
    "        \n",
    "        lib_features.append(u_flat**2)\n",
    "        feature_names.append('u²')\n",
    "        \n",
    "        lib_features.append(u_flat**3)\n",
    "        feature_names.append('u³')\n",
    "        \n",
    "        # Compute spatial derivatives for all frames\n",
    "        lap_all = np.zeros_like(u_data)\n",
    "        bilap_all = np.zeros_like(u_data)\n",
    "        gx_all = np.zeros_like(u_data)\n",
    "        gy_all = np.zeros_like(u_data)\n",
    "        \n",
    "        for i in range(n_frames):\n",
    "            lap_all[i] = get_laplacian(u_data[i])\n",
    "            bilap_all[i] = get_laplacian(lap_all[i])\n",
    "            gx_all[i], gy_all[i] = get_gradients(u_data[i])\n",
    "        \n",
    "        # Spatial derivative terms\n",
    "        lib_features.append(lap_all.reshape(n_samples))\n",
    "        feature_names.append('∇²u')\n",
    "        \n",
    "        lib_features.append(bilap_all.reshape(n_samples))\n",
    "        feature_names.append('∇⁴u')\n",
    "        \n",
    "        lib_features.append((gx_all**2 + gy_all**2).reshape(n_samples))\n",
    "        feature_names.append('|∇u|²')\n",
    "        \n",
    "        lib_features.append((u_data * lap_all).reshape(n_samples))\n",
    "        feature_names.append('u∇²u')\n",
    "        \n",
    "        # Additional terms that might appear in reaction-diffusion\n",
    "        lib_features.append((u_flat * (1 - u_flat)).reshape(-1))\n",
    "        feature_names.append('u(1-u)')\n",
    "        \n",
    "        Theta = np.column_stack(lib_features)\n",
    "        \n",
    "        print(f\"Library built: {len(feature_names)} candidate terms\")\n",
    "        print(f\"Library matrix shape: {Theta.shape}\")\n",
    "        \n",
    "        return Theta, feature_names\n",
    "    \n",
    "    # --- Standard SINDy ---\n",
    "    def sindy_method(u_data, threshold):\n",
    "        \"\"\"Standard SINDy\"\"\"\n",
    "        print(\"\\n\" + \"-\" * 70)\n",
    "        print(\"STANDARD SINDY\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Compute time derivative\n",
    "        u_dot = np.zeros_like(u_data[:-1])\n",
    "        for i in range(len(u_data) - 1):\n",
    "            u_dot[i] = (u_data[i+1] - u_data[i]) / dt\n",
    "        \n",
    "        # Build library (exclude last frame)\n",
    "        Theta, feature_names = build_library(u_data[:-1])\n",
    "        u_dot_flat = u_dot.reshape(-1)\n",
    "        \n",
    "        # Sequential Thresholded Least Squares\n",
    "        coeffs = np.linalg.lstsq(Theta, u_dot_flat, rcond=None)[0]\n",
    "        \n",
    "        for iteration in range(10):\n",
    "            small_inds = np.abs(coeffs) < threshold\n",
    "            coeffs[small_inds] = 0\n",
    "            big_inds = ~small_inds\n",
    "            if np.sum(big_inds) == 0:\n",
    "                break\n",
    "            coeffs[big_inds] = np.linalg.lstsq(Theta[:, big_inds], u_dot_flat, rcond=None)[0]\n",
    "        \n",
    "        # Compute metrics\n",
    "        prediction = Theta @ coeffs\n",
    "        mse = np.mean((prediction - u_dot_flat)**2)\n",
    "        r2 = r2_score(u_dot_flat, prediction)\n",
    "        \n",
    "        print(\"\\nDiscovered equation: ∂u/∂t = \")\n",
    "        for i, (coef, name) in enumerate(zip(coeffs, feature_names)):\n",
    "            if abs(coef) > 1e-10:\n",
    "                print(f\"  {coef:+.6f} · {name}\")\n",
    "        \n",
    "        print(f\"\\nMean Squared Error: {mse:.6e}\")\n",
    "        print(f\"R² Score: {r2:.6f}\")\n",
    "        \n",
    "        return coeffs, feature_names, mse, r2, prediction, u_dot_flat\n",
    "    \n",
    "    # --- Weak SINDy ---\n",
    "    def weak_sindy_method(u_data, threshold, kernel_size=7):\n",
    "        \"\"\"Weak SINDy with test functions\"\"\"\n",
    "        print(\"\\n\" + \"-\" * 70)\n",
    "        print(\"WEAK SINDY\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        n_frames, nx, ny = u_data.shape\n",
    "        \n",
    "        # Create Gaussian test function\n",
    "        def gaussian_kernel_2d(size, sigma):\n",
    "            kernel = np.zeros((size, size))\n",
    "            center = size // 2\n",
    "            for i in range(size):\n",
    "                for j in range(size):\n",
    "                    x, y = i - center, j - center\n",
    "                    kernel[i, j] = np.exp(-(x**2 + y**2) / (2 * sigma**2))\n",
    "            return kernel / kernel.sum()\n",
    "        \n",
    "        test_func = gaussian_kernel_2d(kernel_size, kernel_size/4)\n",
    "        print(f\"Using Gaussian kernel of size {kernel_size}x{kernel_size}\")\n",
    "        \n",
    "        # Apply weak formulation\n",
    "        u_weak = np.zeros((n_frames, nx, ny))\n",
    "        for i in range(n_frames):\n",
    "            u_weak[i] = convolve(u_data[i], test_func, mode='reflect')\n",
    "        \n",
    "        # Time derivative in weak form\n",
    "        u_dot_weak = np.zeros_like(u_weak[:-1])\n",
    "        for i in range(len(u_weak) - 1):\n",
    "            u_dot_weak[i] = (u_weak[i+1] - u_weak[i]) / dt\n",
    "        \n",
    "        # Build weak library\n",
    "        Theta_weak = []\n",
    "        feature_names = ['1', 'u', 'u²', 'u³', '∇²u', '∇⁴u', '|∇u|²', 'u∇²u', 'u(1-u)']\n",
    "        \n",
    "        for frame_idx in range(n_frames - 1):\n",
    "            frame_features = []\n",
    "            \n",
    "            frame_features.append(np.sum(test_func) * np.ones((nx, ny)))\n",
    "            frame_features.append(u_weak[frame_idx])\n",
    "            frame_features.append(convolve(u_data[frame_idx]**2, test_func, mode='reflect'))\n",
    "            frame_features.append(convolve(u_data[frame_idx]**3, test_func, mode='reflect'))\n",
    "            \n",
    "            lap = get_laplacian(u_data[frame_idx])\n",
    "            bilap = get_laplacian(lap)\n",
    "            gx, gy = get_gradients(u_data[frame_idx])\n",
    "            \n",
    "            frame_features.append(convolve(lap, test_func, mode='reflect'))\n",
    "            frame_features.append(convolve(bilap, test_func, mode='reflect'))\n",
    "            frame_features.append(convolve(gx**2 + gy**2, test_func, mode='reflect'))\n",
    "            frame_features.append(convolve(u_data[frame_idx] * lap, test_func, mode='reflect'))\n",
    "            frame_features.append(convolve(u_data[frame_idx] * (1 - u_data[frame_idx]), test_func, mode='reflect'))\n",
    "            \n",
    "            Theta_weak.append(np.column_stack([f.reshape(-1) for f in frame_features]))\n",
    "        \n",
    "        Theta_weak = np.vstack(Theta_weak)\n",
    "        u_dot_weak_flat = u_dot_weak.reshape(-1)\n",
    "        \n",
    "        # Sequential Thresholded Least Squares\n",
    "        coeffs = np.linalg.lstsq(Theta_weak, u_dot_weak_flat, rcond=None)[0]\n",
    "        \n",
    "        for iteration in range(10):\n",
    "            small_inds = np.abs(coeffs) < threshold\n",
    "            coeffs[small_inds] = 0\n",
    "            big_inds = ~small_inds\n",
    "            if np.sum(big_inds) == 0:\n",
    "                break\n",
    "            coeffs[big_inds] = np.linalg.lstsq(Theta_weak[:, big_inds], u_dot_weak_flat, rcond=None)[0]\n",
    "        \n",
    "        # Compute metrics\n",
    "        prediction = Theta_weak @ coeffs\n",
    "        mse = np.mean((prediction - u_dot_weak_flat)**2)\n",
    "        r2 = r2_score(u_dot_weak_flat, prediction)\n",
    "        \n",
    "        print(\"\\nDiscovered equation: ∂u/∂t = \")\n",
    "        for i, (coef, name) in enumerate(zip(coeffs, feature_names)):\n",
    "            if abs(coef) > 1e-10:\n",
    "                print(f\"  {coef:+.6f} · {name}\")\n",
    "        \n",
    "        print(f\"\\nMean Squared Error: {mse:.6e}\")\n",
    "        print(f\"R² Score: {r2:.6f}\")\n",
    "        \n",
    "        return coeffs, feature_names, mse, r2, prediction, u_dot_weak_flat\n",
    "    \n",
    "    # --- Run Both Methods ---\n",
    "    sindy_coeffs, sindy_names, sindy_mse, sindy_r2, sindy_pred, sindy_target = sindy_method(u_data, threshold)\n",
    "    weak_coeffs, weak_names, weak_mse, weak_r2, weak_pred, weak_target = weak_sindy_method(u_data, threshold)\n",
    "    \n",
    "    # --- Verification Strategy ---\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"VERIFICATION STRATEGY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\\n1. RECONSTRUCTION QUALITY\")\n",
    "    print(\"   - R² Score measures how well discovered equation fits data\")\n",
    "    print(f\"   - SINDy R²: {sindy_r2:.4f}\")\n",
    "    print(f\"   - Weak-SINDy R²: {weak_r2:.4f}\")\n",
    "    print(\"   - Values close to 1.0 indicate good fit\")\n",
    "    \n",
    "    print(\"\\n2. CONSISTENCY CHECK\")\n",
    "    print(\"   - Do both methods identify similar dominant terms?\")\n",
    "    sindy_active = set([name for coef, name in zip(sindy_coeffs, sindy_names) if abs(coef) > 1e-6])\n",
    "    weak_active = set([name for coef, name in zip(weak_coeffs, weak_names) if abs(coef) > 1e-6])\n",
    "    common_terms = sindy_active & weak_active\n",
    "    print(f\"   - Common active terms: {common_terms}\")\n",
    "    \n",
    "    print(\"\\n3. PHYSICAL PLAUSIBILITY\")\n",
    "    print(\"   - Check if discovered terms make physical sense\")\n",
    "    print(\"   - Common PDE patterns:\")\n",
    "    print(\"     • Diffusion: ∇²u term\")\n",
    "    print(\"     • Reaction-Diffusion: ∇²u + u(1-u) or ∇²u + u²\")\n",
    "    print(\"     • Pattern formation: ∇²u - ∇⁴u (Swift-Hohenberg)\")\n",
    "    print(\"     • Nonlinear advection: |∇u|² or u∇²u\")\n",
    "    \n",
    "    print(\"\\n4. FORWARD SIMULATION\")\n",
    "    print(\"   - Use discovered PDE to simulate forward in time\")\n",
    "    print(\"   - Compare simulation with actual images\")\n",
    "    print(\"   - This is implemented in the visualization below\")\n",
    "    \n",
    "    # --- Visualization ---\n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "    \n",
    "    # 1. Coefficient Comparison\n",
    "    ax1 = plt.subplot(3, 3, 1)\n",
    "    x_pos = np.arange(len(sindy_names))\n",
    "    width = 0.35\n",
    "    ax1.bar(x_pos - width/2, sindy_coeffs, width, label='SINDy', alpha=0.8)\n",
    "    ax1.bar(x_pos + width/2, [weak_coeffs[weak_names.index(n)] if n in weak_names else 0 for n in sindy_names], \n",
    "            width, label='Weak-SINDy', alpha=0.8)\n",
    "    ax1.set_xlabel('Terms')\n",
    "    ax1.set_ylabel('Coefficient Value')\n",
    "    ax1.set_title('Discovered Coefficients', fontweight='bold')\n",
    "    ax1.set_xticks(x_pos)\n",
    "    ax1.set_xticklabels(sindy_names, rotation=45, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "    \n",
    "    # 2. R² Comparison\n",
    "    ax2 = plt.subplot(3, 3, 2)\n",
    "    methods = ['SINDy', 'Weak-SINDy']\n",
    "    r2_scores = [sindy_r2, weak_r2]\n",
    "    colors = ['#1f77b4', '#ff7f0e']\n",
    "    bars = ax2.bar(methods, r2_scores, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "    ax2.set_ylabel('R² Score')\n",
    "    ax2.set_title('Reconstruction Quality', fontweight='bold')\n",
    "    ax2.set_ylim([0, 1.1])\n",
    "    ax2.axhline(y=1.0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.4f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # 3. MSE Comparison\n",
    "    ax3 = plt.subplot(3, 3, 3)\n",
    "    mse_scores = [sindy_mse, weak_mse]\n",
    "    bars = ax3.bar(methods, mse_scores, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "    ax3.set_ylabel('Mean Squared Error')\n",
    "    ax3.set_title('Prediction Error', fontweight='bold')\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2e}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # 4-6. Sample frames from data\n",
    "    sample_frames = [0, n_frames//2, n_frames-1]\n",
    "    for i, frame_idx in enumerate(sample_frames):\n",
    "        ax = plt.subplot(3, 3, 4+i)\n",
    "        im = ax.imshow(u_data[frame_idx], cmap='viridis', aspect='auto')\n",
    "        ax.set_title(f'Frame {frame_idx}', fontweight='bold')\n",
    "        ax.axis('off')\n",
    "        plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "    \n",
    "    # 7-8. Prediction vs Target scatter plots\n",
    "    # Subsample for visualization\n",
    "    n_subsample = min(10000, len(sindy_target))\n",
    "    indices = np.random.choice(len(sindy_target), n_subsample, replace=False)\n",
    "    \n",
    "    ax7 = plt.subplot(3, 3, 7)\n",
    "    ax7.scatter(sindy_target[indices], sindy_pred[indices], alpha=0.3, s=1)\n",
    "    ax7.plot([sindy_target.min(), sindy_target.max()], \n",
    "             [sindy_target.min(), sindy_target.max()], 'r--', linewidth=2)\n",
    "    ax7.set_xlabel('True ∂u/∂t')\n",
    "    ax7.set_ylabel('Predicted ∂u/∂t')\n",
    "    ax7.set_title(f'SINDy: R²={sindy_r2:.4f}', fontweight='bold')\n",
    "    ax7.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax8 = plt.subplot(3, 3, 8)\n",
    "    ax8.scatter(weak_target[indices], weak_pred[indices], alpha=0.3, s=1, color='orange')\n",
    "    ax8.plot([weak_target.min(), weak_target.max()], \n",
    "             [weak_target.min(), weak_target.max()], 'r--', linewidth=2)\n",
    "    ax8.set_xlabel('True ∂u/∂t')\n",
    "    ax8.set_ylabel('Predicted ∂u/∂t')\n",
    "    ax8.set_title(f'Weak-SINDy: R²={weak_r2:.4f}', fontweight='bold')\n",
    "    ax8.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 9. Temporal evolution of mean intensity\n",
    "    ax9 = plt.subplot(3, 3, 9)\n",
    "    mean_intensity = [u_data[i].mean() for i in range(n_frames)]\n",
    "    ax9.plot(mean_intensity, linewidth=2)\n",
    "    ax9.set_xlabel('Frame')\n",
    "    ax9.set_ylabel('Mean Intensity')\n",
    "    ax9.set_title('Temporal Evolution', fontweight='bold')\n",
    "    ax9.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('SINDy Analysis of Real Experimental Images', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'sindy': {'coeffs': sindy_coeffs, 'names': sindy_names, 'mse': sindy_mse, 'r2': sindy_r2},\n",
    "        'weak': {'coeffs': weak_coeffs, 'names': weak_names, 'mse': weak_mse, 'r2': weak_r2},\n",
    "        'u_data': u_data\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE:\n",
    "results = apply_sindy_to_real_images(images_more_clean, dt=1.0, dx=1.0, threshold=0.1)\n",
    "# ============================================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e76b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = apply_sindy_to_real_images(images_clean_3d, dt=1.0, dx=1.0, threshold=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3773bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = apply_sindy_to_real_images(images, dt=1.0, dx=1.0, threshold=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6a11a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldmproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
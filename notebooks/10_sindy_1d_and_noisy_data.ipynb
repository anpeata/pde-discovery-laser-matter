{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77db3ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "DOMAIN_SIZE = 100.0\n",
    "N_DOF = 200\n",
    "DT = 0.05 \n",
    "\n",
    "class KuramotoSivashinsky():\n",
    "    def __init__(\n",
    "        self,\n",
    "        L,\n",
    "        N,\n",
    "        dt,\n",
    "        a=-1.0,\n",
    "        b=-1.0,\n",
    "        c=-0.5,\n",
    "    ):\n",
    "        self.L = L\n",
    "        self.N = N\n",
    "        self.dt = dt\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "\n",
    "        self.dx = L / N\n",
    "\n",
    "        # Frequencies (cycles per unit length)\n",
    "        freqs = jnp.fft.rfftfreq(N, d=self.dx)\n",
    "        # Convert to angular wavenumbers k = 2pi * freq\n",
    "        k = 2 * jnp.pi * freqs\n",
    "        self.derivative_operator = 1j * k\n",
    "\n",
    "        # Linear operator in Fourier space: a*d^2/dx^2 + b*d^4/dx^4\n",
    "        linear_operator = self.a * (-k**2) + self.b * (k**4)\n",
    "\n",
    "        self.exp_term = jnp.exp(dt * linear_operator)\n",
    "        self.coef = jnp.where(\n",
    "            linear_operator == 0.0,\n",
    "            dt,\n",
    "            (self.exp_term - 1.0) / linear_operator,\n",
    "        )\n",
    "\n",
    "        # 2/3 rule dealiasing mask for nonlinear term\n",
    "        self.alias_mask = (freqs < 2/3 * jnp.max(freqs))\n",
    "\n",
    "    def __call__(self, u):\n",
    "        # Nonlinear term c * d/dx (u^2)\n",
    "        u_nonlin = self.c * u**2\n",
    "        u_hat = jnp.fft.rfft(u)\n",
    "        u_nonlin_hat = jnp.fft.rfft(u_nonlin)\n",
    "        u_nonlin_hat = self.alias_mask * u_nonlin_hat\n",
    "\n",
    "        u_nonlin_der_hat = self.derivative_operator * u_nonlin_hat\n",
    "\n",
    "        # Exponential time differencing step\n",
    "        u_next_hat = self.exp_term * u_hat + self.coef * u_nonlin_der_hat\n",
    "        u_next = jnp.fft.irfft(u_next_hat, n=self.N)\n",
    "\n",
    "        return u_next\n",
    "\n",
    "# Create spatial mesh\n",
    "mesh = jnp.linspace(0.0, DOMAIN_SIZE, N_DOF, endpoint=False)\n",
    "\n",
    "# Initial condition\n",
    "u_0 = jnp.sin(16 * jnp.pi * mesh / DOMAIN_SIZE)\n",
    "\n",
    "ks_stepper = KuramotoSivashinsky(\n",
    "    L=DOMAIN_SIZE,\n",
    "    N=N_DOF,\n",
    "    dt=DT,\n",
    "    a=-1.0,\n",
    "    b=-1.0,\n",
    "    c=-0.5,\n",
    ")\n",
    "\n",
    "ks_stepper = jax.jit(ks_stepper)\n",
    "\n",
    "# METHOD 1: Spatial Translation (Pixels don't coincide between time steps)\n",
    "def generate_spatially_shifted_data():\n",
    "    \"\"\"Apply slight translations between time steps\"\"\"\n",
    "    u_current = u_0\n",
    "    trj_shifted = [u_current]\n",
    "    \n",
    "    # Generate clean trajectory first\n",
    "    for i in range(2000):\n",
    "        u_current = ks_stepper(u_current)\n",
    "        trj_shifted.append(u_current)\n",
    "    \n",
    "    trj_shifted = jnp.stack(trj_shifted)\n",
    "    \n",
    "    # Now apply random spatial shifts to break pixel alignment\n",
    "    rng = np.random.default_rng(42)\n",
    "    trj_unregistered = []\n",
    "    \n",
    "    for i in range(trj_shifted.shape[0]):\n",
    "        # Random shift between -3 and +3 pixels\n",
    "        shift = rng.integers(-3, 4)\n",
    "        shifted_frame = jnp.roll(trj_shifted[i], shift)\n",
    "        trj_unregistered.append(shifted_frame)\n",
    "    \n",
    "    return jnp.stack(trj_unregistered)\n",
    "\n",
    "# METHOD 2: Varying Initial Conditions (Each time step from different IC)\n",
    "def generate_varying_ic_data():\n",
    "    \"\"\"Each time step comes from slightly different initial conditions\"\"\"\n",
    "    rng = np.random.default_rng(42)\n",
    "    trj_varied = []\n",
    "    \n",
    "    # Base initial condition\n",
    "    base_u0 = u_0\n",
    "    \n",
    "    for i in range(2001):  # Same number of time steps\n",
    "        # Add small Gaussian noise to initial condition\n",
    "        noise_level = 0.02  # 2% noise\n",
    "        perturbed_u0 = base_u0 + noise_level * rng.normal(0, 1, base_u0.shape)\n",
    "        \n",
    "        # Simulate forward i steps from perturbed initial condition\n",
    "        u_current = perturbed_u0\n",
    "        for step in range(i):  # Simulate exactly i steps\n",
    "            u_current = ks_stepper(u_current)\n",
    "        \n",
    "        trj_varied.append(u_current)\n",
    "    \n",
    "    return jnp.stack(trj_varied)\n",
    "\n",
    "# Generate both types of unregistered data\n",
    "print(\"Generating spatially shifted data (spatial misregistration noise)...\")\n",
    "trj_spatial_shift = generate_spatially_shifted_data()\n",
    "\n",
    "print(\"Generating varying initial condition data...\")  \n",
    "trj_varied_ic = generate_varying_ic_data()\n",
    "\n",
    "# Generate clean data for comparison\n",
    "print(\"Generating clean data...\")\n",
    "u_current = u_0\n",
    "trj_clean = [u_current]\n",
    "for i in range(2000):\n",
    "    u_current = ks_stepper(u_current)\n",
    "    trj_clean.append(u_current)\n",
    "trj_clean = jnp.stack(trj_clean)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Clean data\n",
    "im0 = axes[0].imshow(trj_clean.T, cmap=\"RdBu\", aspect=\"auto\", origin=\"lower\",\n",
    "                    extent=(0, trj_clean.shape[0] * DT, 0, DOMAIN_SIZE))\n",
    "axes[0].set_title(\"Clean Data\\n(Pixels Aligned)\")\n",
    "axes[0].set_xlabel(\"Time\")\n",
    "axes[0].set_ylabel(\"Space\")\n",
    "plt.colorbar(im0, ax=axes[0])\n",
    "\n",
    "# Spatially shifted\n",
    "im1 = axes[1].imshow(trj_spatial_shift.T, cmap=\"RdBu\", aspect=\"auto\", origin=\"lower\",\n",
    "                    extent=(0, trj_spatial_shift.shape[0] * DT, 0, DOMAIN_SIZE))\n",
    "axes[1].set_title(\"Spatially Shifted\\n(Pixels Don't Coincide)\")\n",
    "axes[1].set_xlabel(\"Time\")\n",
    "axes[1].set_ylabel(\"Space\")\n",
    "plt.colorbar(im1, ax=axes[1])\n",
    "\n",
    "# Varying IC\n",
    "im2 = axes[2].imshow(trj_varied_ic.T, cmap=\"RdBu\", aspect=\"auto\", origin=\"lower\",\n",
    "                    extent=(0, trj_varied_ic.shape[0] * DT, 0, DOMAIN_SIZE))\n",
    "axes[2].set_title(\"Varying Initial Conditions\\n(Pixels Don't Coincide)\")\n",
    "axes[2].set_xlabel(\"Time\")\n",
    "axes[2].set_ylabel(\"Space\")\n",
    "plt.colorbar(im2, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ks_unregistered_data_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save all datasets\n",
    "x = jnp.linspace(0.0, DOMAIN_SIZE, N_DOF, endpoint=False)\n",
    "t = jnp.arange(trj_clean.shape[0]) * DT\n",
    "\n",
    "# Save clean data\n",
    "with h5py.File(\"test_solving_euler_ks_clean.h5\", \"w\") as f:\n",
    "    f.create_dataset(\"u\", data=trj_clean)\n",
    "    f.create_dataset(\"x\", data=x)\n",
    "    f.create_dataset(\"t\", data=t)\n",
    "    f.attrs[\"domain_size\"] = DOMAIN_SIZE\n",
    "    f.attrs[\"n_dof\"] = N_DOF\n",
    "    f.attrs[\"dt\"] = DT\n",
    "    f.attrs[\"n_steps\"] = trj_clean.shape[0]\n",
    "    f.attrs[\"registration\"] = \"aligned\"\n",
    "\n",
    "# Save spatially shifted data\n",
    "with h5py.File(\"test_solving_euler_ks_spatial_shift.h5\", \"w\") as f:\n",
    "    f.create_dataset(\"u\", data=trj_spatial_shift)\n",
    "    f.create_dataset(\"x\", data=x)\n",
    "    f.create_dataset(\"t\", data=t)\n",
    "    f.attrs[\"domain_size\"] = DOMAIN_SIZE\n",
    "    f.attrs[\"n_dof\"] = N_DOF\n",
    "    f.attrs[\"dt\"] = DT\n",
    "    f.attrs[\"n_steps\"] = trj_spatial_shift.shape[0]\n",
    "    f.attrs[\"registration\"] = \"unregistered_spatial_shift\"\n",
    "    f.attrs[\"perturbation_type\"] = \"spatial_translation\"\n",
    "\n",
    "# Save varying IC data\n",
    "with h5py.File(\"test_solving_euler_ks_varied_ic.h5\", \"w\") as f:\n",
    "    f.create_dataset(\"u\", data=trj_varied_ic)\n",
    "    f.create_dataset(\"x\", data=x)\n",
    "    f.create_dataset(\"t\", data=t)\n",
    "    f.attrs[\"domain_size\"] = DOMAIN_SIZE\n",
    "    f.attrs[\"n_dof\"] = N_DOF\n",
    "    f.attrs[\"dt\"] = DT\n",
    "    f.attrs[\"n_steps\"] = trj_varied_ic.shape[0]\n",
    "    f.attrs[\"registration\"] = \"unregistered_varied_ic\"\n",
    "    f.attrs[\"perturbation_type\"] = \"varying_initial_conditions\"\n",
    "\n",
    "print(\"All datasets saved successfully!\")\n",
    "print(\"Files created:\")\n",
    "print(\"- test_solving_euler_ks_clean.h5 (aligned pixels)\")\n",
    "print(\"- test_solving_euler_ks_spatial_shift.h5 (spatial shifts)\")\n",
    "print(\"- test_solving_euler_ks_varied_ic.h5 (varying initial conditions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c225c45-f0e9-4f74-8f24-2bd10967e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.fft import fft, ifft, fftfreq\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "\n",
    "\n",
    "# === Compute spatial derivatives using spectral method ===\n",
    "def spectral_derivative(u_snapshot, k):\n",
    "    \"\"\"Compute first spatial derivative using FFT.\"\"\"\n",
    "    u_hat = fft(u_snapshot)\n",
    "    return np.real(ifft(1j * k * u_hat))\n",
    "\n",
    "def spectral_second_derivative(u_snapshot, k):\n",
    "    \"\"\"Compute second spatial derivative.\"\"\"\n",
    "    u_hat = fft(u_snapshot)\n",
    "    return np.real(ifft(-k**2 * u_hat))\n",
    "\n",
    "def spectral_fourth_derivative(u_snapshot, k):\n",
    "    \"\"\"Compute fourth spatial derivative.\"\"\"\n",
    "    u_hat = fft(u_snapshot)\n",
    "    return np.real(ifft(k**4 * u_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b420fb4-5b1d-4465-8613-100706bf3f41",
   "metadata": {},
   "source": [
    "### FFT SINDy on data with spatial shift "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999b605d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"test_solving_euler_ks_spatial_shift.h5\", \"r\") as f:\n",
    "    u = np.array(f[\"u\"])      # shape (Nt, Nx)\n",
    "    x = np.array(f[\"x\"])      # spatial grid\n",
    "    t = np.array(f[\"t\"])      # time grid\n",
    "    \n",
    "Nx = len(x)\n",
    "Nt = len(t)\n",
    "L = x[-1] - x[0] + (x[1]-x[0])\n",
    "dx = x[1] - x[0]\n",
    "dt = t[1] - t[0]\n",
    "    \n",
    "print(f\"Loaded u(t,x) with shape {u.shape}, dx={dx:.3f}, dt={dt:.3f}\")\n",
    "    \n",
    "    \n",
    "# Define wavenumbers\n",
    "k = 2 * np.pi * fftfreq(Nx, d=dx)\n",
    "    \n",
    "# === Compute temporal derivative u_t using finite difference ===\n",
    "u_t = (u[2:] - u[:-2]) / (2 * dt)          # central difference in time\n",
    "u_mid = u[1:-1]                            # align time dimension\n",
    "    \n",
    "# === Build feature library Θ(u) ===\n",
    "# For each time snapshot, compute spatial derivatives\n",
    "Theta = []\n",
    "ut_flat = []\n",
    "    \n",
    "for snapshot, ut_snapshot in zip(u_mid, u_t):\n",
    "    ux  = spectral_derivative(snapshot, k)\n",
    "    uxx = spectral_second_derivative(snapshot, k)\n",
    "    uxxxx = spectral_fourth_derivative(snapshot, k)\n",
    "    \n",
    "    # Feature terms\n",
    "    Theta_snapshot = np.vstack([\n",
    "            uxx,         # diffusion term\n",
    "            uxxxx,       # hyperdiffusion\n",
    "            snapshot * ux, # nonlinear advection term (u * ux)\n",
    "        ]).T  # shape (Nx, 3)\n",
    "    \n",
    "    Theta.append(Theta_snapshot)\n",
    "    ut_flat.append(ut_snapshot)\n",
    "    \n",
    "Theta = np.vstack(Theta)       # shape (Nt*Nx, 3)\n",
    "ut_flat = np.hstack(ut_flat)   # flatten to 1D (Nt*Nx,)\n",
    "    \n",
    "print(\"Feature matrix Θ shape:\", Theta.shape)\n",
    "print(\"Target vector u_t shape:\", ut_flat.shape)\n",
    "    \n",
    "# === Normalize features for numerical stability ===\n",
    "Theta_mean = Theta.mean(axis=0)\n",
    "Theta_std = Theta.std(axis=0)\n",
    "Theta_norm = (Theta - Theta_mean) / Theta_std\n",
    "    \n",
    "# === Sparse regression (LASSO) ===\n",
    "lasso = Lasso(alpha=1e-4, fit_intercept=False)\n",
    "lasso.fit(Theta_norm, ut_flat)\n",
    "coeffs = lasso.coef_ / Theta_std  # un-normalize coefficients\n",
    "    \n",
    "print(\"\\nRecovered PDE coefficients (Spatial Translation):\")\n",
    "print(f\"a (u_xx):     {coeffs[0]:.4f}\")\n",
    "print(f\"b (u_xxxx):   {coeffs[1]:.4f}\")\n",
    "print(f\"c (u*u_x):    {coeffs[2]:.4f}\") # Updated label for the new term\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c5d096-814b-4ddb-936c-0f9c11e4bb4e",
   "metadata": {},
   "source": [
    "## FFt basd SINDY on data with different initial conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e4134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"test_solving_euler_ks_varied_ic.h5\", \"r\") as f:\n",
    "    u = np.array(f[\"u\"])      # shape (Nt, Nx)\n",
    "    x = np.array(f[\"x\"])      # spatial grid\n",
    "    t = np.array(f[\"t\"])      # time grid\n",
    "    \n",
    "Nx = len(x)\n",
    "Nt = len(t)\n",
    "L = x[-1] - x[0] + (x[1]-x[0])\n",
    "dx = x[1] - x[0]\n",
    "dt = t[1] - t[0]\n",
    "    \n",
    "print(f\"Loaded u(t,x) with shape {u.shape}, dx={dx:.3f}, dt={dt:.3f}\")\n",
    "    \n",
    "    \n",
    "# Define wavenumbers\n",
    "k = 2 * np.pi * fftfreq(Nx, d=dx)\n",
    "    \n",
    "# === Compute temporal derivative u_t using finite difference ===\n",
    "u_t = (u[2:] - u[:-2]) / (2 * dt)          # central difference in time\n",
    "u_mid = u[1:-1]                            # align time dimension\n",
    "    \n",
    "# === Build feature library Θ(u) ===\n",
    "# For each time snapshot, compute spatial derivatives\n",
    "Theta = []\n",
    "ut_flat = []\n",
    "    \n",
    "for snapshot, ut_snapshot in zip(u_mid, u_t):\n",
    "    ux  = spectral_derivative(snapshot, k)\n",
    "    uxx = spectral_second_derivative(snapshot, k)\n",
    "    uxxxx = spectral_fourth_derivative(snapshot, k)\n",
    "    \n",
    "    # Feature terms\n",
    "    Theta_snapshot = np.vstack([\n",
    "            uxx,         # diffusion term\n",
    "            uxxxx,       # hyperdiffusion\n",
    "            snapshot * ux, # nonlinear advection term (u * ux)\n",
    "        ]).T  # shape (Nx, 3)\n",
    "    \n",
    "    Theta.append(Theta_snapshot)\n",
    "    ut_flat.append(ut_snapshot)\n",
    "    \n",
    "Theta = np.vstack(Theta)       # shape (Nt*Nx, 3)\n",
    "ut_flat = np.hstack(ut_flat)   # flatten to 1D (Nt*Nx,)\n",
    "    \n",
    "print(\"Feature matrix Θ shape:\", Theta.shape)\n",
    "print(\"Target vector u_t shape:\", ut_flat.shape)\n",
    "    \n",
    "# === Normalize features for numerical stability ===\n",
    "Theta_mean = Theta.mean(axis=0)\n",
    "Theta_std = Theta.std(axis=0)\n",
    "Theta_norm = (Theta - Theta_mean) / Theta_std\n",
    "    \n",
    "# === Sparse regression (LASSO) ===\n",
    "lasso = Lasso(alpha=1e-4, fit_intercept=False)\n",
    "lasso.fit(Theta_norm, ut_flat)\n",
    "coeffs = lasso.coef_ / Theta_std  # un-normalize coefficients\n",
    "    \n",
    "print(\"\\nRecovered PDE coefficients (Varied Initial Conditions):\")\n",
    "print(f\"a (u_xx):     {coeffs[0]:.4f}\")\n",
    "print(f\"b (u_xxxx):   {coeffs[1]:.4f}\")\n",
    "print(f\"c (u*u_x):    {coeffs[2]:.4f}\") # Updated label for the new term\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb2b15d-3fba-42d7-b251-82ba6b3b1dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FFT based SINDy on data with clean ks data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac6c8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"test_solving_euler_ks_clean.h5\", \"r\") as f:\n",
    "    u = np.array(f[\"u\"])      # shape (Nt, Nx)\n",
    "    x = np.array(f[\"x\"])      # spatial grid\n",
    "    t = np.array(f[\"t\"])      # time grid\n",
    "    \n",
    "Nx = len(x)\n",
    "Nt = len(t)\n",
    "L = x[-1] - x[0] + (x[1]-x[0])\n",
    "dx = x[1] - x[0]\n",
    "dt = t[1] - t[0]\n",
    "    \n",
    "print(f\"Loaded u(t,x) with shape {u.shape}, dx={dx:.3f}, dt={dt:.3f}\")\n",
    "    \n",
    "    \n",
    "# Define wavenumbers\n",
    "k = 2 * np.pi * fftfreq(Nx, d=dx)\n",
    "    \n",
    "# === Compute temporal derivative u_t using finite difference ===\n",
    "u_t = (u[2:] - u[:-2]) / (2 * dt)          # central difference in time\n",
    "u_mid = u[1:-1]                            # align time dimension\n",
    "    \n",
    "# === Build feature library Θ(u) ===\n",
    "# For each time snapshot, compute spatial derivatives\n",
    "Theta = []\n",
    "ut_flat = []\n",
    "    \n",
    "for snapshot, ut_snapshot in zip(u_mid, u_t):\n",
    "    ux  = spectral_derivative(snapshot, k)\n",
    "    uxx = spectral_second_derivative(snapshot, k)\n",
    "    uxxxx = spectral_fourth_derivative(snapshot, k)\n",
    "    \n",
    "    # Feature terms\n",
    "    Theta_snapshot = np.vstack([\n",
    "            uxx,         # diffusion term\n",
    "            uxxxx,       # hyperdiffusion\n",
    "            snapshot * ux, # nonlinear advection term (u * ux)\n",
    "        ]).T  # shape (Nx, 3)\n",
    "    \n",
    "    Theta.append(Theta_snapshot)\n",
    "    ut_flat.append(ut_snapshot)\n",
    "    \n",
    "Theta = np.vstack(Theta)       # shape (Nt*Nx, 3)\n",
    "ut_flat = np.hstack(ut_flat)   # flatten to 1D (Nt*Nx,)\n",
    "    \n",
    "print(\"Feature matrix Θ shape:\", Theta.shape)\n",
    "print(\"Target vector u_t shape:\", ut_flat.shape)\n",
    "    \n",
    "# === Normalize features for numerical stability ===\n",
    "Theta_mean = Theta.mean(axis=0)\n",
    "Theta_std = Theta.std(axis=0)\n",
    "Theta_norm = (Theta - Theta_mean) / Theta_std\n",
    "    \n",
    "# === Sparse regression (LASSO) ===\n",
    "lasso = Lasso(alpha=1e-4, fit_intercept=False)\n",
    "lasso.fit(Theta_norm, ut_flat)\n",
    "coeffs = lasso.coef_ / Theta_std  # un-normalize coefficients\n",
    "    \n",
    "print(\"\\nRecovered PDE coefficients (Ks clean):\")\n",
    "print(f\"a (u_xx):     {coeffs[0]:.4f}\")\n",
    "print(f\"b (u_xxxx):   {coeffs[1]:.4f}\")\n",
    "print(f\"c (u*u_x):    {coeffs[2]:.4f}\") # Updated label for the new term\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6362ae",
   "metadata": {},
   "source": [
    "### FFT based SINDy on data 2 (20,000) steps clean ks data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cf4713",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"ks_data_dtt100.h5\", \"r\") as f:\n",
    "    u = np.array(f[\"u\"])      # shape (Nt, Nx)\n",
    "    x = np.array(f[\"x\"])      # spatial grid\n",
    "    t = np.array(f[\"t\"])      # time grid\n",
    "    \n",
    "Nx = len(x)\n",
    "Nt = len(t)\n",
    "L = x[-1] - x[0] + (x[1]-x[0])\n",
    "dx = x[1] - x[0]\n",
    "dt = t[1] - t[0]\n",
    "    \n",
    "print(f\"Loaded u(t,x) with shape {u.shape}, dx={dx:.3f}, dt={dt:.3f}\")\n",
    "    \n",
    "    \n",
    "# Define wavenumbers\n",
    "k = 2 * np.pi * fftfreq(Nx, d=dx)\n",
    "    \n",
    "# === Compute temporal derivative u_t using finite difference ===\n",
    "u_t = (u[2:] - u[:-2]) / (2 * dt)          # central difference in time\n",
    "u_mid = u[1:-1]                            # align time dimension\n",
    "    \n",
    "# === Build feature library Θ(u) ===\n",
    "# For each time snapshot, compute spatial derivatives\n",
    "Theta = []\n",
    "ut_flat = []\n",
    "    \n",
    "for snapshot, ut_snapshot in zip(u_mid, u_t):\n",
    "    ux  = spectral_derivative(snapshot, k)\n",
    "    uxx = spectral_second_derivative(snapshot, k)\n",
    "    uxxxx = spectral_fourth_derivative(snapshot, k)\n",
    "    \n",
    "    # Feature terms\n",
    "    Theta_snapshot = np.vstack([\n",
    "            uxx,         # diffusion term\n",
    "            uxxxx,       # hyperdiffusion\n",
    "            snapshot * ux, # nonlinear advection term (u * ux)\n",
    "        ]).T  # shape (Nx, 3)\n",
    "    \n",
    "    Theta.append(Theta_snapshot)\n",
    "    ut_flat.append(ut_snapshot)\n",
    "    \n",
    "Theta = np.vstack(Theta)       # shape (Nt*Nx, 3)\n",
    "ut_flat = np.hstack(ut_flat)   # flatten to 1D (Nt*Nx,)\n",
    "    \n",
    "print(\"Feature matrix Θ shape:\", Theta.shape)\n",
    "print(\"Target vector u_t shape:\", ut_flat.shape)\n",
    "    \n",
    "# === Normalize features for numerical stability ===\n",
    "Theta_mean = Theta.mean(axis=0)\n",
    "Theta_std = Theta.std(axis=0)\n",
    "Theta_norm = (Theta - Theta_mean) / Theta_std\n",
    "    \n",
    "# === Sparse regression (LASSO) ===\n",
    "lasso = Lasso(alpha=1e-4, fit_intercept=False)\n",
    "lasso.fit(Theta_norm, ut_flat)\n",
    "coeffs = lasso.coef_ / Theta_std  # un-normalize coefficients\n",
    "    \n",
    "print(\"\\nRecovered PDE coefficients (Ks clean):\")\n",
    "print(f\"a (u_xx):     {coeffs[0]:.4f}\")\n",
    "print(f\"b (u_xxxx):   {coeffs[1]:.4f}\")\n",
    "print(f\"c (u*u_x):    {coeffs[2]:.4f}\") # Updated label for the new term\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38661454",
   "metadata": {},
   "source": [
    "###### LASSO tries to minimize:\n",
    "‖u_t − Θξ‖² + α‖ξ‖₁\n",
    "\n",
    "\n",
    "But when Θ is unnormalized:\n",
    "\n",
    "the columns are correlated\n",
    "\n",
    "the magnitudes differ by orders of magnitude\n",
    "\n",
    "LASSO distributes penalty unevenly\n",
    "\n",
    "The optimizer ends up setting: a ≈ b ≈ c ≈ –0.96\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5043029b",
   "metadata": {},
   "source": [
    "###### We validated two SINDy pipelines. Code 1—using FFT derivatives and unnormalized regression—fails because spectral differentiation amplifies numerical noise and the feature matrix is ill-conditioned. This causes the three coefficients to collapse toward a common biased value around –0.96. Code 2 uses finite differences, normalization, subsampling, and a full SINDy library, which makes the regression well-conditioned. It recovers the correct KS coefficients with high R² and low RMSE. Therefore all noisy-data and denoising experiments use Code 2 as the reliable baseline.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a91478",
   "metadata": {},
   "source": [
    "### Finite Difference Derivatives sindy (periodic boundary conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc5e27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py, numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# ===== single baseline dataset =====\n",
    "DATA_FILE = \"ks_data_dtt100.h5\"   # dt = 0.1 (your best dataset)\n",
    "\n",
    "# ---- load ----\n",
    "with h5py.File(DATA_FILE, \"r\") as f:\n",
    "    u = np.array(f[\"u\"], dtype=np.float32)\n",
    "    x = np.array(f[\"x\"], dtype=np.float32)\n",
    "    t = np.array(f[\"t\"], dtype=np.float32)\n",
    "\n",
    "dx, dt = float(x[1] - x[0]), float(t[1] - t[0])\n",
    "print(f\"Loaded: u.shape={u.shape}, dx={dx:.4f}, dt={dt:.4f}\")\n",
    "\n",
    "# ---- finite difference operators ----\n",
    "def d1(U): return (np.roll(U,-1,1)-np.roll(U,1,1)) / (2*dx)\n",
    "def d2(U): return (np.roll(U,-1,1)-2*U+np.roll(U,1,1)) / (dx**2)\n",
    "def d4(U):\n",
    "    return (np.roll(U,-2,1)-4*np.roll(U,-1,1)+6*U-4*np.roll(U,1,1)+np.roll(U,2,1)) / (dx**4)\n",
    "\n",
    "# ---- derivatives ----\n",
    "U_t = (u[1:] - u[:-1]) / dt\n",
    "U   = u[:-1]\n",
    "U_x, U_xx, U_xxxx = d1(U), d2(U), d4(U)\n",
    "U2_x = d1(U**2)  # nonlinear (u^2)_x term\n",
    "\n",
    "# ---- flatten & sample ----\n",
    "N_total = U_t.size\n",
    "N_sample = 60000\n",
    "idx = np.random.choice(N_total, N_sample, replace=False)\n",
    "Theta = np.column_stack([\n",
    "    np.ones_like(U).ravel(),\n",
    "    U.ravel(),\n",
    "    U_x.ravel(),\n",
    "    U_xx.ravel(),\n",
    "    U_xxxx.ravel(),\n",
    "    U2_x.ravel()\n",
    "])[idx]\n",
    "y = U_t.ravel()[idx]\n",
    "names = [\"1\", \"u\", \"u_x\", \"u_xx\", \"u_xxxx\", \"(u^2)_x\"]\n",
    "\n",
    "# ---- normalize ----\n",
    "Theta_mean, Theta_std = Theta.mean(0), Theta.std(0) + 1e-8\n",
    "Theta_n = (Theta - Theta_mean) / Theta_std\n",
    "\n",
    "# ---- Lasso regression ----\n",
    "lasso = Lasso(alpha=1e-4, fit_intercept=False, max_iter=50000, tol=1e-6)\n",
    "lasso.fit(Theta_n, y)\n",
    "coef = lasso.coef_ / Theta_std  # unscale\n",
    "\n",
    "# ---- extract recovered coefficients ----\n",
    "a = coef[names.index(\"u_xx\")]\n",
    "b = coef[names.index(\"u_xxxx\")]\n",
    "c_phys = coef[names.index(\"(u^2)_x\")]\n",
    "\n",
    "# ---- true coefficients ----\n",
    "a_true, b_true, c_true = -1.0, -1.0, -0.5\n",
    "\n",
    "# ---- metrics ----\n",
    "Δa = a - a_true\n",
    "Δb = b - b_true\n",
    "Δc = c_phys - c_true\n",
    "\n",
    "rel_a = abs(Δa / a_true)\n",
    "rel_b = abs(Δb / b_true)\n",
    "rel_c = abs(Δc / c_true)\n",
    "\n",
    "L1_recov = abs(a) + abs(b) + abs(c_phys)\n",
    "L1_true  = abs(a_true) + abs(b_true) + abs(c_true)\n",
    "L1_err   = abs(L1_recov - L1_true)\n",
    "\n",
    "# ---- report ----\n",
    "print(\"\\n==================== PDE DISCOVERY RESULTS ====================\")\n",
    "print(\"Discovered PDE (Euler–SINDy form):\")\n",
    "for n, c in zip(names, coef):\n",
    "    print(f\"{n:>8s} : {c:+.4f}\")\n",
    "\n",
    "print(\"\\nRecovered KS coefficients:\")\n",
    "print(f\"a (u_xx term)   = {a:+.4f}\")\n",
    "print(f\"b (u_xxxx term) = {b:+.4f}\")\n",
    "print(f\"c ((u^2)_x term)= {c_phys:+.4f}\")\n",
    "\n",
    "print(\"\\n------------------ ERROR METRICS ------------------\")\n",
    "print(f\"Δa (error) = {Δa:+.5f} | Relative = {rel_a*100:.3f}%\")\n",
    "print(f\"Δb (error) = {Δb:+.5f} | Relative = {rel_b*100:.3f}%\")\n",
    "print(f\"Δc (error) = {Δc:+.5f} | Relative = {rel_c*100:.3f}%\")\n",
    "print(f\"L1 norm recovered = {L1_recov:.4f} (true = {L1_true:.4f}) | ΔL1 = {L1_err:.4f}\")\n",
    "print(\"===============================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5538349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# ===== Load data =====\n",
    "DATA_FILE = \"ks_data_dtt100.h5\"\n",
    "\n",
    "with h5py.File(DATA_FILE, \"r\") as f:\n",
    "    u = np.array(f[\"u\"], dtype=np.float32)\n",
    "    x = np.array(f[\"x\"], dtype=np.float32)\n",
    "    t = np.array(f[\"t\"], dtype=np.float32)\n",
    "\n",
    "dx = float(x[1] - x[0])\n",
    "dt = float(t[1] - t[0])\n",
    "print(f\"Loaded: u.shape={u.shape}, dx={dx:.4f}, dt={dt:.4f}\")\n",
    "\n",
    "# ===== Finite difference operators =====\n",
    "def d1(U): \n",
    "    return (np.roll(U, -1, 1) - np.roll(U, 1, 1)) / (2*dx)\n",
    "\n",
    "def d2(U): \n",
    "    return (np.roll(U, -1, 1) - 2*U + np.roll(U, 1, 1)) / (dx**2)\n",
    "\n",
    "def d4(U):\n",
    "    return (np.roll(U, -2, 1) - 4*np.roll(U, -1, 1) + 6*U - \n",
    "            4*np.roll(U, 1, 1) + np.roll(U, 2, 1)) / (dx**4)\n",
    "\n",
    "# ===== Compute derivatives =====\n",
    "U_t = (u[1:] - u[:-1]) / dt\n",
    "U = u[:-1]\n",
    "U_x = d1(U)\n",
    "U_xx = d2(U)\n",
    "U_xxxx = d4(U)\n",
    "U2_x = d1(U**2)  # (u²)_x term\n",
    "\n",
    "# ===== Build feature matrix (ALL data points) =====\n",
    "Theta = np.column_stack([\n",
    "    np.ones_like(U).ravel(),\n",
    "    U.ravel(),\n",
    "    U_x.ravel(),\n",
    "    U_xx.ravel(),\n",
    "    U_xxxx.ravel(),\n",
    "    U2_x.ravel()\n",
    "])\n",
    "y = U_t.ravel()\n",
    "names = [\"1\", \"u\", \"u_x\", \"u_xx\", \"u_xxxx\", \"(u^2)_x\"]\n",
    "\n",
    "print(f\"Using ALL data: {len(y):,} points\")\n",
    "\n",
    "# ===== Normalize features =====\n",
    "Theta_mean = Theta.mean(0)\n",
    "Theta_std = Theta.std(0) + 1e-8\n",
    "Theta_n = (Theta - Theta_mean) / Theta_std\n",
    "\n",
    "# ===== Lasso regression =====\n",
    "lasso = Lasso(alpha=1e-4, fit_intercept=False, max_iter=50000, tol=1e-6)\n",
    "lasso.fit(Theta_n, y)\n",
    "coef = lasso.coef_ / Theta_std  # un-normalize\n",
    "\n",
    "# ===== Extract KS coefficients =====\n",
    "a = coef[names.index(\"u_xx\")]\n",
    "b = coef[names.index(\"u_xxxx\")]\n",
    "c_phys = coef[names.index(\"(u^2)_x\")]\n",
    "\n",
    "# ===== True coefficients =====\n",
    "a_true, b_true, c_true = -1.0, -1.0, -0.5\n",
    "\n",
    "# ===== Compute errors =====\n",
    "Δa = a - a_true\n",
    "Δb = b - b_true\n",
    "Δc = c_phys - c_true\n",
    "\n",
    "rel_a = abs(Δa / a_true)\n",
    "rel_b = abs(Δb / b_true)\n",
    "rel_c = abs(Δc / c_true)\n",
    "\n",
    "L1_recov = abs(a) + abs(b) + abs(c_phys)\n",
    "L1_true = abs(a_true) + abs(b_true) + abs(c_true)\n",
    "L1_err = abs(L1_recov - L1_true)\n",
    "\n",
    "# ===== Print results =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PDE DISCOVERY RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"Discovered PDE (Euler-SINDy form):\")\n",
    "for n, c in zip(names, coef):\n",
    "    print(f\"{n:>8s} : {c:+.4f}\")\n",
    "\n",
    "print(\"\\nRecovered KS coefficients:\")\n",
    "print(f\"a (u_xx term)   = {a:+.4f}\")\n",
    "print(f\"b (u_xxxx term) = {b:+.4f}\")\n",
    "print(f\"c ((u^2)_x term)= {c_phys:+.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"ERROR METRICS\")\n",
    "print(\"-\"*60)\n",
    "print(f\"Δa (error) = {Δa:+.5f} | Relative = {rel_a*100:.3f}%\")\n",
    "print(f\"Δb (error) = {Δb:+.5f} | Relative = {rel_b*100:.3f}%\")\n",
    "print(f\"Δc (error) = {Δc:+.5f} | Relative = {rel_c*100:.3f}%\")\n",
    "print(f\"L1 norm: {L1_recov:.4f} (true: {L1_true:.4f}) | ΔL1 = {L1_err:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f715ae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_solving_euler_ks_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b863d80",
   "metadata": {},
   "source": [
    "Finite Difference Derivatives sindy (periodic boundary conditions) on 2000 step clean ks data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3a00ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# ===== Load data =====\n",
    "DATA_FILE = \"test_solving_euler_ks_clean.h5\"\n",
    "\n",
    "with h5py.File(DATA_FILE, \"r\") as f:\n",
    "    u = np.array(f[\"u\"], dtype=np.float32)\n",
    "    x = np.array(f[\"x\"], dtype=np.float32)\n",
    "    t = np.array(f[\"t\"], dtype=np.float32)\n",
    "\n",
    "dx = float(x[1] - x[0])\n",
    "dt = float(t[1] - t[0])\n",
    "print(f\"Loaded: u.shape={u.shape}, dx={dx:.4f}, dt={dt:.4f}\")\n",
    "\n",
    "# ===== Finite difference operators =====\n",
    "def d1(U): \n",
    "    return (np.roll(U, -1, 1) - np.roll(U, 1, 1)) / (2*dx)\n",
    "\n",
    "def d2(U): \n",
    "    return (np.roll(U, -1, 1) - 2*U + np.roll(U, 1, 1)) / (dx**2)\n",
    "\n",
    "def d4(U):\n",
    "    return (np.roll(U, -2, 1) - 4*np.roll(U, -1, 1) + 6*U - \n",
    "            4*np.roll(U, 1, 1) + np.roll(U, 2, 1)) / (dx**4)\n",
    "\n",
    "# ===== Compute derivatives =====\n",
    "U_t = (u[1:] - u[:-1]) / dt\n",
    "U = u[:-1]\n",
    "U_x = d1(U)\n",
    "U_xx = d2(U)\n",
    "U_xxxx = d4(U)\n",
    "U2_x = d1(U**2)  # (u²)_x term\n",
    "\n",
    "# ===== Build feature matrix (ALL data points) =====\n",
    "Theta = np.column_stack([\n",
    "    np.ones_like(U).ravel(),\n",
    "    U.ravel(),\n",
    "    U_x.ravel(),\n",
    "    U_xx.ravel(),\n",
    "    U_xxxx.ravel(),\n",
    "    U2_x.ravel()\n",
    "])\n",
    "y = U_t.ravel()\n",
    "names = [\"1\", \"u\", \"u_x\", \"u_xx\", \"u_xxxx\", \"(u^2)_x\"]\n",
    "\n",
    "print(f\"Using ALL data: {len(y):,} points\")\n",
    "\n",
    "# ===== Normalize features =====\n",
    "Theta_mean = Theta.mean(0)\n",
    "Theta_std = Theta.std(0) + 1e-8\n",
    "Theta_n = (Theta - Theta_mean) / Theta_std\n",
    "\n",
    "# ===== Lasso regression =====\n",
    "lasso = Lasso(alpha=1e-4, fit_intercept=False, max_iter=50000, tol=1e-6)\n",
    "lasso.fit(Theta_n, y)\n",
    "coef = lasso.coef_ / Theta_std  # un-normalize\n",
    "\n",
    "# ===== Extract KS coefficients =====\n",
    "a = coef[names.index(\"u_xx\")]\n",
    "b = coef[names.index(\"u_xxxx\")]\n",
    "c_phys = coef[names.index(\"(u^2)_x\")]\n",
    "\n",
    "# ===== True coefficients =====\n",
    "a_true, b_true, c_true = -1.0, -1.0, -0.5\n",
    "\n",
    "# ===== Compute errors =====\n",
    "Δa = a - a_true\n",
    "Δb = b - b_true\n",
    "Δc = c_phys - c_true\n",
    "\n",
    "rel_a = abs(Δa / a_true)\n",
    "rel_b = abs(Δb / b_true)\n",
    "rel_c = abs(Δc / c_true)\n",
    "\n",
    "L1_recov = abs(a) + abs(b) + abs(c_phys)\n",
    "L1_true = abs(a_true) + abs(b_true) + abs(c_true)\n",
    "L1_err = abs(L1_recov - L1_true)\n",
    "\n",
    "# ===== Print results =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PDE DISCOVERY RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"Discovered PDE (Euler-SINDy form):\")\n",
    "for n, c in zip(names, coef):\n",
    "    print(f\"{n:>8s} : {c:+.4f}\")\n",
    "\n",
    "print(\"\\nRecovered KS coefficients:\")\n",
    "print(f\"a (u_xx term)   = {a:+.4f}\")\n",
    "print(f\"b (u_xxxx term) = {b:+.4f}\")\n",
    "print(f\"c ((u^2)_x term)= {c_phys:+.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"ERROR METRICS\")\n",
    "print(\"-\"*60)\n",
    "print(f\"Δa (error) = {Δa:+.5f} | Relative = {rel_a*100:.3f}%\")\n",
    "print(f\"Δb (error) = {Δb:+.5f} | Relative = {rel_b*100:.3f}%\")\n",
    "print(f\"Δc (error) = {Δc:+.5f} | Relative = {rel_c*100:.3f}%\")\n",
    "print(f\"L1 norm: {L1_recov:.4f} (true: {L1_true:.4f}) | ΔL1 = {L1_err:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb60c07",
   "metadata": {},
   "source": [
    "Finite Difference Derivatives sindy on 2000 step spatial shift data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abff580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# ===== Load data =====\n",
    "DATA_FILE = \"test_solving_euler_ks_spatial_shift.h5\"\n",
    "\n",
    "with h5py.File(DATA_FILE, \"r\") as f:\n",
    "    u = np.array(f[\"u\"], dtype=np.float32)\n",
    "    x = np.array(f[\"x\"], dtype=np.float32)\n",
    "    t = np.array(f[\"t\"], dtype=np.float32)\n",
    "\n",
    "dx = float(x[1] - x[0])\n",
    "dt = float(t[1] - t[0])\n",
    "print(f\"Loaded: u.shape={u.shape}, dx={dx:.4f}, dt={dt:.4f}\")\n",
    "\n",
    "# ===== Finite difference operators =====\n",
    "def d1(U): \n",
    "    return (np.roll(U, -1, 1) - np.roll(U, 1, 1)) / (2*dx)\n",
    "\n",
    "def d2(U): \n",
    "    return (np.roll(U, -1, 1) - 2*U + np.roll(U, 1, 1)) / (dx**2)\n",
    "\n",
    "def d4(U):\n",
    "    return (np.roll(U, -2, 1) - 4*np.roll(U, -1, 1) + 6*U - \n",
    "            4*np.roll(U, 1, 1) + np.roll(U, 2, 1)) / (dx**4)\n",
    "\n",
    "# ===== Compute derivatives =====\n",
    "U_t = (u[1:] - u[:-1]) / dt\n",
    "U = u[:-1]\n",
    "U_x = d1(U)\n",
    "U_xx = d2(U)\n",
    "U_xxxx = d4(U)\n",
    "U2_x = d1(U**2)  # (u²)_x term\n",
    "\n",
    "# ===== Build feature matrix (ALL data points) =====\n",
    "Theta = np.column_stack([\n",
    "    np.ones_like(U).ravel(),\n",
    "    U.ravel(),\n",
    "    U_x.ravel(),\n",
    "    U_xx.ravel(),\n",
    "    U_xxxx.ravel(),\n",
    "    U2_x.ravel()\n",
    "])\n",
    "y = U_t.ravel()\n",
    "names = [\"1\", \"u\", \"u_x\", \"u_xx\", \"u_xxxx\", \"(u^2)_x\"]\n",
    "\n",
    "print(f\"Using ALL data: {len(y):,} points\")\n",
    "\n",
    "# ===== Normalize features =====\n",
    "Theta_mean = Theta.mean(0)\n",
    "Theta_std = Theta.std(0) + 1e-8\n",
    "Theta_n = (Theta - Theta_mean) / Theta_std\n",
    "\n",
    "# ===== Lasso regression =====\n",
    "lasso = Lasso(alpha=1e-4, fit_intercept=False, max_iter=50000, tol=1e-6)\n",
    "lasso.fit(Theta_n, y)\n",
    "coef = lasso.coef_ / Theta_std  # un-normalize\n",
    "\n",
    "# ===== Extract KS coefficients =====\n",
    "a = coef[names.index(\"u_xx\")]\n",
    "b = coef[names.index(\"u_xxxx\")]\n",
    "c_phys = coef[names.index(\"(u^2)_x\")]\n",
    "\n",
    "# ===== True coefficients =====\n",
    "a_true, b_true, c_true = -1.0, -1.0, -0.5\n",
    "\n",
    "# ===== Compute errors =====\n",
    "Δa = a - a_true\n",
    "Δb = b - b_true\n",
    "Δc = c_phys - c_true\n",
    "\n",
    "rel_a = abs(Δa / a_true)\n",
    "rel_b = abs(Δb / b_true)\n",
    "rel_c = abs(Δc / c_true)\n",
    "\n",
    "L1_recov = abs(a) + abs(b) + abs(c_phys)\n",
    "L1_true = abs(a_true) + abs(b_true) + abs(c_true)\n",
    "L1_err = abs(L1_recov - L1_true)\n",
    "\n",
    "# ===== Print results =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PDE DISCOVERY RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"Discovered PDE (Euler-SINDy form):\")\n",
    "for n, c in zip(names, coef):\n",
    "    print(f\"{n:>8s} : {c:+.4f}\")\n",
    "\n",
    "print(\"\\nRecovered KS coefficients:\")\n",
    "print(f\"a (u_xx term)   = {a:+.4f}\")\n",
    "print(f\"b (u_xxxx term) = {b:+.4f}\")\n",
    "print(f\"c ((u^2)_x term)= {c_phys:+.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"ERROR METRICS\")\n",
    "print(\"-\"*60)\n",
    "print(f\"Δa (error) = {Δa:+.5f} | Relative = {rel_a*100:.3f}%\")\n",
    "print(f\"Δb (error) = {Δb:+.5f} | Relative = {rel_b*100:.3f}%\")\n",
    "print(f\"Δc (error) = {Δc:+.5f} | Relative = {rel_c*100:.3f}%\")\n",
    "print(f\"L1 norm: {L1_recov:.4f} (true: {L1_true:.4f}) | ΔL1 = {L1_err:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6508b6",
   "metadata": {},
   "source": [
    "### Finite Difference Derivatives SINDY on data with different initial conditions (2000 step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf71279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# ===== Load data =====\n",
    "DATA_FILE = \"test_solving_euler_ks_varied_ic.h5\"\n",
    "\n",
    "with h5py.File(DATA_FILE, \"r\") as f:\n",
    "    u = np.array(f[\"u\"], dtype=np.float32)\n",
    "    x = np.array(f[\"x\"], dtype=np.float32)\n",
    "    t = np.array(f[\"t\"], dtype=np.float32)\n",
    "\n",
    "dx = float(x[1] - x[0])\n",
    "dt = float(t[1] - t[0])\n",
    "print(f\"Loaded: u.shape={u.shape}, dx={dx:.4f}, dt={dt:.4f}\")\n",
    "\n",
    "# ===== Finite difference operators =====\n",
    "def d1(U): \n",
    "    return (np.roll(U, -1, 1) - np.roll(U, 1, 1)) / (2*dx)\n",
    "\n",
    "def d2(U): \n",
    "    return (np.roll(U, -1, 1) - 2*U + np.roll(U, 1, 1)) / (dx**2)\n",
    "\n",
    "def d4(U):\n",
    "    return (np.roll(U, -2, 1) - 4*np.roll(U, -1, 1) + 6*U - \n",
    "            4*np.roll(U, 1, 1) + np.roll(U, 2, 1)) / (dx**4)\n",
    "\n",
    "# ===== Compute derivatives =====\n",
    "U_t = (u[1:] - u[:-1]) / dt\n",
    "U = u[:-1]\n",
    "U_x = d1(U)\n",
    "U_xx = d2(U)\n",
    "U_xxxx = d4(U)\n",
    "U2_x = d1(U**2)  # (u²)_x term\n",
    "\n",
    "# ===== Build feature matrix (ALL data points) =====\n",
    "Theta = np.column_stack([\n",
    "    np.ones_like(U).ravel(),\n",
    "    U.ravel(),\n",
    "    U_x.ravel(),\n",
    "    U_xx.ravel(),\n",
    "    U_xxxx.ravel(),\n",
    "    U2_x.ravel()\n",
    "])\n",
    "y = U_t.ravel()\n",
    "names = [\"1\", \"u\", \"u_x\", \"u_xx\", \"u_xxxx\", \"(u^2)_x\"]\n",
    "\n",
    "print(f\"Using ALL data: {len(y):,} points\")\n",
    "\n",
    "# ===== Normalize features =====\n",
    "Theta_mean = Theta.mean(0)\n",
    "Theta_std = Theta.std(0) + 1e-8\n",
    "Theta_n = (Theta - Theta_mean) / Theta_std\n",
    "\n",
    "# ===== Lasso regression =====\n",
    "lasso = Lasso(alpha=1e-4, fit_intercept=False, max_iter=50000, tol=1e-6)\n",
    "lasso.fit(Theta_n, y)\n",
    "coef = lasso.coef_ / Theta_std  # un-normalize\n",
    "\n",
    "# ===== Extract KS coefficients =====\n",
    "a = coef[names.index(\"u_xx\")]\n",
    "b = coef[names.index(\"u_xxxx\")]\n",
    "c_phys = coef[names.index(\"(u^2)_x\")]\n",
    "\n",
    "# ===== True coefficients =====\n",
    "a_true, b_true, c_true = -1.0, -1.0, -0.5\n",
    "\n",
    "# ===== Compute errors =====\n",
    "Δa = a - a_true\n",
    "Δb = b - b_true\n",
    "Δc = c_phys - c_true\n",
    "\n",
    "rel_a = abs(Δa / a_true)\n",
    "rel_b = abs(Δb / b_true)\n",
    "rel_c = abs(Δc / c_true)\n",
    "\n",
    "L1_recov = abs(a) + abs(b) + abs(c_phys)\n",
    "L1_true = abs(a_true) + abs(b_true) + abs(c_true)\n",
    "L1_err = abs(L1_recov - L1_true)\n",
    "\n",
    "# ===== Print results =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PDE DISCOVERY RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"Discovered PDE (Euler-SINDy form):\")\n",
    "for n, c in zip(names, coef):\n",
    "    print(f\"{n:>8s} : {c:+.4f}\")\n",
    "\n",
    "print(\"\\nRecovered KS coefficients:\")\n",
    "print(f\"a (u_xx term)   = {a:+.4f}\")\n",
    "print(f\"b (u_xxxx term) = {b:+.4f}\")\n",
    "print(f\"c ((u^2)_x term)= {c_phys:+.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"ERROR METRICS\")\n",
    "print(\"-\"*60)\n",
    "print(f\"Δa (error) = {Δa:+.5f} | Relative = {rel_a*100:.3f}%\")\n",
    "print(f\"Δb (error) = {Δb:+.5f} | Relative = {rel_b*100:.3f}%\")\n",
    "print(f\"Δc (error) = {Δc:+.5f} | Relative = {rel_c*100:.3f}%\")\n",
    "print(f\"L1 norm: {L1_recov:.4f} (true: {L1_true:.4f}) | ΔL1 = {L1_err:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ca93b5",
   "metadata": {},
   "source": [
    "Generate Additive Gaussian Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5100ac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "CLEAN_FILE = \"ks_data_dtt100.h5\"      # or \"test_solving_euler_ks_clean.h5\"\n",
    "SIGMAS = [0.005, 0.01, 0.03, 0.05, 0.08, 0.10]   # noise levels you want\n",
    "OUT_PREFIX = \"ks20k_gaussian_sigma_\"     # output file prefix\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# LOAD CLEAN REGISTERED KS DATA\n",
    "# =====================================================\n",
    "\n",
    "with h5py.File(CLEAN_FILE, \"r\") as f:\n",
    "    u_clean = np.array(f[\"u\"], dtype=np.float32)\n",
    "    x = np.array(f[\"x\"], dtype=np.float32)\n",
    "    t = np.array(f[\"t\"], dtype=np.float32)\n",
    "    attrs = dict(f.attrs)\n",
    "\n",
    "print(f\"Loaded clean dataset: u.shape = {u_clean.shape}\")\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# FUNCTION: Add Gaussian Noise\n",
    "# =====================================================\n",
    "\n",
    "def add_gaussian_noise(u, sigma):\n",
    "    \"\"\"\n",
    "    Add Gaussian noise with zero mean and std = sigma.\n",
    "    Noise is added independently to every pixel.\n",
    "    \"\"\"\n",
    "    noise = sigma * np.random.randn(*u.shape)\n",
    "    return u + noise\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# GENERATE & SAVE NOISY DATASETS\n",
    "# =====================================================\n",
    "\n",
    "for sigma in SIGMAS:\n",
    "    print(f\"Generating σ = {sigma} dataset...\")\n",
    "\n",
    "    u_noisy = add_gaussian_noise(u_clean, sigma)\n",
    "\n",
    "    out_filename = f\"{OUT_PREFIX}{sigma:.3f}.h5\"\n",
    "    \n",
    "    with h5py.File(out_filename, \"w\") as f:\n",
    "        f.create_dataset(\"u\", data=u_noisy)\n",
    "        f.create_dataset(\"x\", data=x)\n",
    "        f.create_dataset(\"t\", data=t)\n",
    "\n",
    "        # store metadata\n",
    "        for k, v in attrs.items():\n",
    "            f.attrs[k] = v\n",
    "        f.attrs[\"noise_type\"] = \"gaussian\"\n",
    "        f.attrs[\"sigma\"] = sigma\n",
    "    \n",
    "    print(f\"Saved: {out_filename}\")\n",
    "\n",
    "print(\"\\nAll noisy datasets generated successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276fa22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "CLEAN_FILE = \"test_solving_euler_ks_clean.h5\"      # or \"test_solving_euler_ks_clean.h5\"\n",
    "SIGMAS = [0.005, 0.01, 0.03, 0.05, 0.08, 0.10]   # noise levels you want\n",
    "OUT_PREFIX = \"ks_gaussian_sigma_\"     # output file prefix\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# LOAD CLEAN REGISTERED KS DATA\n",
    "# =====================================================\n",
    "\n",
    "with h5py.File(CLEAN_FILE, \"r\") as f:\n",
    "    u_clean = np.array(f[\"u\"], dtype=np.float32)\n",
    "    x = np.array(f[\"x\"], dtype=np.float32)\n",
    "    t = np.array(f[\"t\"], dtype=np.float32)\n",
    "    attrs = dict(f.attrs)\n",
    "\n",
    "print(f\"Loaded clean dataset: u.shape = {u_clean.shape}\")\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# FUNCTION: Add Gaussian Noise\n",
    "# =====================================================\n",
    "\n",
    "def add_gaussian_noise(u, sigma):\n",
    "    \"\"\"\n",
    "    Add Gaussian noise with zero mean and std = sigma.\n",
    "    Noise is added independently to every pixel.\n",
    "    \"\"\"\n",
    "    noise = sigma * np.random.randn(*u.shape)\n",
    "    return u + noise\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# GENERATE & SAVE NOISY DATASETS\n",
    "# =====================================================\n",
    "\n",
    "for sigma in SIGMAS:\n",
    "    print(f\"Generating σ = {sigma} dataset...\")\n",
    "\n",
    "    u_noisy = add_gaussian_noise(u_clean, sigma)\n",
    "\n",
    "    out_filename = f\"{OUT_PREFIX}{sigma:.3f}.h5\"\n",
    "    \n",
    "    with h5py.File(out_filename, \"w\") as f:\n",
    "        f.create_dataset(\"u\", data=u_noisy)\n",
    "        f.create_dataset(\"x\", data=x)\n",
    "        f.create_dataset(\"t\", data=t)\n",
    "\n",
    "        # store metadata\n",
    "        for k, v in attrs.items():\n",
    "            f.attrs[k] = v\n",
    "        f.attrs[\"noise_type\"] = \"gaussian\"\n",
    "        f.attrs[\"sigma\"] = sigma\n",
    "    \n",
    "    print(f\"Saved: {out_filename}\")\n",
    "\n",
    "print(\"\\nAll noisy datasets generated successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdee22a",
   "metadata": {},
   "source": [
    "Finite diff deriavative on additive noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e02e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "\n",
    "# ===== Evaluation Metrics Functions =====\n",
    "def compute_rmse(y_true, y_pred):\n",
    "    \"\"\"Root Mean Squared Error\"\"\"\n",
    "    return np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "\n",
    "def compute_r2(y_true, y_pred):\n",
    "    \"\"\"R-squared (coefficient of determination)\"\"\"\n",
    "    return r2_score(y_true, y_pred)\n",
    "\n",
    "def compute_l1_norm(coeffs):\n",
    "    \"\"\"L1 norm of coefficient vector\"\"\"\n",
    "    return np.sum(np.abs(coeffs))\n",
    "\n",
    "def compute_relative_error(true_val, pred_val):\n",
    "    \"\"\"Relative error in percentage\"\"\"\n",
    "    if true_val == 0:\n",
    "        return 0.0 if pred_val == 0 else 100.0\n",
    "    return abs((pred_val - true_val) / true_val) * 100\n",
    "\n",
    "def compute_f1_score(true_coeffs, pred_coeffs, threshold=1e-3):\n",
    "    \"\"\"\n",
    "    F1 score for term selection stability\n",
    "    Treats coefficients above threshold as 'selected'\n",
    "    \"\"\"\n",
    "    true_selected = np.abs(true_coeffs) > threshold\n",
    "    pred_selected = np.abs(pred_coeffs) > threshold\n",
    "    \n",
    "    # True Positives, False Positives, False Negatives\n",
    "    TP = np.sum(true_selected & pred_selected)\n",
    "    FP = np.sum(~true_selected & pred_selected)\n",
    "    FN = np.sum(true_selected & ~pred_selected)\n",
    "    \n",
    "    # Precision and Recall\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    \n",
    "    # F1 Score\n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1 = 0.0\n",
    "    \n",
    "    return f1\n",
    "\n",
    "\n",
    "# ===== List of noise datasets =====\n",
    "noise_files = [\n",
    "    \"test_solving_euler_ks_clean.h5\",              # Clean (no noise)\n",
    "    \"ks_gaussian_sigma_0.005.h5\",\n",
    "    \"ks_gaussian_sigma_0.010.h5\",\n",
    "    \"ks_gaussian_sigma_0.030.h5\",\n",
    "    \"ks_gaussian_sigma_0.050.h5\",\n",
    "    \"ks_gaussian_sigma_0.080.h5\",\n",
    "    \"ks_gaussian_sigma_0.100.h5\"\n",
    "]\n",
    "\n",
    "noise_levels = [0.0, 0.005, 0.01, 0.03, 0.05, 0.08, 0.1]\n",
    "\n",
    "# ===== Store results =====\n",
    "results = []\n",
    "\n",
    "# ===== True coefficients =====\n",
    "a_true, b_true, c_true = -1.0, -1.0, -0.5\n",
    "true_coeffs = np.array([a_true, b_true, c_true])\n",
    "\n",
    "# ===== Loop through each dataset =====\n",
    "for i, (file, sigma) in enumerate(zip(noise_files, noise_levels)):\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"TESTING NOISE LEVEL σ = {sigma} (FINITE DIFFERENCE)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # ---- Load data ----\n",
    "    with h5py.File(file, \"r\") as f:\n",
    "        u = np.array(f[\"u\"], dtype=np.float32)\n",
    "        x = np.array(f[\"x\"], dtype=np.float32)\n",
    "        t = np.array(f[\"t\"], dtype=np.float32)\n",
    "    \n",
    "    dx = float(x[1] - x[0])\n",
    "    dt = float(t[1] - t[0])\n",
    "    print(f\"Loaded: {file}\")\n",
    "    print(f\"Shape: u.shape={u.shape}, dx={dx:.4f}, dt={dt:.4f}\")\n",
    "    \n",
    "    # ---- Finite difference operators ----\n",
    "    def d1(U): \n",
    "        return (np.roll(U, -1, 1) - np.roll(U, 1, 1)) / (2*dx)\n",
    "    \n",
    "    def d2(U): \n",
    "        return (np.roll(U, -1, 1) - 2*U + np.roll(U, 1, 1)) / (dx**2)\n",
    "    \n",
    "    def d4(U):\n",
    "        return (np.roll(U, -2, 1) - 4*np.roll(U, -1, 1) + 6*U - \n",
    "                4*np.roll(U, 1, 1) + np.roll(U, 2, 1)) / (dx**4)\n",
    "    \n",
    "    # ---- Compute derivatives ----\n",
    "    U_t = (u[1:] - u[:-1]) / dt\n",
    "    U = u[:-1]\n",
    "    U_x = d1(U)\n",
    "    U_xx = d2(U)\n",
    "    U_xxxx = d4(U)\n",
    "    U2_x = d1(U**2)\n",
    "    \n",
    "    # ---- Build feature matrix ----\n",
    "    Theta = np.column_stack([\n",
    "        np.ones_like(U).ravel(),\n",
    "        U.ravel(),\n",
    "        U_x.ravel(),\n",
    "        U_xx.ravel(),\n",
    "        U_xxxx.ravel(),\n",
    "        U2_x.ravel()\n",
    "    ])\n",
    "    y = U_t.ravel()\n",
    "    names = [\"1\", \"u\", \"u_x\", \"u_xx\", \"u_xxxx\", \"(u^2)_x\"]\n",
    "    \n",
    "    print(f\"Feature matrix Θ shape: {Theta.shape}\")\n",
    "    print(f\"Target vector u_t shape: {y.shape}\")\n",
    "    print(f\"Using all data: {len(y):,} points\")\n",
    "    \n",
    "    # ---- Normalize ----\n",
    "    Theta_mean = Theta.mean(0)\n",
    "    Theta_std = Theta.std(0) + 1e-8\n",
    "    Theta_n = (Theta - Theta_mean) / Theta_std\n",
    "    \n",
    "    # ---- Lasso regression ----\n",
    "    lasso = Lasso(alpha=1e-4, fit_intercept=False, max_iter=50000, tol=1e-6)\n",
    "    lasso.fit(Theta_n, y)\n",
    "    coef = lasso.coef_ / Theta_std\n",
    "    \n",
    "    # ---- Extract coefficients ----\n",
    "    a = coef[names.index(\"u_xx\")]\n",
    "    b = coef[names.index(\"u_xxxx\")]\n",
    "    c_phys = coef[names.index(\"(u^2)_x\")]\n",
    "    \n",
    "    pred_coeffs = np.array([a, b, c_phys])\n",
    "    \n",
    "    # ---- Predict u_t using recovered coefficients ----\n",
    "    y_pred = Theta @ coef\n",
    "    \n",
    "    # ==================== COMPUTE ALL METRICS ====================\n",
    "    \n",
    "    # 1. RMSE\n",
    "    rmse = compute_rmse(y, y_pred)\n",
    "    \n",
    "    # 2. R² Score\n",
    "    r2 = compute_r2(y, y_pred)\n",
    "    \n",
    "    # 3. L1 Norm\n",
    "    l1_recovered = compute_l1_norm(pred_coeffs)\n",
    "    l1_true = compute_l1_norm(true_coeffs)\n",
    "    l1_error = abs(l1_recovered - l1_true)\n",
    "    \n",
    "    # 4. Relative Coefficient Errors\n",
    "    rel_a = compute_relative_error(a_true, a)\n",
    "    rel_b = compute_relative_error(b_true, b)\n",
    "    rel_c = compute_relative_error(c_true, c_phys)\n",
    "    mean_rel_error = (rel_a + rel_b + rel_c) / 3\n",
    "    \n",
    "    # 5. F1 Score (term selection stability)\n",
    "    f1 = compute_f1_score(true_coeffs, pred_coeffs, threshold=1e-3)\n",
    "    \n",
    "    # 6. Individual coefficient errors (absolute)\n",
    "    abs_error_a = abs(a - a_true)\n",
    "    abs_error_b = abs(b - b_true)\n",
    "    abs_error_c = abs(c_phys - c_true)\n",
    "    \n",
    "    # ---- Store results ----\n",
    "    results.append({\n",
    "        'sigma': sigma,\n",
    "        'file': file,\n",
    "        # Recovered coefficients\n",
    "        'a': a,\n",
    "        'b': b,\n",
    "        'c': c_phys,\n",
    "        # Relative errors per coefficient\n",
    "        'rel_a': rel_a,\n",
    "        'rel_b': rel_b,\n",
    "        'rel_c': rel_c,\n",
    "        # Absolute errors per coefficient\n",
    "        'abs_error_a': abs_error_a,\n",
    "        'abs_error_b': abs_error_b,\n",
    "        'abs_error_c': abs_error_c,\n",
    "        # Summary metrics\n",
    "        'mean_rel_error': mean_rel_error,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2,\n",
    "        'l1_recovered': l1_recovered,\n",
    "        'l1_true': l1_true,\n",
    "        'l1_error': l1_error,\n",
    "        'f1_score': f1\n",
    "    })\n",
    "    \n",
    "    # ---- Print results ----\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"RECOVERED PDE COEFFICIENTS:\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"  a (u_xx):   {a:+.6f}  (true: {a_true:+.2f}) | Abs Error: {abs_error_a:.6f} | Rel Error: {rel_a:.2f}%\")\n",
    "    print(f\"  b (u_xxxx): {b:+.6f}  (true: {b_true:+.2f}) | Abs Error: {abs_error_b:.6f} | Rel Error: {rel_b:.2f}%\")\n",
    "    print(f\"  c ((u²)_x): {c_phys:+.6f}  (true: {c_true:+.2f}) | Abs Error: {abs_error_c:.6f} | Rel Error: {rel_c:.2f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"EVALUATION METRICS:\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"  RMSE:                {rmse:.6f}\")\n",
    "    print(f\"  R² Score:            {r2:.6f}\")\n",
    "    print(f\"  L1 Norm (recovered): {l1_recovered:.6f}\")\n",
    "    print(f\"  L1 Norm (true):      {l1_true:.6f}\")\n",
    "    print(f\"  L1 Error:            {l1_error:.6f}\")\n",
    "    print(f\"  Mean Rel Error:      {mean_rel_error:.2f}%\")\n",
    "    print(f\"  F1 Score:            {f1:.6f}\")\n",
    "\n",
    "\n",
    "# ===== SUMMARY TABLE 1: RECOVERED COEFFICIENTS =====\n",
    "print(\"\\n\\n\" + \"=\"*100)\n",
    "print(\"SUMMARY TABLE 1: RECOVERED PDE COEFFICIENTS (FINITE DIFFERENCE)\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Sigma':>8} {'a (u_xx)':>12} {'b (u_xxxx)':>12} {'c ((u²)_x)':>12} {'Rel Err a':>12} {'Rel Err b':>12} {'Rel Err c':>12}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "for r in results:\n",
    "    print(f\"{r['sigma']:>8.3f} {r['a']:>12.6f} {r['b']:>12.6f} {r['c']:>12.6f} \"\n",
    "          f\"{r['rel_a']:>11.2f}% {r['rel_b']:>11.2f}% {r['rel_c']:>11.2f}%\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(f\"{'TRUE:':>8} {a_true:>12.6f} {b_true:>12.6f} {c_true:>12.6f}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "\n",
    "# ===== SUMMARY TABLE 2: ALL EVALUATION METRICS =====\n",
    "print(\"\\n\\n\" + \"=\"*120)\n",
    "print(\"SUMMARY TABLE 2: COMPREHENSIVE EVALUATION METRICS (FINITE DIFFERENCE)\")\n",
    "print(\"=\"*120)\n",
    "print(f\"{'Sigma':>8} {'RMSE':>12} {'R²':>12} {'L1 Error':>12} {'Mean Rel Err':>15} {'F1 Score':>12}\")\n",
    "print(\"-\"*120)\n",
    "\n",
    "for r in results:\n",
    "    print(f\"{r['sigma']:>8.3f} {r['rmse']:>12.6f} {r['r2']:>12.6f} {r['l1_error']:>12.6f} \"\n",
    "          f\"{r['mean_rel_error']:>14.2f}% {r['f1_score']:>12.6f}\")\n",
    "\n",
    "print(\"=\"*120)\n",
    "\n",
    "\n",
    "# ===== SUMMARY TABLE 3: DETAILED COEFFICIENT ERRORS =====\n",
    "print(\"\\n\\n\" + \"=\"*120)\n",
    "print(\"SUMMARY TABLE 3: ABSOLUTE COEFFICIENT ERRORS (FINITE DIFFERENCE)\")\n",
    "print(\"=\"*120)\n",
    "print(f\"{'Sigma':>8} {'|Δa|':>12} {'|Δb|':>12} {'|Δc|':>12} {'Mean Abs Err':>15}\")\n",
    "print(\"-\"*120)\n",
    "\n",
    "for r in results:\n",
    "    mean_abs_err = (r['abs_error_a'] + r['abs_error_b'] + r['abs_error_c']) / 3\n",
    "    print(f\"{r['sigma']:>8.3f} {r['abs_error_a']:>12.6f} {r['abs_error_b']:>12.6f} {r['abs_error_c']:>12.6f} \"\n",
    "          f\"{mean_abs_err:>15.6f}\")\n",
    "\n",
    "print(\"=\"*120)\n",
    "\n",
    "\n",
    "# ===== FIND BEST RESULTS =====\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"BEST RESULTS (FINITE DIFFERENCE SINDY)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_rmse = min(results, key=lambda x: x['rmse'])\n",
    "best_r2 = max(results, key=lambda x: x['r2'])\n",
    "best_l1 = min(results, key=lambda x: x['l1_error'])\n",
    "best_rel = min(results, key=lambda x: x['mean_rel_error'])\n",
    "best_f1 = max(results, key=lambda x: x['f1_score'])\n",
    "\n",
    "print(f\"\\n    BEST RMSE:            σ = {best_rmse['sigma']:.3f}  |  RMSE = {best_rmse['rmse']:.6f}\")\n",
    "print(f\"    BEST R² Score:        σ = {best_r2['sigma']:.3f}  |  R² = {best_r2['r2']:.6f}\")\n",
    "print(f\"    BEST L1 Error:        σ = {best_l1['sigma']:.3f}  |  L1 Error = {best_l1['l1_error']:.6f}\")\n",
    "print(f\"    BEST Mean Rel Error:  σ = {best_rel['sigma']:.3f}  |  Mean Rel Error = {best_rel['mean_rel_error']:.2f}%\")\n",
    "print(f\"    BEST F1 Score:        σ = {best_f1['sigma']:.3f}  |  F1 Score = {best_f1['f1_score']:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"OVERALL BEST: σ = {best_rel['sigma']:.3f} (based on Mean Relative Error)\")\n",
    "print(f\"  Coefficients: a={best_rel['a']:.6f}, b={best_rel['b']:.6f}, c={best_rel['c']:.6f}\")\n",
    "print(f\"  Mean Relative Error: {best_rel['mean_rel_error']:.2f}%\")\n",
    "print(f\"  RMSE: {best_rel['rmse']:.6f}\")\n",
    "print(f\"  R²: {best_rel['r2']:.6f}\")\n",
    "print(f\"  F1 Score: {best_rel['f1_score']:.6f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "# ===== SAVE RESULTS TO CSV =====\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('finite_diff_sindy_results.csv', index=False)\n",
    "print(\"\\n    Results saved to: finite_diff_sindy_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da4d55",
   "metadata": {},
   "source": [
    "FFT sindy with additive noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32ac89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from scipy.fft import fft, ifft, fftfreq\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "\n",
    "# ===== Spectral derivative functions =====\n",
    "def spectral_derivative(u_snapshot, k):\n",
    "    \"\"\"First derivative using FFT\"\"\"\n",
    "    u_hat = fft(u_snapshot)\n",
    "    du_hat = 1j * k * u_hat\n",
    "    return np.real(ifft(du_hat))\n",
    "\n",
    "def spectral_second_derivative(u_snapshot, k):\n",
    "    \"\"\"Second derivative using FFT\"\"\"\n",
    "    u_hat = fft(u_snapshot)\n",
    "    d2u_hat = -(k**2) * u_hat\n",
    "    return np.real(ifft(d2u_hat))\n",
    "\n",
    "def spectral_fourth_derivative(u_snapshot, k):\n",
    "    \"\"\"Fourth derivative using FFT\"\"\"\n",
    "    u_hat = fft(u_snapshot)\n",
    "    d4u_hat = (k**4) * u_hat\n",
    "    return np.real(ifft(d4u_hat))\n",
    "\n",
    "\n",
    "# ===== Evaluation Metrics Functions =====\n",
    "def compute_rmse(y_true, y_pred):\n",
    "    \"\"\"Root Mean Squared Error\"\"\"\n",
    "    return np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "\n",
    "def compute_r2(y_true, y_pred):\n",
    "    \"\"\"R-squared (coefficient of determination)\"\"\"\n",
    "    return r2_score(y_true, y_pred)\n",
    "\n",
    "def compute_l1_norm(coeffs):\n",
    "    \"\"\"L1 norm of coefficient vector\"\"\"\n",
    "    return np.sum(np.abs(coeffs))\n",
    "\n",
    "def compute_relative_error(true_val, pred_val):\n",
    "    \"\"\"Relative error in percentage\"\"\"\n",
    "    if true_val == 0:\n",
    "        return 0.0 if pred_val == 0 else 100.0\n",
    "    return abs((pred_val - true_val) / true_val) * 100\n",
    "\n",
    "def compute_f1_score(true_coeffs, pred_coeffs, threshold=1e-3):\n",
    "    \"\"\"\n",
    "    F1 score for term selection stability\n",
    "    Treats coefficients above threshold as 'selected'\n",
    "    \"\"\"\n",
    "    true_selected = np.abs(true_coeffs) > threshold\n",
    "    pred_selected = np.abs(pred_coeffs) > threshold\n",
    "    \n",
    "    # True Positives, False Positives, False Negatives\n",
    "    TP = np.sum(true_selected & pred_selected)\n",
    "    FP = np.sum(~true_selected & pred_selected)\n",
    "    FN = np.sum(true_selected & ~pred_selected)\n",
    "    \n",
    "    # Precision and Recall\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    \n",
    "    # F1 Score\n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1 = 0.0\n",
    "    \n",
    "    return f1\n",
    "\n",
    "\n",
    "# ===== List of noise datasets =====\n",
    "noise_files = [\n",
    "    \"test_solving_euler_ks_clean.h5\",              # Clean (no noise)\n",
    "    \"ks_gaussian_sigma_0.005.h5\",\n",
    "    \"ks_gaussian_sigma_0.010.h5\",\n",
    "    \"ks_gaussian_sigma_0.030.h5\",\n",
    "    \"ks_gaussian_sigma_0.050.h5\",\n",
    "    \"ks_gaussian_sigma_0.080.h5\",\n",
    "    \"ks_gaussian_sigma_0.100.h5\"\n",
    "]\n",
    "\n",
    "noise_levels = [0.0, 0.005, 0.01, 0.03, 0.05, 0.08, 0.1]\n",
    "\n",
    "# ===== True coefficients (conservative form) =====\n",
    "a_true, b_true, c_true = -1.0, -1.0, -1.0\n",
    "true_coeffs = np.array([a_true, b_true, c_true])\n",
    "\n",
    "# ===== Store results =====\n",
    "results = []\n",
    "\n",
    "# ===== Loop through each dataset =====\n",
    "for file, sigma in zip(noise_files, noise_levels):\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"TESTING NOISE LEVEL σ = {sigma} (FFT/SPECTRAL)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # ---- Load data ----\n",
    "    with h5py.File(file, \"r\") as f:\n",
    "        u = np.array(f[\"u\"])\n",
    "        x = np.array(f[\"x\"])\n",
    "        t = np.array(f[\"t\"])\n",
    "    \n",
    "    Nx = len(x)\n",
    "    Nt = len(t)\n",
    "    L = x[-1] - x[0] + (x[1] - x[0])\n",
    "    dx = x[1] - x[0]\n",
    "    dt = t[1] - t[0]\n",
    "    \n",
    "    print(f\"Loaded: {file}\")\n",
    "    print(f\"Shape: u.shape={u.shape}, dx={dx:.3f}, dt={dt:.3f}\")\n",
    "    \n",
    "    # ---- Define wavenumbers ----\n",
    "    k = 2 * np.pi * fftfreq(Nx, d=dx)\n",
    "    \n",
    "    # ---- Compute temporal derivative (central difference) ----\n",
    "    u_t = (u[2:] - u[:-2]) / (2 * dt)\n",
    "    u_mid = u[1:-1]\n",
    "    \n",
    "    # ---- Build feature library using FFT ----\n",
    "    Theta = []\n",
    "    ut_flat = []\n",
    "    \n",
    "    for snapshot, ut_snapshot in zip(u_mid, u_t):\n",
    "        ux = spectral_derivative(snapshot, k)\n",
    "        uxx = spectral_second_derivative(snapshot, k)\n",
    "        uxxxx = spectral_fourth_derivative(snapshot, k)\n",
    "        \n",
    "        # Feature terms: [u_xx, u_xxxx, u*u_x]\n",
    "        Theta_snapshot = np.vstack([\n",
    "            uxx,\n",
    "            uxxxx,\n",
    "            snapshot * ux\n",
    "        ]).T  # shape (Nx, 3)\n",
    "        \n",
    "        Theta.append(Theta_snapshot)\n",
    "        ut_flat.append(ut_snapshot)\n",
    "    \n",
    "    Theta = np.vstack(Theta)\n",
    "    ut_flat = np.hstack(ut_flat)\n",
    "    \n",
    "    print(f\"Feature matrix Θ shape: {Theta.shape}\")\n",
    "    print(f\"Target vector u_t shape: {ut_flat.shape}\")\n",
    "    \n",
    "    # ---- Normalize features ----\n",
    "    Theta_mean = Theta.mean(axis=0)\n",
    "    Theta_std = Theta.std(axis=0)\n",
    "    Theta_norm = (Theta - Theta_mean) / Theta_std\n",
    "    \n",
    "    # ---- Lasso regression ----\n",
    "    lasso = Lasso(alpha=1e-4, fit_intercept=False, max_iter=50000)\n",
    "    lasso.fit(Theta_norm, ut_flat)\n",
    "    coeffs = lasso.coef_ / Theta_std\n",
    "    \n",
    "    # ---- Extract coefficients ----\n",
    "    a = coeffs[0]  # u_xx\n",
    "    b = coeffs[1]  # u_xxxx\n",
    "    c = coeffs[2]  # u*u_x\n",
    "    \n",
    "    pred_coeffs = np.array([a, b, c])\n",
    "    \n",
    "    # ---- Predict u_t using recovered coefficients ----\n",
    "    u_t_pred = Theta @ coeffs\n",
    "    \n",
    "    # ==================== COMPUTE ALL METRICS ====================\n",
    "    \n",
    "    # 1. RMSE\n",
    "    rmse = compute_rmse(ut_flat, u_t_pred)\n",
    "    \n",
    "    # 2. R² Score\n",
    "    r2 = compute_r2(ut_flat, u_t_pred)\n",
    "    \n",
    "    # 3. L1 Norm\n",
    "    l1_recovered = compute_l1_norm(pred_coeffs)\n",
    "    l1_true = compute_l1_norm(true_coeffs)\n",
    "    l1_error = abs(l1_recovered - l1_true)\n",
    "    \n",
    "    # 4. Relative Coefficient Errors\n",
    "    rel_a = compute_relative_error(a_true, a)\n",
    "    rel_b = compute_relative_error(b_true, b)\n",
    "    rel_c = compute_relative_error(c_true, c)\n",
    "    mean_rel_error = (rel_a + rel_b + rel_c) / 3\n",
    "    \n",
    "    # 5. F1 Score (term selection stability)\n",
    "    f1 = compute_f1_score(true_coeffs, pred_coeffs, threshold=1e-3)\n",
    "    \n",
    "    # 6. Individual coefficient errors (absolute)\n",
    "    abs_error_a = abs(a - a_true)\n",
    "    abs_error_b = abs(b - b_true)\n",
    "    abs_error_c = abs(c - c_true)\n",
    "    \n",
    "    # ---- Store results ----\n",
    "    results.append({\n",
    "        'sigma': sigma,\n",
    "        'file': file,\n",
    "        # Recovered coefficients\n",
    "        'a': a,\n",
    "        'b': b,\n",
    "        'c': c,\n",
    "        # Relative errors per coefficient\n",
    "        'rel_a': rel_a,\n",
    "        'rel_b': rel_b,\n",
    "        'rel_c': rel_c,\n",
    "        # Absolute errors per coefficient\n",
    "        'abs_error_a': abs_error_a,\n",
    "        'abs_error_b': abs_error_b,\n",
    "        'abs_error_c': abs_error_c,\n",
    "        # Summary metrics\n",
    "        'mean_rel_error': mean_rel_error,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2,\n",
    "        'l1_recovered': l1_recovered,\n",
    "        'l1_true': l1_true,\n",
    "        'l1_error': l1_error,\n",
    "        'f1_score': f1\n",
    "    })\n",
    "    \n",
    "    # ---- Print results ----\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"RECOVERED PDE COEFFICIENTS:\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"  a (u_xx):   {a:+.6f}  (true: {a_true:+.2f}) | Abs Error: {abs_error_a:.6f} | Rel Error: {rel_a:.2f}%\")\n",
    "    print(f\"  b (u_xxxx): {b:+.6f}  (true: {b_true:+.2f}) | Abs Error: {abs_error_b:.6f} | Rel Error: {rel_b:.2f}%\")\n",
    "    print(f\"  c (u*u_x):  {c:+.6f}  (true: {c_true:+.2f}) | Abs Error: {abs_error_c:.6f} | Rel Error: {rel_c:.2f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"EVALUATION METRICS:\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"  RMSE:                {rmse:.6f}\")\n",
    "    print(f\"  R² Score:            {r2:.6f}\")\n",
    "    print(f\"  L1 Norm (recovered): {l1_recovered:.6f}\")\n",
    "    print(f\"  L1 Norm (true):      {l1_true:.6f}\")\n",
    "    print(f\"  L1 Error:            {l1_error:.6f}\")\n",
    "    print(f\"  Mean Rel Error:      {mean_rel_error:.2f}%\")\n",
    "    print(f\"  F1 Score:            {f1:.6f}\")\n",
    "\n",
    "\n",
    "# ===== SUMMARY TABLE 1: RECOVERED COEFFICIENTS =====\n",
    "print(\"\\n\\n\" + \"=\"*100)\n",
    "print(\"SUMMARY TABLE 1: RECOVERED PDE COEFFICIENTS (FFT/SPECTRAL)\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Sigma':>8} {'a (u_xx)':>12} {'b (u_xxxx)':>12} {'c (u*u_x)':>12} {'Rel Err a':>12} {'Rel Err b':>12} {'Rel Err c':>12}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "for r in results:\n",
    "    print(f\"{r['sigma']:>8.3f} {r['a']:>12.6f} {r['b']:>12.6f} {r['c']:>12.6f} \"\n",
    "          f\"{r['rel_a']:>11.2f}% {r['rel_b']:>11.2f}% {r['rel_c']:>11.2f}%\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(f\"{'TRUE:':>8} {a_true:>12.6f} {b_true:>12.6f} {c_true:>12.6f}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "\n",
    "# ===== SUMMARY TABLE 2: ALL EVALUATION METRICS =====\n",
    "print(\"\\n\\n\" + \"=\"*120)\n",
    "print(\"SUMMARY TABLE 2: COMPREHENSIVE EVALUATION METRICS (FFT/SPECTRAL)\")\n",
    "print(\"=\"*120)\n",
    "print(f\"{'Sigma':>8} {'RMSE':>12} {'R²':>12} {'L1 Error':>12} {'Mean Rel Err':>15} {'F1 Score':>12}\")\n",
    "print(\"-\"*120)\n",
    "\n",
    "for r in results:\n",
    "    print(f\"{r['sigma']:>8.3f} {r['rmse']:>12.6f} {r['r2']:>12.6f} {r['l1_error']:>12.6f} \"\n",
    "          f\"{r['mean_rel_error']:>14.2f}% {r['f1_score']:>12.6f}\")\n",
    "\n",
    "print(\"=\"*120)\n",
    "\n",
    "\n",
    "# ===== SUMMARY TABLE 3: DETAILED COEFFICIENT ERRORS =====\n",
    "print(\"\\n\\n\" + \"=\"*120)\n",
    "print(\"SUMMARY TABLE 3: ABSOLUTE COEFFICIENT ERRORS (FFT/SPECTRAL)\")\n",
    "print(\"=\"*120)\n",
    "print(f\"{'Sigma':>8} {'|Δa|':>12} {'|Δb|':>12} {'|Δc|':>12} {'Mean Abs Err':>15}\")\n",
    "print(\"-\"*120)\n",
    "\n",
    "for r in results:\n",
    "    mean_abs_err = (r['abs_error_a'] + r['abs_error_b'] + r['abs_error_c']) / 3\n",
    "    print(f\"{r['sigma']:>8.3f} {r['abs_error_a']:>12.6f} {r['abs_error_b']:>12.6f} {r['abs_error_c']:>12.6f} \"\n",
    "          f\"{mean_abs_err:>15.6f}\")\n",
    "\n",
    "print(\"=\"*120)\n",
    "\n",
    "\n",
    "# ===== FIND BEST RESULTS =====\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"BEST RESULTS (FFT/SPECTRAL SINDY)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_rmse = min(results, key=lambda x: x['rmse'])\n",
    "best_r2 = max(results, key=lambda x: x['r2'])\n",
    "best_l1 = min(results, key=lambda x: x['l1_error'])\n",
    "best_rel = min(results, key=lambda x: x['mean_rel_error'])\n",
    "best_f1 = max(results, key=lambda x: x['f1_score'])\n",
    "\n",
    "print(f\"\\n    BEST RMSE:            σ = {best_rmse['sigma']:.3f}  |  RMSE = {best_rmse['rmse']:.6f}\")\n",
    "print(f\"    BEST R² Score:        σ = {best_r2['sigma']:.3f}  |  R² = {best_r2['r2']:.6f}\")\n",
    "print(f\"    BEST L1 Error:        σ = {best_l1['sigma']:.3f}  |  L1 Error = {best_l1['l1_error']:.6f}\")\n",
    "print(f\"    BEST Mean Rel Error:  σ = {best_rel['sigma']:.3f}  |  Mean Rel Error = {best_rel['mean_rel_error']:.2f}%\")\n",
    "print(f\"    BEST F1 Score:        σ = {best_f1['sigma']:.3f}  |  F1 Score = {best_f1['f1_score']:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"OVERALL BEST: σ = {best_rel['sigma']:.3f} (based on Mean Relative Error)\")\n",
    "print(f\"  Coefficients: a={best_rel['a']:.6f}, b={best_rel['b']:.6f}, c={best_rel['c']:.6f}\")\n",
    "print(f\"  Mean Relative Error: {best_rel['mean_rel_error']:.2f}%\")\n",
    "print(f\"  RMSE: {best_rel['rmse']:.6f}\")\n",
    "print(f\"  R²: {best_rel['r2']:.6f}\")\n",
    "print(f\"  F1 Score: {best_rel['f1_score']:.6f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "# ===== SAVE RESULTS TO CSV =====\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('fft_sindy_results.csv', index=False)\n",
    "print(\"\\n    Results saved to: fft_sindy_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9402d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for better-looking plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SINDY METHODS COMPARISON: FFT vs FINITE DIFFERENCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ===== Load Results =====\n",
    "print(\"\\n📂 Loading results...\")\n",
    "try:\n",
    "    fft_results = pd.read_csv('fft_sindy_results.csv')\n",
    "    fd_results = pd.read_csv('finite_diff_sindy_results.csv')\n",
    "    print(\"    Successfully loaded both result files\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"    Error: {e}\")\n",
    "    print(\"Please run both enhanced scripts first to generate the CSV files.\")\n",
    "    exit()\n",
    "\n",
    "# ===== Display Data Preview =====\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA PREVIEW\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nFFT Results (first 3 rows):\")\n",
    "print(fft_results.head(3))\n",
    "print(\"\\nFinite Difference Results (first 3 rows):\")\n",
    "print(fd_results.head(3))\n",
    "\n",
    "# ===== Compare Metrics =====\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"SIDE-BY-SIDE COMPARISON TABLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Sigma': fft_results['sigma'],\n",
    "    'FFT_a': fft_results['a'],\n",
    "    'FD_a': fd_results['a'],\n",
    "    'FFT_b': fft_results['b'],\n",
    "    'FD_b': fd_results['b'],\n",
    "    'FFT_c': fft_results['c'],\n",
    "    'FD_c': fd_results['c'],\n",
    "    'FFT_RMSE': fft_results['rmse'],\n",
    "    'FD_RMSE': fd_results['rmse'],\n",
    "    'FFT_R2': fft_results['r2'],\n",
    "    'FD_R2': fd_results['r2'],\n",
    "    'FFT_MeanRelErr': fft_results['mean_rel_error'],\n",
    "    'FD_MeanRelErr': fd_results['mean_rel_error'],\n",
    "    'FFT_F1': fft_results['f1_score'],\n",
    "    'FD_F1': fd_results['f1_score']\n",
    "})\n",
    "\n",
    "print(\"\\n1. RECOVERED COEFFICIENTS COMPARISON\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Sigma':>8} {'FFT a':>12} {'FD a':>12} {'FFT b':>12} {'FD b':>12} {'FFT c':>12} {'FD c':>12}\")\n",
    "print(\"-\"*80)\n",
    "for _, row in comparison.iterrows():\n",
    "    print(f\"{row['Sigma']:>8.3f} {row['FFT_a']:>12.6f} {row['FD_a']:>12.6f} \"\n",
    "          f\"{row['FFT_b']:>12.6f} {row['FD_b']:>12.6f} {row['FFT_c']:>12.6f} {row['FD_c']:>12.6f}\")\n",
    "\n",
    "print(\"\\n2. EVALUATION METRICS COMPARISON\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Sigma':>8} {'FFT RMSE':>12} {'FD RMSE':>12} {'FFT R²':>12} {'FD R²':>12} {'FFT F1':>12} {'FD F1':>12}\")\n",
    "print(\"-\"*80)\n",
    "for _, row in comparison.iterrows():\n",
    "    print(f\"{row['Sigma']:>8.3f} {row['FFT_RMSE']:>12.6f} {row['FD_RMSE']:>12.6f} \"\n",
    "          f\"{row['FFT_R2']:>12.6f} {row['FD_R2']:>12.6f} {row['FFT_F1']:>12.6f} {row['FD_F1']:>12.6f}\")\n",
    "\n",
    "print(\"\\n3. MEAN RELATIVE ERROR COMPARISON\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Sigma':>8} {'FFT MeanRelErr':>18} {'FD MeanRelErr':>18} {'Winner':>12}\")\n",
    "print(\"-\"*80)\n",
    "for _, row in comparison.iterrows():\n",
    "    winner = \"FFT\" if row['FFT_MeanRelErr'] < row['FD_MeanRelErr'] else \"FD\"\n",
    "    print(f\"{row['Sigma']:>8.3f} {row['FFT_MeanRelErr']:>17.2f}% {row['FD_MeanRelErr']:>17.2f}% {winner:>12}\")\n",
    "\n",
    "# ===== Statistical Summary =====\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"STATISTICAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "metrics = ['rmse', 'r2', 'mean_rel_error', 'f1_score', 'l1_error']\n",
    "metric_names = ['RMSE', 'R² Score', 'Mean Rel Error (%)', 'F1 Score', 'L1 Error']\n",
    "\n",
    "print(f\"\\n{'Metric':<25} {'FFT Mean':>15} {'FD Mean':>15} {'Better Method':>15}\")\n",
    "print(\"-\"*80)\n",
    "for metric, name in zip(metrics, metric_names):\n",
    "    fft_mean = fft_results[metric].mean()\n",
    "    fd_mean = fd_results[metric].mean()\n",
    "    \n",
    "    # For RMSE, mean_rel_error, l1_error: lower is better\n",
    "    # For R2, F1: higher is better\n",
    "    if metric in ['rmse', 'mean_rel_error', 'l1_error']:\n",
    "        better = \"FFT\" if fft_mean < fd_mean else \"Finite Diff\"\n",
    "    else:\n",
    "        better = \"FFT\" if fft_mean > fd_mean else \"Finite Diff\"\n",
    "    \n",
    "    print(f\"{name:<25} {fft_mean:>15.6f} {fd_mean:>15.6f} {better:>15}\")\n",
    "\n",
    "# ===== Winner Count =====\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"WINNER COUNT BY NOISE LEVEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fft_wins = 0\n",
    "fd_wins = 0\n",
    "\n",
    "for _, row in comparison.iterrows():\n",
    "    if row['FFT_MeanRelErr'] < row['FD_MeanRelErr']:\n",
    "        fft_wins += 1\n",
    "    else:\n",
    "        fd_wins += 1\n",
    "\n",
    "print(f\"\\nBased on Mean Relative Error:\")\n",
    "print(f\"  FFT wins: {fft_wins} times\")\n",
    "print(f\"  Finite Difference wins: {fd_wins} times\")\n",
    "\n",
    "# ===== Best Overall =====\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"BEST OVERALL PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fft_best = fft_results.loc[fft_results['mean_rel_error'].idxmin()]\n",
    "fd_best = fd_results.loc[fd_results['mean_rel_error'].idxmin()]\n",
    "\n",
    "print(\"\\n🏆 FFT Best Performance:\")\n",
    "print(f\"  Sigma: {fft_best['sigma']:.3f}\")\n",
    "print(f\"  Mean Relative Error: {fft_best['mean_rel_error']:.2f}%\")\n",
    "print(f\"  RMSE: {fft_best['rmse']:.6f}\")\n",
    "print(f\"  R²: {fft_best['r2']:.6f}\")\n",
    "print(f\"  F1: {fft_best['f1_score']:.6f}\")\n",
    "print(f\"  Coefficients: a={fft_best['a']:.6f}, b={fft_best['b']:.6f}, c={fft_best['c']:.6f}\")\n",
    "\n",
    "print(\"\\n🏆 Finite Difference Best Performance:\")\n",
    "print(f\"  Sigma: {fd_best['sigma']:.3f}\")\n",
    "print(f\"  Mean Relative Error: {fd_best['mean_rel_error']:.2f}%\")\n",
    "print(f\"  RMSE: {fd_best['rmse']:.6f}\")\n",
    "print(f\"  R²: {fd_best['r2']:.6f}\")\n",
    "print(f\"  F1: {fd_best['f1_score']:.6f}\")\n",
    "print(f\"  Coefficients: a={fd_best['a']:.6f}, b={fd_best['b']:.6f}, c={fd_best['c']:.6f}\")\n",
    "\n",
    "overall_winner = \"FFT\" if fft_best['mean_rel_error'] < fd_best['mean_rel_error'] else \"Finite Difference\"\n",
    "print(f\"\\n🎯 Overall Winner: {overall_winner}\")\n",
    "\n",
    "# ===== VISUALIZATIONS =====\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"GENERATING COMPARISON PLOTS...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Coefficient Recovery (a, b, c)\n",
    "ax1 = plt.subplot(3, 3, 1)\n",
    "plt.plot(fft_results['sigma'], fft_results['a'], 'o-', label='FFT', linewidth=2, markersize=6)\n",
    "plt.plot(fd_results['sigma'], fd_results['a'], 's--', label='Finite Diff', linewidth=2, markersize=6)\n",
    "plt.axhline(y=-1.0, color='r', linestyle=':', label='True value', linewidth=2)\n",
    "plt.xlabel('Noise Level (σ)')\n",
    "plt.ylabel('Coefficient a (u_xx)')\n",
    "plt.title('Coefficient a Recovery')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "ax2 = plt.subplot(3, 3, 2)\n",
    "plt.plot(fft_results['sigma'], fft_results['b'], 'o-', label='FFT', linewidth=2, markersize=6)\n",
    "plt.plot(fd_results['sigma'], fd_results['b'], 's--', label='Finite Diff', linewidth=2, markersize=6)\n",
    "plt.axhline(y=-1.0, color='r', linestyle=':', label='True value', linewidth=2)\n",
    "plt.xlabel('Noise Level (σ)')\n",
    "plt.ylabel('Coefficient b (u_xxxx)')\n",
    "plt.title('Coefficient b Recovery')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "ax3 = plt.subplot(3, 3, 3)\n",
    "plt.plot(fft_results['sigma'], fft_results['c'], 'o-', label='FFT', linewidth=2, markersize=6)\n",
    "plt.plot(fd_results['sigma'], fd_results['c'], 's--', label='Finite Diff', linewidth=2, markersize=6)\n",
    "plt.axhline(y=-1.0, color='r', linestyle=':', label='FFT True', linewidth=2)\n",
    "plt.axhline(y=-0.5, color='orange', linestyle=':', label='FD True', linewidth=2)\n",
    "plt.xlabel('Noise Level (σ)')\n",
    "plt.ylabel('Coefficient c')\n",
    "plt.title('Coefficient c Recovery')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: RMSE\n",
    "ax4 = plt.subplot(3, 3, 4)\n",
    "plt.plot(fft_results['sigma'], fft_results['rmse'], 'o-', label='FFT', linewidth=2, markersize=6)\n",
    "plt.plot(fd_results['sigma'], fd_results['rmse'], 's--', label='Finite Diff', linewidth=2, markersize=6)\n",
    "plt.xlabel('Noise Level (σ)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Root Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Plot 3: R² Score\n",
    "ax5 = plt.subplot(3, 3, 5)\n",
    "plt.plot(fft_results['sigma'], fft_results['r2'], 'o-', label='FFT', linewidth=2, markersize=6)\n",
    "plt.plot(fd_results['sigma'], fd_results['r2'], 's--', label='Finite Diff', linewidth=2, markersize=6)\n",
    "plt.xlabel('Noise Level (σ)')\n",
    "plt.ylabel('R² Score')\n",
    "plt.title('R² Coefficient of Determination')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Mean Relative Error\n",
    "ax6 = plt.subplot(3, 3, 6)\n",
    "plt.plot(fft_results['sigma'], fft_results['mean_rel_error'], 'o-', label='FFT', linewidth=2, markersize=6)\n",
    "plt.plot(fd_results['sigma'], fd_results['mean_rel_error'], 's--', label='Finite Diff', linewidth=2, markersize=6)\n",
    "plt.xlabel('Noise Level (σ)')\n",
    "plt.ylabel('Mean Relative Error (%)')\n",
    "plt.title('Mean Relative Error')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: F1 Score\n",
    "ax7 = plt.subplot(3, 3, 7)\n",
    "plt.plot(fft_results['sigma'], fft_results['f1_score'], 'o-', label='FFT', linewidth=2, markersize=6)\n",
    "plt.plot(fd_results['sigma'], fd_results['f1_score'], 's--', label='Finite Diff', linewidth=2, markersize=6)\n",
    "plt.xlabel('Noise Level (σ)')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score (Term Selection)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: L1 Error\n",
    "ax8 = plt.subplot(3, 3, 8)\n",
    "plt.plot(fft_results['sigma'], fft_results['l1_error'], 'o-', label='FFT', linewidth=2, markersize=6)\n",
    "plt.plot(fd_results['sigma'], fd_results['l1_error'], 's--', label='Finite Diff', linewidth=2, markersize=6)\n",
    "plt.xlabel('Noise Level (σ)')\n",
    "plt.ylabel('L1 Error')\n",
    "plt.title('L1 Norm Error')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 7: Relative Errors per Coefficient\n",
    "ax9 = plt.subplot(3, 3, 9)\n",
    "width = 0.035\n",
    "x = np.arange(len(fft_results))\n",
    "plt.bar(x - 1.5*width, fft_results['rel_a'], width, label='FFT a', alpha=0.8)\n",
    "plt.bar(x - 0.5*width, fft_results['rel_b'], width, label='FFT b', alpha=0.8)\n",
    "plt.bar(x + 0.5*width, fft_results['rel_c'], width, label='FFT c', alpha=0.8)\n",
    "plt.bar(x + 1.5*width, fd_results['mean_rel_error'], width, label='FD mean', alpha=0.8)\n",
    "plt.xlabel('Noise Level Index')\n",
    "plt.ylabel('Relative Error (%)')\n",
    "plt.title('Relative Errors Comparison')\n",
    "plt.xticks(x, [f\"{s:.3f}\" for s in fft_results['sigma']], rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sindy_comparison_plots.png', dpi=300, bbox_inches='tight')\n",
    "print(\"    Saved: sindy_comparison_plots.png\")\n",
    "\n",
    "# ===== Additional Visualization: Heatmap =====\n",
    "fig2, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# FFT Heatmap\n",
    "fft_data = fft_results[['rel_a', 'rel_b', 'rel_c']].T\n",
    "sns.heatmap(fft_data, annot=True, fmt='.2f', cmap='RdYlGn_r', \n",
    "            xticklabels=[f\"{s:.3f}\" for s in fft_results['sigma']],\n",
    "            yticklabels=['a (u_xx)', 'b (u_xxxx)', 'c'],\n",
    "            ax=ax1, cbar_kws={'label': 'Relative Error (%)'})\n",
    "ax1.set_title('FFT: Relative Errors Heatmap')\n",
    "ax1.set_xlabel('Noise Level (σ)')\n",
    "\n",
    "# Finite Difference Heatmap\n",
    "fd_data = fd_results[['rel_a', 'rel_b', 'rel_c']].T\n",
    "sns.heatmap(fd_data, annot=True, fmt='.2f', cmap='RdYlGn_r',\n",
    "            xticklabels=[f\"{s:.3f}\" for s in fd_results['sigma']],\n",
    "            yticklabels=['a (u_xx)', 'b (u_xxxx)', 'c ((u²)_x)'],\n",
    "            ax=ax2, cbar_kws={'label': 'Relative Error (%)'})\n",
    "ax2.set_title('Finite Difference: Relative Errors Heatmap')\n",
    "ax2.set_xlabel('Noise Level (σ)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sindy_comparison_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "print(\"    Saved: sindy_comparison_heatmap.png\")\n",
    "\n",
    "# ===== Radar Chart for Best Results =====\n",
    "from math import pi\n",
    "\n",
    "fig3, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "categories = ['RMSE\\n(lower better)', 'R²\\n(higher better)', 'Mean Rel Err\\n(lower better)', \n",
    "              'F1 Score\\n(higher better)', 'L1 Error\\n(lower better)']\n",
    "N = len(categories)\n",
    "\n",
    "# Normalize metrics to 0-1 scale for comparison\n",
    "def normalize_metric(values, inverse=False):\n",
    "    vmin, vmax = values.min(), values.max()\n",
    "    if vmax == vmin:\n",
    "        return np.ones_like(values) * 0.5\n",
    "    normalized = (values - vmin) / (vmax - vmin)\n",
    "    return 1 - normalized if inverse else normalized\n",
    "\n",
    "# Get best result for each method\n",
    "fft_best_idx = fft_results['mean_rel_error'].idxmin()\n",
    "fd_best_idx = fd_results['mean_rel_error'].idxmin()\n",
    "\n",
    "fft_metrics = [\n",
    "    1 - normalize_metric(fft_results['rmse'].values, inverse=True)[fft_best_idx],\n",
    "    normalize_metric(fft_results['r2'].values, inverse=False)[fft_best_idx],\n",
    "    1 - normalize_metric(fft_results['mean_rel_error'].values, inverse=True)[fft_best_idx],\n",
    "    normalize_metric(fft_results['f1_score'].values, inverse=False)[fft_best_idx],\n",
    "    1 - normalize_metric(fft_results['l1_error'].values, inverse=True)[fft_best_idx]\n",
    "]\n",
    "\n",
    "fd_metrics = [\n",
    "    1 - normalize_metric(fd_results['rmse'].values, inverse=True)[fd_best_idx],\n",
    "    normalize_metric(fd_results['r2'].values, inverse=False)[fd_best_idx],\n",
    "    1 - normalize_metric(fd_results['mean_rel_error'].values, inverse=True)[fd_best_idx],\n",
    "    normalize_metric(fd_results['f1_score'].values, inverse=False)[fd_best_idx],\n",
    "    1 - normalize_metric(fd_results['l1_error'].values, inverse=True)[fd_best_idx]\n",
    "]\n",
    "\n",
    "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "fft_metrics += fft_metrics[:1]\n",
    "fd_metrics += fd_metrics[:1]\n",
    "angles += angles[:1]\n",
    "\n",
    "# FFT Radar\n",
    "ax1.plot(angles, fft_metrics, 'o-', linewidth=2, color='blue', label='FFT')\n",
    "ax1.fill(angles, fft_metrics, alpha=0.25, color='blue')\n",
    "ax1.set_xticks(angles[:-1])\n",
    "ax1.set_xticklabels(categories, size=9)\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.set_title(f'FFT Best Performance\\n(σ={fft_results.loc[fft_best_idx, \"sigma\"]:.3f})', \n",
    "              size=12, pad=20)\n",
    "ax1.grid(True)\n",
    "\n",
    "# FD Radar\n",
    "ax2.plot(angles, fd_metrics, 's-', linewidth=2, color='green', label='Finite Diff')\n",
    "ax2.fill(angles, fd_metrics, alpha=0.25, color='green')\n",
    "ax2.set_xticks(angles[:-1])\n",
    "ax2.set_xticklabels(categories, size=9)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.set_title(f'Finite Difference Best Performance\\n(σ={fd_results.loc[fd_best_idx, \"sigma\"]:.3f})', \n",
    "              size=12, pad=20)\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sindy_comparison_radar.png', dpi=300, bbox_inches='tight')\n",
    "print(\"      Saved: sindy_comparison_radar.png\")\n",
    "\n",
    "# ===== Save Comparison Table =====\n",
    "comparison.to_csv('sindy_comparison_table.csv', index=False)\n",
    "print(\"    Saved: sindy_comparison_table.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"   COMPARISON COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  1. sindy_comparison_plots.png - Main comparison plots\")\n",
    "print(\"  2. sindy_comparison_heatmap.png - Relative errors heatmap\")\n",
    "print(\"  3. sindy_comparison_radar.png - Radar charts for best results\")\n",
    "print(\"  4. sindy_comparison_table.csv - Side-by-side comparison data\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aec58b",
   "metadata": {},
   "source": [
    "Denoising methods \n",
    "\n",
    "Implement 3 Denoising Methods on KS Synthetic Noisy Data\n",
    "\n",
    "We will implement:\n",
    "\n",
    "1. Gaussian smoothing (spatial filtering)\n",
    "2. Savitzky–Golay smoothing (polynomial filtering)\n",
    "3. Spectral low-pass filtering (Fourier truncation)\n",
    "\n",
    "We then evaluate:\n",
    "\n",
    "recovered coefficients a, b, c\n",
    "r2\n",
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf514d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# ============================================================\n",
    "# 1. List your noisy datasets here\n",
    "#    Replace the filenames with your actual .h5 file names.\n",
    "# ============================================================\n",
    "data_files = [\n",
    "    (0.00, \"test_solving_euler_ks_clean.h5\"),  # clean (no added noise)\n",
    "    (0.01, \"ks_gaussian_sigma_0.005.h5\"),\n",
    "    (0.03, \"ks_gaussian_sigma_0.010.h5\"),\n",
    "    (0.05, \"ks_gaussian_sigma_0.030.h5\"),\n",
    "    (0.08, \"ks_gaussian_sigma_0.080.h5\"),\n",
    "    (0.10, \"ks_gaussian_sigma_0.100.h5\"),\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# 2. Denoising operators (space-only, along x)\n",
    "# ============================================================\n",
    "\n",
    "def denoise_gaussian(U, sigma_x=1.0):\n",
    "    \"\"\"\n",
    "    Gaussian smoothing along space (axis=1) with periodic boundary (wrap).\n",
    "    U: array (T, X)\n",
    "    \"\"\"\n",
    "    return gaussian_filter1d(U, sigma=sigma_x, axis=1, mode=\"wrap\")\n",
    "\n",
    "def denoise_savgol(U, window=11, poly=3):\n",
    "    \"\"\"\n",
    "    Savitzky–Golay smoothing along space (axis=1) with periodic boundary (wrap).\n",
    "    window must be odd and <= number of spatial points.\n",
    "    \"\"\"\n",
    "    T, X = U.shape\n",
    "    # ensure window is odd and <= X\n",
    "    window = min(window, X - (1 - X % 2))\n",
    "    if window % 2 == 0:\n",
    "        window -= 1\n",
    "    return savgol_filter(U, window_length=window, polyorder=poly,\n",
    "                         axis=1, mode=\"wrap\")\n",
    "\n",
    "def denoise_spectral(U, cutoff_ratio=0.5, dx=0.5):\n",
    "    \"\"\"\n",
    "    Spectral low-pass denoising:\n",
    "    - FFT in space (axis=1)\n",
    "    - zero out frequencies above cutoff_ratio * k_max\n",
    "    \"\"\"\n",
    "    T, X = U.shape\n",
    "    U_hat = np.fft.rfft(U, axis=1)\n",
    "    freqs = np.fft.rfftfreq(X, d=dx)\n",
    "    k_max = freqs.max()\n",
    "    mask = (np.abs(freqs) <= cutoff_ratio * k_max).astype(U_hat.dtype)\n",
    "    U_hat_filtered = U_hat * mask[None, :]\n",
    "    U_lp = np.fft.irfft(U_hat_filtered, n=X, axis=1)\n",
    "    return U_lp\n",
    "\n",
    "# ============================================================\n",
    "# 3. Main loop: load each σ–dataset, apply denoisers, save\n",
    "# ============================================================\n",
    "\n",
    "for sigma, fname in data_files:\n",
    "    print(f\"\\n=== Processing σ={sigma:.3f}, file={fname} ===\")\n",
    "\n",
    "    # ---- load original noisy dataset ----\n",
    "    with h5py.File(fname, \"r\") as f:\n",
    "        u = np.array(f[\"u\"], dtype=np.float32)   # (T, X)\n",
    "        x = np.array(f[\"x\"], dtype=np.float32)\n",
    "        t = np.array(f[\"t\"], dtype=np.float32)\n",
    "        attrs = dict(f.attrs)  # copy attributes\n",
    "\n",
    "    T, X = u.shape\n",
    "    dx = float(x[1] - x[0])\n",
    "    dt = float(t[1] - t[0])\n",
    "    print(f\"  u.shape={u.shape}, dx={dx:.4f}, dt={dt:.4f}\")\n",
    "\n",
    "    # ---- apply denoising methods ----\n",
    "    u_gauss   = denoise_gaussian(u, sigma_x=1.0)\n",
    "    u_savgol  = denoise_savgol(u, window=11, poly=3)\n",
    "    u_spectral= denoise_spectral(u, cutoff_ratio=0.5, dx=dx)\n",
    "\n",
    "    # ---- base name for outputs ----\n",
    "    if fname.lower().endswith(\".h5\"):\n",
    "        base = fname[:-3]\n",
    "    else:\n",
    "        base = fname\n",
    "\n",
    "    # ---- helper to save a denoised version ----\n",
    "    def save_denoised(u_denoised, suffix, method_name):\n",
    "        out_name = f\"{base}_{suffix}.h5\"\n",
    "        print(f\"  Saving {method_name} denoised data → {out_name}\")\n",
    "        with h5py.File(out_name, \"w\") as f_out:\n",
    "            f_out.create_dataset(\"u\", data=u_denoised)\n",
    "            f_out.create_dataset(\"x\", data=x)\n",
    "            f_out.create_dataset(\"t\", data=t)\n",
    "            # copy original attrs, then add denoising info\n",
    "            for k, v in attrs.items():\n",
    "                f_out.attrs[k] = v\n",
    "            f_out.attrs[\"denoising_method\"] = method_name\n",
    "            f_out.attrs[\"sigma_source\"] = sigma\n",
    "\n",
    "    # ---- save all three denoised versions ----\n",
    "    save_denoised(u_gauss,    \"gauss\",   \"gaussian_spatial\")\n",
    "    save_denoised(u_savgol,   \"savgol\",  \"savgol_spatial\")\n",
    "    save_denoised(u_spectral, \"spectral\",\"spectral_lowpass\")\n",
    "\n",
    "print(\"\\nAll denoised datasets saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77566f4",
   "metadata": {},
   "source": [
    "## Phase 2: Denoising methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9c3246",
   "metadata": {},
   "source": [
    "### 3 denoising methods, FFT-based SINDy, and useful visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f428a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from scipy.fft import fft, ifft, fftfreq\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DENOISING METHODS + FFT-BASED SINDY EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ===== Spectral derivative functions =====\n",
    "def spectral_derivative(u_snapshot, k):\n",
    "    \"\"\"First derivative using FFT\"\"\"\n",
    "    u_hat = fft(u_snapshot)\n",
    "    du_hat = 1j * k * u_hat\n",
    "    return np.real(ifft(du_hat))\n",
    "\n",
    "def spectral_second_derivative(u_snapshot, k):\n",
    "    \"\"\"Second derivative using FFT\"\"\"\n",
    "    u_hat = fft(u_snapshot)\n",
    "    d2u_hat = -(k**2) * u_hat\n",
    "    return np.real(ifft(d2u_hat))\n",
    "\n",
    "def spectral_fourth_derivative(u_snapshot, k):\n",
    "    \"\"\"Fourth derivative using FFT\"\"\"\n",
    "    u_hat = fft(u_snapshot)\n",
    "    d4u_hat = (k**4) * u_hat\n",
    "    return np.real(ifft(d4u_hat))\n",
    "\n",
    "\n",
    "# ===== Denoising Methods =====\n",
    "\n",
    "def gaussian_denoising(u, sigma=2.0):\n",
    "    \"\"\"\n",
    "    Gaussian smoothing - spatial filtering\n",
    "    Apply Gaussian filter along spatial dimension\n",
    "    \"\"\"\n",
    "    u_denoised = np.zeros_like(u)\n",
    "    for i in range(u.shape[0]):  # For each time snapshot\n",
    "        u_denoised[i] = gaussian_filter1d(u[i], sigma=sigma)\n",
    "    return u_denoised\n",
    "\n",
    "\n",
    "def savitzky_golay_denoising(u, window_length=11, polyorder=3):\n",
    "    \"\"\"\n",
    "    Savitzky-Golay smoothing - polynomial filtering\n",
    "    Preserves edges and peaks better than Gaussian\n",
    "    Apply along spatial dimension\n",
    "    \"\"\"\n",
    "    u_denoised = np.zeros_like(u)\n",
    "    # Ensure window_length is odd and less than data length\n",
    "    window_length = min(window_length, u.shape[1] - 1)\n",
    "    if window_length % 2 == 0:\n",
    "        window_length -= 1\n",
    "    \n",
    "    for i in range(u.shape[0]):  # For each time snapshot\n",
    "        u_denoised[i] = savgol_filter(u[i], window_length=window_length, \n",
    "                                       polyorder=polyorder, mode='wrap')\n",
    "    return u_denoised\n",
    "\n",
    "\n",
    "def spectral_lowpass_denoising(u, cutoff_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Spectral low-pass filtering - Fourier truncation\n",
    "    Remove high-frequency components (noise)\n",
    "    cutoff_ratio: fraction of frequencies to keep (0.5 = keep lower 50%)\n",
    "    \"\"\"\n",
    "    u_denoised = np.zeros_like(u)\n",
    "    Nx = u.shape[1]\n",
    "    cutoff_idx = int(Nx * cutoff_ratio / 2)\n",
    "    \n",
    "    for i in range(u.shape[0]):  # For each time snapshot\n",
    "        u_hat = fft(u[i])\n",
    "        # Zero out high frequencies\n",
    "        u_hat_filtered = u_hat.copy()\n",
    "        u_hat_filtered[cutoff_idx:-cutoff_idx] = 0\n",
    "        u_denoised[i] = np.real(ifft(u_hat_filtered))\n",
    "    \n",
    "    return u_denoised\n",
    "\n",
    "\n",
    "# ===== Evaluation Function =====\n",
    "def evaluate_sindy_fft(u, x, t, method_name=\"Unknown\", sigma_noise=0.0):\n",
    "    \"\"\"\n",
    "    Apply FFT-based SINDy and compute evaluation metrics\n",
    "    Returns: dict with coefficients, R², RMSE, errors\n",
    "    \"\"\"\n",
    "    Nx = len(x)\n",
    "    dx = x[1] - x[0]\n",
    "    dt = t[1] - t[0]\n",
    "    \n",
    "    # Define wavenumbers\n",
    "    k = 2 * np.pi * fftfreq(Nx, d=dx)\n",
    "    \n",
    "    # Compute temporal derivative (central difference)\n",
    "    u_t = (u[2:] - u[:-2]) / (2 * dt)\n",
    "    u_mid = u[1:-1]\n",
    "    \n",
    "    # Build feature library using FFT\n",
    "    Theta = []\n",
    "    ut_flat = []\n",
    "    \n",
    "    for snapshot, ut_snapshot in zip(u_mid, u_t):\n",
    "        ux = spectral_derivative(snapshot, k)\n",
    "        uxx = spectral_second_derivative(snapshot, k)\n",
    "        uxxxx = spectral_fourth_derivative(snapshot, k)\n",
    "        \n",
    "        # Feature terms: [u_xx, u_xxxx, u*u_x]\n",
    "        Theta_snapshot = np.vstack([\n",
    "            uxx,\n",
    "            uxxxx,\n",
    "            snapshot * ux\n",
    "        ]).T  # shape (Nx, 3)\n",
    "        \n",
    "        Theta.append(Theta_snapshot)\n",
    "        ut_flat.append(ut_snapshot)\n",
    "    \n",
    "    Theta = np.vstack(Theta)\n",
    "    ut_flat = np.hstack(ut_flat)\n",
    "    \n",
    "    # Normalize features\n",
    "    Theta_mean = Theta.mean(axis=0)\n",
    "    Theta_std = Theta.std(axis=0)\n",
    "    Theta_norm = (Theta - Theta_mean) / Theta_std\n",
    "    \n",
    "    # Lasso regression\n",
    "    lasso = Lasso(alpha=1e-4, fit_intercept=False, max_iter=50000)\n",
    "    lasso.fit(Theta_norm, ut_flat)\n",
    "    coeffs = lasso.coef_ / Theta_std\n",
    "    \n",
    "    # Extract coefficients\n",
    "    a = coeffs[0]  # u_xx\n",
    "    b = coeffs[1]  # u_xxxx\n",
    "    c = coeffs[2]  # u*u_x\n",
    "    \n",
    "    # Predict u_t\n",
    "    u_t_pred = Theta @ coeffs\n",
    "    \n",
    "    # Compute metrics\n",
    "    rmse = np.sqrt(np.mean((ut_flat - u_t_pred)**2))\n",
    "    r2 = r2_score(ut_flat, u_t_pred)\n",
    "    \n",
    "    # True coefficients\n",
    "    a_true, b_true, c_true = -1.0, -1.0, -1.0\n",
    "    \n",
    "    # Coefficient errors\n",
    "    error_a = a - a_true\n",
    "    error_b = b - b_true\n",
    "    error_c = c - c_true\n",
    "    \n",
    "    abs_error_a = abs(error_a)\n",
    "    abs_error_b = abs(error_b)\n",
    "    abs_error_c = abs(error_c)\n",
    "    \n",
    "    return {\n",
    "        'method': method_name,\n",
    "        'sigma': sigma_noise,\n",
    "        'a': a,\n",
    "        'b': b,\n",
    "        'c': c,\n",
    "        'error_a': error_a,\n",
    "        'error_b': error_b,\n",
    "        'error_c': error_c,\n",
    "        'abs_error_a': abs_error_a,\n",
    "        'abs_error_b': abs_error_b,\n",
    "        'abs_error_c': abs_error_c,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2\n",
    "    }\n",
    "\n",
    "\n",
    "# ===== Main Processing =====\n",
    "\n",
    "# Noise datasets (excluding clean data)\n",
    "noise_files = [\n",
    "    \"ks_gaussian_sigma_0.005.h5\",\n",
    "    \"ks_gaussian_sigma_0.010.h5\",\n",
    "    \"ks_gaussian_sigma_0.030.h5\",\n",
    "    \"ks_gaussian_sigma_0.050.h5\",\n",
    "    \"ks_gaussian_sigma_0.080.h5\",\n",
    "    \"ks_gaussian_sigma_0.100.h5\"\n",
    "]\n",
    "\n",
    "noise_levels = [0.005, 0.01, 0.03, 0.05, 0.08, 0.1]\n",
    "\n",
    "# Store all results\n",
    "all_results = []\n",
    "\n",
    "# Process each noise level\n",
    "for file, sigma in zip(noise_files, noise_levels):\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"PROCESSING NOISE LEVEL σ = {sigma}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load noisy data\n",
    "    with h5py.File(file, \"r\") as f:\n",
    "        u_noisy = np.array(f[\"u\"])\n",
    "        x = np.array(f[\"x\"])\n",
    "        t = np.array(f[\"t\"])\n",
    "    \n",
    "    print(f\"Loaded: {file}\")\n",
    "    print(f\"Shape: {u_noisy.shape}\")\n",
    "    \n",
    "    # ===== 1. NO DENOISING (Baseline) =====\n",
    "    print(\"\\n--- Method 1: No Denoising (Baseline) ---\")\n",
    "    result_baseline = evaluate_sindy_fft(u_noisy, x, t, \n",
    "                                          method_name=\"No Denoising\", \n",
    "                                          sigma_noise=sigma)\n",
    "    all_results.append(result_baseline)\n",
    "    print(f\"  Coefficients: a={result_baseline['a']:.6f}, b={result_baseline['b']:.6f}, c={result_baseline['c']:.6f}\")\n",
    "    print(f\"  RMSE: {result_baseline['rmse']:.6f}, R²: {result_baseline['r2']:.6f}\")\n",
    "    \n",
    "    \n",
    "    # ===== 2. GAUSSIAN DENOISING =====\n",
    "    print(\"\\n--- Method 2: Gaussian Smoothing ---\")\n",
    "    u_gaussian = gaussian_denoising(u_noisy, sigma=2.0)\n",
    "    result_gaussian = evaluate_sindy_fft(u_gaussian, x, t, \n",
    "                                          method_name=\"Gaussian\", \n",
    "                                          sigma_noise=sigma)\n",
    "    all_results.append(result_gaussian)\n",
    "    print(f\"  Coefficients: a={result_gaussian['a']:.6f}, b={result_gaussian['b']:.6f}, c={result_gaussian['c']:.6f}\")\n",
    "    print(f\"  RMSE: {result_gaussian['rmse']:.6f}, R²: {result_gaussian['r2']:.6f}\")\n",
    "    \n",
    "    \n",
    "    # ===== 3. SAVITZKY-GOLAY DENOISING =====\n",
    "    print(\"\\n--- Method 3: Savitzky-Golay Smoothing ---\")\n",
    "    u_savgol = savitzky_golay_denoising(u_noisy, window_length=11, polyorder=3)\n",
    "    result_savgol = evaluate_sindy_fft(u_savgol, x, t, \n",
    "                                        method_name=\"Savitzky-Golay\", \n",
    "                                        sigma_noise=sigma)\n",
    "    all_results.append(result_savgol)\n",
    "    print(f\"  Coefficients: a={result_savgol['a']:.6f}, b={result_savgol['b']:.6f}, c={result_savgol['c']:.6f}\")\n",
    "    print(f\"  RMSE: {result_savgol['rmse']:.6f}, R²: {result_savgol['r2']:.6f}\")\n",
    "    \n",
    "    \n",
    "    # ===== 4. SPECTRAL LOW-PASS DENOISING =====\n",
    "    print(\"\\n--- Method 4: Spectral Low-Pass Filtering ---\")\n",
    "    u_spectral = spectral_lowpass_denoising(u_noisy, cutoff_ratio=0.5)\n",
    "    result_spectral = evaluate_sindy_fft(u_spectral, x, t, \n",
    "                                          method_name=\"Spectral\", \n",
    "                                          sigma_noise=sigma)\n",
    "    all_results.append(result_spectral)\n",
    "    print(f\"  Coefficients: a={result_spectral['a']:.6f}, b={result_spectral['b']:.6f}, c={result_spectral['c']:.6f}\")\n",
    "    print(f\"  RMSE: {result_spectral['rmse']:.6f}, R²: {result_spectral['r2']:.6f}\")\n",
    "\n",
    "\n",
    "# ===== Convert to DataFrame =====\n",
    "df_results = pd.DataFrame(all_results)\n",
    "\n",
    "# ===== SUMMARY TABLES =====\n",
    "print(\"\\n\\n\" + \"=\"*100)\n",
    "print(\"SUMMARY TABLE 1: RECOVERED COEFFICIENTS BY DENOISING METHOD\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Sigma':>8} {'Method':>18} {'a (u_xx)':>12} {'b (u_xxxx)':>12} {'c (u*u_x)':>12} {'Err_a':>10} {'Err_b':>10} {'Err_c':>10}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "for _, row in df_results.iterrows():\n",
    "    print(f\"{row['sigma']:>8.3f} {row['method']:>18} {row['a']:>12.6f} {row['b']:>12.6f} {row['c']:>12.6f} \"\n",
    "          f\"{row['error_a']:>10.6f} {row['error_b']:>10.6f} {row['error_c']:>10.6f}\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(f\"{'TRUE:':>27} {-1.0:>12.6f} {-1.0:>12.6f} {-1.0:>12.6f}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*100)\n",
    "print(\"SUMMARY TABLE 2: EVALUATION METRICS BY DENOISING METHOD\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Sigma':>8} {'Method':>18} {'RMSE':>12} {'R²':>12} {'|Err_a|':>10} {'|Err_b|':>10} {'|Err_c|':>10}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "for _, row in df_results.iterrows():\n",
    "    print(f\"{row['sigma']:>8.3f} {row['method']:>18} {row['rmse']:>12.6f} {row['r2']:>12.6f} \"\n",
    "          f\"{row['abs_error_a']:>10.6f} {row['abs_error_b']:>10.6f} {row['abs_error_c']:>10.6f}\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "\n",
    "\n",
    "# ===== COMPARISON BY NOISE LEVEL =====\n",
    "print(\"\\n\\n\" + \"=\"*100)\n",
    "print(\"SUMMARY TABLE 3: BEST METHOD PER NOISE LEVEL (by RMSE)\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Sigma':>8} {'Best Method':>18} {'RMSE':>12} {'R²':>12} {'Coeff a':>12} {'Coeff b':>12} {'Coeff c':>12}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "for sigma in noise_levels:\n",
    "    sigma_data = df_results[df_results['sigma'] == sigma]\n",
    "    best_idx = sigma_data['rmse'].idxmin()\n",
    "    best = df_results.loc[best_idx]\n",
    "    print(f\"{sigma:>8.3f} {best['method']:>18} {best['rmse']:>12.6f} {best['r2']:>12.6f} \"\n",
    "          f\"{best['a']:>12.6f} {best['b']:>12.6f} {best['c']:>12.6f}\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "\n",
    "\n",
    "# ===== OVERALL STATISTICS =====\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"OVERALL STATISTICS BY METHOD (Averaged Across All Noise Levels)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "method_stats = df_results.groupby('method').agg({\n",
    "    'rmse': 'mean',\n",
    "    'r2': 'mean',\n",
    "    'abs_error_a': 'mean',\n",
    "    'abs_error_b': 'mean',\n",
    "    'abs_error_c': 'mean'\n",
    "}).round(6)\n",
    "\n",
    "print(method_stats)\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_method = method_stats['rmse'].idxmin()\n",
    "print(f\"\\n🏆 BEST OVERALL METHOD (by average RMSE): {best_method}\")\n",
    "print(f\"   Average RMSE: {method_stats.loc[best_method, 'rmse']:.6f}\")\n",
    "print(f\"   Average R²: {method_stats.loc[best_method, 'r2']:.6f}\")\n",
    "\n",
    "\n",
    "# ===== VISUALIZATION 1: Coefficient Recovery =====\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"GENERATING VISUALIZATIONS...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "methods = df_results['method'].unique()\n",
    "colors = {'No Denoising': 'red', 'Gaussian': 'blue', \n",
    "          'Savitzky-Golay': 'green', 'Spectral': 'purple'}\n",
    "markers = {'No Denoising': 'x', 'Gaussian': 'o', \n",
    "           'Savitzky-Golay': 's', 'Spectral': '^'}\n",
    "\n",
    "# Plot 1: Coefficient a recovery\n",
    "ax1 = plt.subplot(3, 3, 1)\n",
    "for method in methods:\n",
    "    data = df_results[df_results['method'] == method]\n",
    "    plt.plot(data['sigma'], data['a'], marker=markers[method], \n",
    "             label=method, linewidth=2, markersize=8, color=colors[method])\n",
    "plt.axhline(y=-1.0, color='black', linestyle='--', linewidth=2, label='True value')\n",
    "plt.xlabel('Noise Level (σ)')\n",
    "plt.ylabel('Coefficient a (u_xx)')\n",
    "plt.title('Coefficient a Recovery')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Coefficient b recovery\n",
    "ax2 = plt.subplot(3, 3, 2)\n",
    "for method in methods:\n",
    "    data = df_results[df_results['method'] == method]\n",
    "    plt.plot(data['sigma'], data['b'], marker=markers[method], \n",
    "             label=method, linewidth=2, markersize=8, color=colors[method])\n",
    "plt.axhline(y=-1.0, color='black', linestyle='--', linewidth=2, label='True value')\n",
    "plt.xlabel('Noise Level (σ)')\n",
    "plt.ylabel('Coefficient b (u_xxxx)')\n",
    "plt.title('Coefficient b Recovery')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Coefficient c recovery\n",
    "ax3 = plt.subplot(3, 3, 3)\n",
    "for method in methods:\n",
    "    data = df_results[df_results['method'] == method]\n",
    "    plt.plot(data['sigma'], data['c'], marker=markers[method], \n",
    "             label=method, linewidth=2, markersize=8, color=colors[method])\n",
    "plt.axhline(y=-1.0, color='black', linestyle='--', linewidth=2, label='True value')\n",
    "plt.xlabel('Noise Level (σ)')\n",
    "plt.ylabel('Coefficient c (u*u_x)')\n",
    "plt.title('Coefficient c Recovery')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: RMSE comparison\n",
    "ax4 = plt.subplot(3, 3, 4)\n",
    "for method in methods:\n",
    "    data = df_results[df_results['method'] == method]\n",
    "    plt.plot(data['sigma'], data['rmse'], marker=markers[method], \n",
    "             label=method, linewidth=2, markersize=8, color=colors[method])\n",
    "plt.xlabel('Noise Level (σ)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Root Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Plot 5: R² comparison\n",
    "ax5 = plt.subplot(3, 3, 5)\n",
    "for method in methods:\n",
    "    data = df_results[df_results['method'] == method]\n",
    "    plt.plot(data['sigma'], data['r2'], marker=markers[method], \n",
    "             label=method, linewidth=2, markersize=8, color=colors[method])\n",
    "plt.xlabel('Noise Level (σ)')\n",
    "plt.ylabel('R² Score')\n",
    "plt.title('R² Coefficient of Determination')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Absolute error in a\n",
    "ax6 = plt.subplot(3, 3, 6)\n",
    "for method in methods:\n",
    "    data = df_results[df_results['method'] == method]\n",
    "    plt.plot(data['sigma'], data['abs_error_a'], marker=markers[method], \n",
    "             label=method, linewidth=2, markersize=8, color=colors[method])\n",
    "plt.xlabel('Noise Level (σ)')\n",
    "plt.ylabel('|Error in a|')\n",
    "plt.title('Absolute Error in Coefficient a')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 7: Absolute error in b\n",
    "ax7 = plt.subplot(3, 3, 7)\n",
    "for method in methods:\n",
    "    data = df_results[df_results['method'] == method]\n",
    "    plt.plot(data['sigma'], data['abs_error_b'], marker=markers[method], \n",
    "             label=method, linewidth=2, markersize=8, color=colors[method])\n",
    "plt.xlabel('Noise Level (σ)')\n",
    "plt.ylabel('|Error in b|')\n",
    "plt.title('Absolute Error in Coefficient b')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 8: Absolute error in c\n",
    "ax8 = plt.subplot(3, 3, 8)\n",
    "for method in methods:\n",
    "    data = df_results[df_results['method'] == method]\n",
    "    plt.plot(data['sigma'], data['abs_error_c'], marker=markers[method], \n",
    "             label=method, linewidth=2, markersize=8, color=colors[method])\n",
    "plt.xlabel('Noise Level (σ)')\n",
    "plt.ylabel('|Error in c|')\n",
    "plt.title('Absolute Error in Coefficient c')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 9: Bar chart of average performance\n",
    "ax9 = plt.subplot(3, 3, 9)\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "rmse_means = [method_stats.loc[m, 'rmse'] for m in methods]\n",
    "r2_means = [method_stats.loc[m, 'r2'] for m in methods]\n",
    "ax9_twin = ax9.twinx()\n",
    "bars1 = ax9.bar(x - width/2, rmse_means, width, label='Avg RMSE', alpha=0.8)\n",
    "bars2 = ax9_twin.bar(x + width/2, r2_means, width, label='Avg R²', alpha=0.8, color='orange')\n",
    "ax9.set_xlabel('Denoising Method')\n",
    "ax9.set_ylabel('Average RMSE', color='blue')\n",
    "ax9_twin.set_ylabel('Average R²', color='orange')\n",
    "ax9.set_xticks(x)\n",
    "ax9.set_xticklabels(methods, rotation=45, ha='right')\n",
    "ax9.set_title('Average Performance by Method')\n",
    "ax9.legend(loc='upper left')\n",
    "ax9_twin.legend(loc='upper right')\n",
    "ax9.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('denoising_fft_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"    Saved: denoising_fft_comparison.png\")\n",
    "\n",
    "\n",
    "# ===== VISUALIZATION 2: Heatmaps =====\n",
    "fig2, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Heatmap 1: RMSE\n",
    "pivot_rmse = df_results.pivot(index='method', columns='sigma', values='rmse')\n",
    "sns.heatmap(pivot_rmse, annot=True, fmt='.4f', cmap='RdYlGn_r', ax=axes[0, 0],\n",
    "            cbar_kws={'label': 'RMSE'})\n",
    "axes[0, 0].set_title('RMSE by Method and Noise Level', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Noise Level (σ)')\n",
    "axes[0, 0].set_ylabel('Denoising Method')\n",
    "\n",
    "# Heatmap 2: R²\n",
    "pivot_r2 = df_results.pivot(index='method', columns='sigma', values='r2')\n",
    "sns.heatmap(pivot_r2, annot=True, fmt='.4f', cmap='RdYlGn', ax=axes[0, 1],\n",
    "            cbar_kws={'label': 'R² Score'})\n",
    "axes[0, 1].set_title('R² Score by Method and Noise Level', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Noise Level (σ)')\n",
    "axes[0, 1].set_ylabel('Denoising Method')\n",
    "\n",
    "# Heatmap 3: Absolute Error in coefficient a\n",
    "pivot_err_a = df_results.pivot(index='method', columns='sigma', values='abs_error_a')\n",
    "sns.heatmap(pivot_err_a, annot=True, fmt='.4f', cmap='RdYlGn_r', ax=axes[1, 0],\n",
    "            cbar_kws={'label': '|Error in a|'})\n",
    "axes[1, 0].set_title('Absolute Error in Coefficient a', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Noise Level (σ)')\n",
    "axes[1, 0].set_ylabel('Denoising Method')\n",
    "\n",
    "# Heatmap 4: Absolute Error in coefficient b\n",
    "pivot_err_b = df_results.pivot(index='method', columns='sigma', values='abs_error_b')\n",
    "sns.heatmap(pivot_err_b, annot=True, fmt='.4f', cmap='RdYlGn_r', ax=axes[1, 1],\n",
    "            cbar_kws={'label': '|Error in b|'})\n",
    "axes[1, 1].set_title('Absolute Error in Coefficient b', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Noise Level (σ)')\n",
    "axes[1, 1].set_ylabel('Denoising Method')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('denoising_fft_heatmaps.png', dpi=300, bbox_inches='tight')\n",
    "print(\"    Saved: denoising_fft_heatmaps.png\")\n",
    "\n",
    "\n",
    "# ===== Save Results to CSV =====\n",
    "df_results.to_csv('denoising_fft_results.csv', index=False)\n",
    "print(\"    Saved: denoising_fft_results.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"   DENOISING + FFT SINDY EVALUATION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  1. denoising_fft_comparison.png - Main comparison plots (9 subplots)\")\n",
    "print(\"  2. denoising_fft_heatmaps.png - Heatmaps of metrics\")\n",
    "print(\"  3. denoising_fft_results.csv - Complete results data\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac40c0c",
   "metadata": {},
   "source": [
    " 3 denoising methods, Finite Difference sindy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0b1657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.fft import fft, ifft\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DENOISING METHODS + FINITE DIFFERENCE SINDY EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ===== Denoising Methods =====\n",
    "\n",
    "def gaussian_denoising(u, sigma=2.0):\n",
    "    \"\"\"\n",
    "    Gaussian smoothing - spatial filtering\n",
    "    Apply Gaussian filter along spatial dimension\n",
    "    \"\"\"\n",
    "    u_denoised = np.zeros_like(u)\n",
    "    for i in range(u.shape[0]):  # For each time snapshot\n",
    "        u_denoised[i] = gaussian_filter1d(u[i], sigma=sigma)\n",
    "    return u_denoised\n",
    "\n",
    "\n",
    "def savitzky_golay_denoising(u, window_length=11, polyorder=3):\n",
    "    \"\"\"\n",
    "    Savitzky-Golay smoothing - polynomial filtering\n",
    "    Preserves edges and peaks better than Gaussian\n",
    "    Apply along spatial dimension\n",
    "    \"\"\"\n",
    "    u_denoised = np.zeros_like(u)\n",
    "    # Ensure window_length is odd and less than data length\n",
    "    window_length = min(window_length, u.shape[1] - 1)\n",
    "    if window_length % 2 == 0:\n",
    "        window_length -= 1\n",
    "    \n",
    "    for i in range(u.shape[0]):  # For each time snapshot\n",
    "        u_denoised[i] = savgol_filter(u[i], window_length=window_length, \n",
    "                                       polyorder=polyorder, mode='wrap')\n",
    "    return u_denoised\n",
    "\n",
    "\n",
    "def spectral_lowpass_denoising(u, cutoff_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Spectral low-pass filtering - Fourier truncation\n",
    "    Remove high-frequency components (noise)\n",
    "    cutoff_ratio: fraction of frequencies to keep (0.5 = keep lower 50%)\n",
    "    \"\"\"\n",
    "    u_denoised = np.zeros_like(u)\n",
    "    Nx = u.shape[1]\n",
    "    cutoff_idx = int(Nx * cutoff_ratio / 2)\n",
    "    \n",
    "    for i in range(u.shape[0]):  # For each time snapshot\n",
    "        u_hat = fft(u[i])\n",
    "        # Zero out high frequencies\n",
    "        u_hat_filtered = u_hat.copy()\n",
    "        u_hat_filtered[cutoff_idx:-cutoff_idx] = 0\n",
    "        u_denoised[i] = np.real(ifft(u_hat_filtered))\n",
    "    \n",
    "    return u_denoised\n",
    "\n",
    "\n",
    "# ===== Evaluation Function with Finite Difference =====\n",
    "def evaluate_sindy_finite_diff(u, x, t, method_name=\"Unknown\", sigma_noise=0.0):\n",
    "    \"\"\"\n",
    "    Apply Finite Difference SINDy and compute evaluation metrics\n",
    "    Returns: dict with coefficients, R², RMSE, errors\n",
    "    \"\"\"\n",
    "    dx = float(x[1] - x[0])\n",
    "    dt = float(t[1] - t[0])\n",
    "    \n",
    "    # Finite difference operators\n",
    "    def d1(U): \n",
    "        return (np.roll(U, -1, 1) - np.roll(U, 1, 1)) / (2*dx)\n",
    "    \n",
    "    def d2(U): \n",
    "        return (np.roll(U, -1, 1) - 2*U + np.roll(U, 1, 1)) / (dx**2)\n",
    "    \n",
    "    def d4(U):\n",
    "        return (np.roll(U, -2, 1) - 4*np.roll(U, -1, 1) + 6*U - \n",
    "                4*np.roll(U, 1, 1) + np.roll(U, 2, 1)) / (dx**4)\n",
    "    \n",
    "    # Compute derivatives\n",
    "    U_t = (u[1:] - u[:-1]) / dt\n",
    "    U = u[:-1]\n",
    "    U_x = d1(U)\n",
    "    U_xx = d2(U)\n",
    "    U_xxxx = d4(U)\n",
    "    U2_x = d1(U**2)\n",
    "    \n",
    "    # Build feature matrix\n",
    "    Theta = np.column_stack([\n",
    "        np.ones_like(U).ravel(),\n",
    "        U.ravel(),\n",
    "        U_x.ravel(),\n",
    "        U_xx.ravel(),\n",
    "        U_xxxx.ravel(),\n",
    "        U2_x.ravel()\n",
    "    ])\n",
    "    y = U_t.ravel()\n",
    "    names = [\"1\", \"u\", \"u_x\", \"u_xx\", \"u_xxxx\", \"(u^2)_x\"]\n",
    "    \n",
    "    # Normalize\n",
    "    Theta_mean = Theta.mean(0)\n",
    "    Theta_std = Theta.std(0) + 1e-8\n",
    "    Theta_n = (Theta - Theta_mean) / Theta_std\n",
    "    \n",
    "    # Lasso regression\n",
    "    lasso = Lasso(alpha=1e-4, fit_intercept=False, max_iter=50000, tol=1e-6)\n",
    "    lasso.fit(Theta_n, y)\n",
    "    coef = lasso.coef_ / Theta_std\n",
    "    \n",
    "    # Extract coefficients\n",
    "    a = coef[names.index(\"u_xx\")]\n",
    "    b = coef[names.index(\"u_xxxx\")]\n",
    "    c_phys = coef[names.index(\"(u^2)_x\")]\n",
    "    \n",
    "    # Predict u_t\n",
    "    y_pred = Theta @ coef\n",
    "    \n",
    "    # Compute metrics\n",
    "    rmse = np.sqrt(np.mean((y - y_pred)**2))\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    \n",
    "    # True coefficients (physical form)\n",
    "    a_true, b_true, c_true = -1.0, -1.0, -0.5\n",
    "    \n",
    "    # Coefficient errors\n",
    "    error_a = a - a_true\n",
    "    error_b = b - b_true\n",
    "    error_c = c_phys - c_true\n",
    "    \n",
    "    abs_error_a = abs(error_a)\n",
    "    abs_error_b = abs(error_b)\n",
    "    abs_error_c = abs(error_c)\n",
    "    \n",
    "    return {\n",
    "        'method': method_name,\n",
    "        'sigma': sigma_noise,\n",
    "        'a': a,\n",
    "        'b': b,\n",
    "        'c': c_phys,\n",
    "        'error_a': error_a,\n",
    "        'error_b': error_b,\n",
    "        'error_c': error_c,\n",
    "        'abs_error_a': abs_error_a,\n",
    "        'abs_error_b': abs_error_b,\n",
    "        'abs_error_c': abs_error_c,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2\n",
    "    }\n",
    "\n",
    "\n",
    "# ===== Main Processing =====\n",
    "\n",
    "# Noise datasets (excluding clean data)\n",
    "noise_files = [\n",
    "    \"ks_gaussian_sigma_0.005.h5\",\n",
    "    \"ks_gaussian_sigma_0.010.h5\",\n",
    "    \"ks_gaussian_sigma_0.030.h5\",\n",
    "    \"ks_gaussian_sigma_0.050.h5\",\n",
    "    \"ks_gaussian_sigma_0.080.h5\",\n",
    "    \"ks_gaussian_sigma_0.100.h5\"\n",
    "]\n",
    "\n",
    "noise_levels = [0.005, 0.01, 0.03, 0.05, 0.08, 0.1]\n",
    "\n",
    "# Store all results\n",
    "all_results = []\n",
    "\n",
    "# Process each noise level\n",
    "for file, sigma in zip(noise_files, noise_levels):\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"PROCESSING NOISE LEVEL σ = {sigma}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load noisy data\n",
    "    with h5py.File(file, \"r\") as f:\n",
    "        u_noisy = np.array(f[\"u\"], dtype=np.float32)\n",
    "        x = np.array(f[\"x\"], dtype=np.float32)\n",
    "        t = np.array(f[\"t\"], dtype=np.float32)\n",
    "    \n",
    "    print(f\"Loaded: {file}\")\n",
    "    print(f\"Shape: {u_noisy.shape}\")\n",
    "    \n",
    "    # ===== 1. NO DENOISING (Baseline) =====\n",
    "    print(\"\\n--- Method 1: No Denoising (Baseline) ---\")\n",
    "    result_baseline = evaluate_sindy_finite_diff(u_noisy, x, t, \n",
    "                                                   method_name=\"No Denoising\", \n",
    "                                                   sigma_noise=sigma)\n",
    "    all_results.append(result_baseline)\n",
    "    print(f\"  Coefficients: a={result_baseline['a']:.6f}, b={result_baseline['b']:.6f}, c={result_baseline['c']:.6f}\")\n",
    "    print(f\"  RMSE: {result_baseline['rmse']:.6f}, R²: {result_baseline['r2']:.6f}\")\n",
    "    \n",
    "    \n",
    "    # ===== 2. GAUSSIAN DENOISING =====\n",
    "    print(\"\\n--- Method 2: Gaussian Smoothing ---\")\n",
    "    u_gaussian = gaussian_denoising(u_noisy, sigma=2.0)\n",
    "    result_gaussian = evaluate_sindy_finite_diff(u_gaussian, x, t, \n",
    "                                                   method_name=\"Gaussian\", \n",
    "                                                   sigma_noise=sigma)\n",
    "    all_results.append(result_gaussian)\n",
    "    print(f\"  Coefficients: a={result_gaussian['a']:.6f}, b={result_gaussian['b']:.6f}, c={result_gaussian['c']:.6f}\")\n",
    "    print(f\"  RMSE: {result_gaussian['rmse']:.6f}, R²: {result_gaussian['r2']:.6f}\")\n",
    "    \n",
    "    \n",
    "    # ===== 3. SAVITZKY-GOLAY DENOISING =====\n",
    "    print(\"\\n--- Method 3: Savitzky-Golay Smoothing ---\")\n",
    "    u_savgol = savitzky_golay_denoising(u_noisy, window_length=11, polyorder=3)\n",
    "    result_savgol = evaluate_sindy_finite_diff(u_savgol, x, t, \n",
    "                                                 method_name=\"Savitzky-Golay\", \n",
    "                                                 sigma_noise=sigma)\n",
    "    all_results.append(result_savgol)\n",
    "    print(f\"  Coefficients: a={result_savgol['a']:.6f}, b={result_savgol['b']:.6f}, c={result_savgol['c']:.6f}\")\n",
    "    print(f\"  RMSE: {result_savgol['rmse']:.6f}, R²: {result_savgol['r2']:.6f}\")\n",
    "    \n",
    "    \n",
    "    # ===== 4. SPECTRAL LOW-PASS DENOISING =====\n",
    "    print(\"\\n--- Method 4: Spectral Low-Pass Filtering ---\")\n",
    "    u_spectral = spectral_lowpass_denoising(u_noisy, cutoff_ratio=0.5)\n",
    "    result_spectral = evaluate_sindy_finite_diff(u_spectral, x, t, \n",
    "                                                   method_name=\"Spectral\", \n",
    "                                                   sigma_noise=sigma)\n",
    "    all_results.append(result_spectral)\n",
    "    print(f\"  Coefficients: a={result_spectral['a']:.6f}, b={result_spectral['b']:.6f}, c={result_spectral['c']:.6f}\")\n",
    "    print(f\"  RMSE: {result_spectral['rmse']:.6f}, R²: {result_spectral['r2']:.6f}\")\n",
    "\n",
    "\n",
    "# ===== Convert to DataFrame =====\n",
    "df_results = pd.DataFrame(all_results)\n",
    "\n",
    "# ===== SUMMARY TABLES =====\n",
    "print(\"\\n\\n\" + \"=\"*100)\n",
    "print(\"SUMMARY TABLE 1: RECOVERED COEFFICIENTS BY DENOISING METHOD\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Sigma':>8} {'Method':>18} {'a (u_xx)':>12} {'b (u_xxxx)':>12} {'c ((u²)_x)':>12} {'Err_a':>10} {'Err_b':>10} {'Err_c':>10}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "for _, row in df_results.iterrows():\n",
    "    print(f\"{row['sigma']:>8.3f} {row['method']:>18} {row['a']:>12.6f} {row['b']:>12.6f} {row['c']:>12.6f} \"\n",
    "          f\"{row['error_a']:>10.6f} {row['error_b']:>10.6f} {row['error_c']:>10.6f}\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(f\"{'TRUE:':>27} {-1.0:>12.6f} {-1.0:>12.6f} {-0.5:>12.6f}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*100)\n",
    "print(\"SUMMARY TABLE 2: EVALUATION METRICS BY DENOISING METHOD\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Sigma':>8} {'Method':>18} {'RMSE':>12} {'R²':>12} {'|Err_a|':>10} {'|Err_b|':>10} {'|Err_c|':>10}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "for _, row in df_results.iterrows():\n",
    "    print(f\"{row['sigma']:>8.3f} {row['method']:>18} {row['rmse']:>12.6f} {row['r2']:>12.6f} \"\n",
    "          f\"{row['abs_error_a']:>10.6f} {row['abs_error_b']:>10.6f} {row['abs_error_c']:>10.6f}\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "\n",
    "\n",
    "# ===== COMPARISON BY NOISE LEVEL =====\n",
    "print(\"\\n\\n\" + \"=\"*100)\n",
    "print(\"SUMMARY TABLE 3: BEST METHOD PER NOISE LEVEL (by RMSE)\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Sigma':>8} {'Best Method':>18} {'RMSE':>12} {'R²':>12} {'Coeff a':>12} {'Coeff b':>12} {'Coeff c':>12}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "for sigma in noise_levels:\n",
    "    sigma_data = df_results[df_results['sigma'] == sigma]\n",
    "    best_idx = sigma_data['rmse'].idxmin()\n",
    "    best = df_results.loc[best_idx]\n",
    "    print(f\"{sigma:>8.3f} {best['method']:>18} {best['rmse']:>12.6f} {best['r2']:>12.6f} \"\n",
    "          f\"{best['a']:>12.6f} {best['b']:>12.6f} {best['c']:>12.6f}\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "\n",
    "\n",
    "# ===== OVERALL STATISTICS =====\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"OVERALL STATISTICS BY METHOD (Averaged Across All Noise Levels)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "method_stats = df_results.groupby('method').agg({\n",
    "    'rmse': 'mean',\n",
    "    'r2': 'mean',\n",
    "    'abs_error_a': 'mean',\n",
    "    'abs_error_b': 'mean',\n",
    "    'abs_error_c': 'mean'\n",
    "}).round(6)\n",
    "\n",
    "print(method_stats)\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_method = method_stats['rmse'].idxmin()\n",
    "print(f\"\\n🏆 BEST OVERALL METHOD (by average RMSE): {best_method}\")\n",
    "print(f\"   Average RMSE: {method_stats.loc[best_method, 'rmse']:.6f}\")\n",
    "print(f\"   Average R²: {method_stats.loc[best_method, 'r2']:.6f}\")\n",
    "\n",
    "\n",
    "# ===== VISUALIZATION 1: Coefficient Recovery =====\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"GENERATING VISUALIZATIONS...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "methods = df_results['method'].unique()\n",
    "colors = {'No Denoising': 'red', 'Gaussian': 'blue', \n",
    "          'Savitzky-Golay': 'green', 'Spectral': 'purple'}\n",
    "markers = {'No Denoising': 'x', 'Gaussian': 'o', \n",
    "           'Savitzky-Golay': 's', 'Spectral': '^'}\n",
    "\n",
    "# Plot 1: Coefficient a recovery\n",
    "ax1 = plt.subplot(3, 3, 1)\n",
    "for method in methods:\n",
    "    data = df_results[df_results['method'] == method]\n",
    "    plt.plot(data['sigma'], data['a'], marker=markers[method], \n",
    "             label=method, linewidth=2, markersize=8, color=colors[method])\n",
    "plt.axhline(y=-1.0, color='black', linestyle='--', linewidth=2, label='True value')\n",
    "plt.xlabel('Noise Level (σ)')\n",
    "plt.ylabel('Coefficient a (u_xx)')\n",
    "plt.title('Coefficient a Recovery')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Coefficient b recovery\n",
    "ax2 = plt.subplot(3, 3, 2)\n",
    "for method in methods:\n",
    "    data = df_results[df_results['method'] == method]\n",
    "    plt.plot(data['sigma'], data['b'], marker=markers[method], \n",
    "             label=method, linewidth=2, markersize=8, color=colors[method])\n",
    "plt.axhline(y=-1.0, color='black', linestyle='--', linewidth=2, label='True value')\n",
    "plt.xlabel('Noise Level (σ)')\n",
    "plt.ylabel('Coefficient b (u_xxxx)')\n",
    "plt.title('Coefficient b Recovery')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Coefficient c recovery\n",
    "ax3 = plt.subplot(3, 3, 3)\n",
    "for method in methods:\n",
    "    data = df_results[df_results['method'] == method]\n",
    "    plt.plot(data['sigma'], data['c'], marker=markers[method], \n",
    "             label=method, linewidth=2, markersize=8, color=colors[method])\n",
    "plt.axhline(y=-0.5, color='black', linestyle='--', linewidth=2, label='True value')\n",
    "plt.xlabel('Noise Level (σ)')\n",
    "plt.ylabel('Coefficient c ((u²)_x)')\n",
    "plt.title('Coefficient c Recovery')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: RMSE comparison\n",
    "ax4 = plt.subplot(3, 3, 4)\n",
    "for method in methods:\n",
    "    data = df_results[df_results['method'] == method]\n",
    "    plt.plot(data['sigma'], data['rmse'], marker=markers[method], \n",
    "             label=method, linewidth=2, markersize=8, color=colors[method])\n",
    "plt.xlabel('Noise Level (σ)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Root Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Plot 5: R² comparison\n",
    "ax5 = plt.subplot(3, 3, 5)\n",
    "for method in methods:\n",
    "    data = df_results[df_results['method'] == method]\n",
    "    plt.plot(data['sigma'], data['r2'], marker=markers[method], \n",
    "             label=method, linewidth=2, markersize=8, color=colors[method])\n",
    "plt.xlabel('Noise Level (σ)')\n",
    "plt.ylabel('R² Score')\n",
    "plt.title('R² Coefficient of Determination')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Absolute error in a\n",
    "ax6 = plt.subplot(3, 3, 6)\n",
    "for method in methods:\n",
    "    data = df_results[df_results['method'] == method]\n",
    "    plt.plot(data['sigma'], data['abs_error_a'], marker=markers[method], \n",
    "             label=method, linewidth=2, markersize=8, color=colors[method])\n",
    "plt.xlabel('Noise Level (σ)')\n",
    "plt.ylabel('|Error in a|')\n",
    "plt.title('Absolute Error in Coefficient a')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 7: Absolute error in b\n",
    "ax7 = plt.subplot(3, 3, 7)\n",
    "for method in methods:\n",
    "    data = df_results[df_results['method'] == method]\n",
    "    plt.plot(data['sigma'], data['abs_error_b'], marker=markers[method], \n",
    "             label=method, linewidth=2, markersize=8, color=colors[method])\n",
    "plt.xlabel('Noise Level (σ)')\n",
    "plt.ylabel('|Error in b|')\n",
    "plt.title('Absolute Error in Coefficient b')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 8: Absolute error in c\n",
    "ax8 = plt.subplot(3, 3, 8)\n",
    "for method in methods:\n",
    "    data = df_results[df_results['method'] == method]\n",
    "    plt.plot(data['sigma'], data['abs_error_c'], marker=markers[method], \n",
    "             label=method, linewidth=2, markersize=8, color=colors[method])\n",
    "plt.xlabel('Noise Level (σ)')\n",
    "plt.ylabel('|Error in c|')\n",
    "plt.title('Absolute Error in Coefficient c')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 9: Bar chart of average performance\n",
    "ax9 = plt.subplot(3, 3, 9)\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "rmse_means = [method_stats.loc[m, 'rmse'] for m in methods]\n",
    "r2_means = [method_stats.loc[m, 'r2'] for m in methods]\n",
    "ax9_twin = ax9.twinx()\n",
    "bars1 = ax9.bar(x - width/2, rmse_means, width, label='Avg RMSE', alpha=0.8)\n",
    "bars2 = ax9_twin.bar(x + width/2, r2_means, width, label='Avg R²', alpha=0.8, color='orange')\n",
    "ax9.set_xlabel('Denoising Method')\n",
    "ax9.set_ylabel('Average RMSE', color='blue')\n",
    "ax9_twin.set_ylabel('Average R²', color='orange')\n",
    "ax9.set_xticks(x)\n",
    "ax9.set_xticklabels(methods, rotation=45, ha='right')\n",
    "ax9.set_title('Average Performance by Method')\n",
    "ax9.legend(loc='upper left')\n",
    "ax9_twin.legend(loc='upper right')\n",
    "ax9.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('denoising_finitediff_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"    Saved: denoising_finitediff_comparison.png\")\n",
    "\n",
    "\n",
    "# ===== VISUALIZATION 2: Heatmaps =====\n",
    "fig2, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Heatmap 1: RMSE\n",
    "pivot_rmse = df_results.pivot(index='method', columns='sigma', values='rmse')\n",
    "sns.heatmap(pivot_rmse, annot=True, fmt='.4f', cmap='RdYlGn_r', ax=axes[0, 0],\n",
    "            cbar_kws={'label': 'RMSE'})\n",
    "axes[0, 0].set_title('RMSE by Method and Noise Level', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Noise Level (σ)')\n",
    "axes[0, 0].set_ylabel('Denoising Method')\n",
    "\n",
    "# Heatmap 2: R²\n",
    "pivot_r2 = df_results.pivot(index='method', columns='sigma', values='r2')\n",
    "sns.heatmap(pivot_r2, annot=True, fmt='.4f', cmap='RdYlGn', ax=axes[0, 1],\n",
    "            cbar_kws={'label': 'R² Score'})\n",
    "axes[0, 1].set_title('R² Score by Method and Noise Level', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Noise Level (σ)')\n",
    "axes[0, 1].set_ylabel('Denoising Method')\n",
    "\n",
    "# Heatmap 3: Absolute Error in coefficient a\n",
    "pivot_err_a = df_results.pivot(index='method', columns='sigma', values='abs_error_a')\n",
    "sns.heatmap(pivot_err_a, annot=True, fmt='.4f', cmap='RdYlGn_r', ax=axes[1, 0],\n",
    "            cbar_kws={'label': '|Error in a|'})\n",
    "axes[1, 0].set_title('Absolute Error in Coefficient a', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Noise Level (σ)')\n",
    "axes[1, 0].set_ylabel('Denoising Method')\n",
    "\n",
    "# Heatmap 4: Absolute Error in coefficient b\n",
    "pivot_err_b = df_results.pivot(index='method', columns='sigma', values='abs_error_b')\n",
    "sns.heatmap(pivot_err_b, annot=True, fmt='.4f', cmap='RdYlGn_r', ax=axes[1, 1],\n",
    "            cbar_kws={'label': '|Error in b|'})\n",
    "axes[1, 1].set_title('Absolute Error in Coefficient b', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Noise Level (σ)')\n",
    "axes[1, 1].set_ylabel('Denoising Method')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('denoising_finitediff_heatmaps.png', dpi=300, bbox_inches='tight')\n",
    "print(\"    Saved: denoising_finitediff_heatmaps.png\")\n",
    "\n",
    "\n",
    "# ===== Save Results to CSV =====\n",
    "df_results.to_csv('denoising_finitediff_results.csv', index=False)\n",
    "print(\"    Saved: denoising_finitediff_results.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"   DENOISING + FINITE DIFFERENCE SINDY EVALUATION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  1. denoising_finitediff_comparison.png - Main comparison plots (9 subplots)\")\n",
    "print(\"  2. denoising_finitediff_heatmaps.png - Heatmaps of metrics\")\n",
    "print(\"  3. denoising_finitediff_results.csv - Complete results data\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7097a0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (18, 12)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DENOISING COMPARISON: FFT vs FINITE DIFFERENCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ===== Load Results =====\n",
    "print(\"\\n📂 Loading results...\")\n",
    "try:\n",
    "    fft_results = pd.read_csv('denoising_fft_results.csv')\n",
    "    fd_results = pd.read_csv('denoising_finitediff_results.csv')\n",
    "    print(\"    Successfully loaded both result files\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"    Error: {e}\")\n",
    "    print(\"Please run both denoising scripts first to generate the CSV files.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\nFFT Results: {len(fft_results)} rows\")\n",
    "print(f\"Finite Diff Results: {len(fd_results)} rows\")\n",
    "\n",
    "# ===== Overall Statistics =====\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"OVERALL STATISTICS: FFT vs FINITE DIFFERENCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fft_stats = fft_results.groupby('method').agg({\n",
    "    'rmse': 'mean',\n",
    "    'r2': 'mean',\n",
    "    'abs_error_a': 'mean',\n",
    "    'abs_error_b': 'mean',\n",
    "    'abs_error_c': 'mean'\n",
    "}).round(6)\n",
    "\n",
    "fd_stats = fd_results.groupby('method').agg({\n",
    "    'rmse': 'mean',\n",
    "    'r2': 'mean',\n",
    "    'abs_error_a': 'mean',\n",
    "    'abs_error_b': 'mean',\n",
    "    'abs_error_c': 'mean'\n",
    "}).round(6)\n",
    "\n",
    "print(\"\\n--- FFT-Based SINDy (Average Metrics) ---\")\n",
    "print(fft_stats)\n",
    "\n",
    "print(\"\\n--- Finite Difference SINDy (Average Metrics) ---\")\n",
    "print(fd_stats)\n",
    "\n",
    "# ===== Best Method Per Derivative Type =====\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"BEST DENOISING METHOD BY DERIVATIVE TYPE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fft_best = fft_stats['rmse'].idxmin()\n",
    "fd_best = fd_stats['rmse'].idxmin()\n",
    "\n",
    "print(f\"\\n🏆 FFT Best Method: {fft_best}\")\n",
    "print(f\"   Average RMSE: {fft_stats.loc[fft_best, 'rmse']:.6f}\")\n",
    "print(f\"   Average R²: {fft_stats.loc[fft_best, 'r2']:.6f}\")\n",
    "\n",
    "print(f\"\\n🏆 Finite Difference Best Method: {fd_best}\")\n",
    "print(f\"   Average RMSE: {fd_stats.loc[fd_best, 'rmse']:.6f}\")\n",
    "print(f\"   Average R²: {fd_stats.loc[fd_best, 'r2']:.6f}\")\n",
    "\n",
    "# ===== Method-by-Method Comparison =====\n",
    "print(\"\\n\\n\" + \"=\"*100)\n",
    "print(\"METHOD-BY-METHOD COMPARISON (Average RMSE)\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Method':>18} {'FFT RMSE':>15} {'FD RMSE':>15} {'Winner':>15} {'Improvement':>15}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "methods = fft_stats.index\n",
    "for method in methods:\n",
    "    fft_rmse = fft_stats.loc[method, 'rmse']\n",
    "    fd_rmse = fd_stats.loc[method, 'rmse']\n",
    "    winner = \"FFT\" if fft_rmse < fd_rmse else \"Finite Diff\"\n",
    "    improvement = abs(fft_rmse - fd_rmse) / max(fft_rmse, fd_rmse) * 100\n",
    "    print(f\"{method:>18} {fft_rmse:>15.6f} {fd_rmse:>15.6f} {winner:>15} {improvement:>14.2f}%\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "\n",
    "# ===== Noise Level Analysis =====\n",
    "print(\"\\n\\n\" + \"=\"*100)\n",
    "print(\"BEST METHOD PER NOISE LEVEL\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Sigma':>8} {'FFT Best':>18} {'FFT RMSE':>12} {'FD Best':>18} {'FD RMSE':>12} {'Overall Winner':>18}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "noise_levels = sorted(fft_results['sigma'].unique())\n",
    "for sigma in noise_levels:\n",
    "    fft_sigma = fft_results[fft_results['sigma'] == sigma]\n",
    "    fd_sigma = fd_results[fd_results['sigma'] == sigma]\n",
    "    \n",
    "    fft_best_idx = fft_sigma['rmse'].idxmin()\n",
    "    fd_best_idx = fd_sigma['rmse'].idxmin()\n",
    "    \n",
    "    fft_best_method = fft_results.loc[fft_best_idx, 'method']\n",
    "    fft_best_rmse = fft_results.loc[fft_best_idx, 'rmse']\n",
    "    \n",
    "    fd_best_method = fd_results.loc[fd_best_idx, 'method']\n",
    "    fd_best_rmse = fd_results.loc[fd_best_idx, 'rmse']\n",
    "    \n",
    "    overall_winner = \"FFT\" if fft_best_rmse < fd_best_rmse else \"Finite Diff\"\n",
    "    \n",
    "    print(f\"{sigma:>8.3f} {fft_best_method:>18} {fft_best_rmse:>12.6f} {fd_best_method:>18} {fd_best_rmse:>12.6f} {overall_winner:>18}\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "\n",
    "# ===== VISUALIZATION 1: Side-by-Side Comparison =====\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"GENERATING COMPARISON VISUALIZATIONS...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(20, 14))\n",
    "\n",
    "methods = ['No Denoising', 'Gaussian', 'Savitzky-Golay', 'Spectral']\n",
    "colors_fft = 'blue'\n",
    "colors_fd = 'green'\n",
    "\n",
    "# Row 1: Coefficient a\n",
    "for i, method in enumerate(methods):\n",
    "    ax = axes[0, i]\n",
    "    fft_data = fft_results[fft_results['method'] == method]\n",
    "    fd_data = fd_results[fd_results['method'] == method]\n",
    "    \n",
    "    ax.plot(fft_data['sigma'], fft_data['a'], 'o-', label='FFT', \n",
    "            linewidth=2, markersize=8, color=colors_fft)\n",
    "    ax.plot(fd_data['sigma'], fd_data['a'], 's--', label='Finite Diff', \n",
    "            linewidth=2, markersize=8, color=colors_fd)\n",
    "    ax.axhline(y=-1.0, color='red', linestyle=':', linewidth=2, label='True')\n",
    "    ax.set_title(f'{method}\\nCoefficient a', fontweight='bold')\n",
    "    ax.set_xlabel('Noise Level (σ)')\n",
    "    ax.set_ylabel('Coefficient a')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Row 2: RMSE\n",
    "for i, method in enumerate(methods):\n",
    "    ax = axes[1, i]\n",
    "    fft_data = fft_results[fft_results['method'] == method]\n",
    "    fd_data = fd_results[fd_results['method'] == method]\n",
    "    \n",
    "    ax.plot(fft_data['sigma'], fft_data['rmse'], 'o-', label='FFT', \n",
    "            linewidth=2, markersize=8, color=colors_fft)\n",
    "    ax.plot(fd_data['sigma'], fd_data['rmse'], 's--', label='Finite Diff', \n",
    "            linewidth=2, markersize=8, color=colors_fd)\n",
    "    ax.set_title(f'{method}\\nRMSE', fontweight='bold')\n",
    "    ax.set_xlabel('Noise Level (σ)')\n",
    "    ax.set_ylabel('RMSE')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "# Row 3: R²\n",
    "for i, method in enumerate(methods):\n",
    "    ax = axes[2, i]\n",
    "    fft_data = fft_results[fft_results['method'] == method]\n",
    "    fd_data = fd_results[fd_results['method'] == method]\n",
    "    \n",
    "    ax.plot(fft_data['sigma'], fft_data['r2'], 'o-', label='FFT', \n",
    "            linewidth=2, markersize=8, color=colors_fft)\n",
    "    ax.plot(fd_data['sigma'], fd_data['r2'], 's--', label='Finite Diff', \n",
    "            linewidth=2, markersize=8, color=colors_fd)\n",
    "    ax.set_title(f'{method}\\nR² Score', fontweight='bold')\n",
    "    ax.set_xlabel('Noise Level (σ)')\n",
    "    ax.set_ylabel('R²')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('denoising_fft_vs_fd_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"    Saved: denoising_fft_vs_fd_comparison.png\")\n",
    "\n",
    "\n",
    "# ===== VISUALIZATION 2: Average Performance Comparison =====\n",
    "fig2, axes2 = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Average RMSE\n",
    "ax1 = axes2[0, 0]\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "fft_rmse = [fft_stats.loc[m, 'rmse'] for m in methods]\n",
    "fd_rmse = [fd_stats.loc[m, 'rmse'] for m in methods]\n",
    "ax1.bar(x - width/2, fft_rmse, width, label='FFT', alpha=0.8, color='blue')\n",
    "ax1.bar(x + width/2, fd_rmse, width, label='Finite Diff', alpha=0.8, color='green')\n",
    "ax1.set_xlabel('Denoising Method')\n",
    "ax1.set_ylabel('Average RMSE')\n",
    "ax1.set_title('Average RMSE by Method', fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(methods, rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: Average R²\n",
    "ax2 = axes2[0, 1]\n",
    "fft_r2 = [fft_stats.loc[m, 'r2'] for m in methods]\n",
    "fd_r2 = [fd_stats.loc[m, 'r2'] for m in methods]\n",
    "ax2.bar(x - width/2, fft_r2, width, label='FFT', alpha=0.8, color='blue')\n",
    "ax2.bar(x + width/2, fd_r2, width, label='Finite Diff', alpha=0.8, color='green')\n",
    "ax2.set_xlabel('Denoising Method')\n",
    "ax2.set_ylabel('Average R²')\n",
    "ax2.set_title('Average R² Score by Method', fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(methods, rotation=45, ha='right')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 3: Average Absolute Error in coefficient a\n",
    "ax3 = axes2[1, 0]\n",
    "fft_err_a = [fft_stats.loc[m, 'abs_error_a'] for m in methods]\n",
    "fd_err_a = [fd_stats.loc[m, 'abs_error_a'] for m in methods]\n",
    "ax3.bar(x - width/2, fft_err_a, width, label='FFT', alpha=0.8, color='blue')\n",
    "ax3.bar(x + width/2, fd_err_a, width, label='Finite Diff', alpha=0.8, color='green')\n",
    "ax3.set_xlabel('Denoising Method')\n",
    "ax3.set_ylabel('Average |Error in a|')\n",
    "ax3.set_title('Average Absolute Error in Coefficient a', fontweight='bold')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(methods, rotation=45, ha='right')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 4: Average Absolute Error in coefficient b\n",
    "ax4 = axes2[1, 1]\n",
    "fft_err_b = [fft_stats.loc[m, 'abs_error_b'] for m in methods]\n",
    "fd_err_b = [fd_stats.loc[m, 'abs_error_b'] for m in methods]\n",
    "ax4.bar(x - width/2, fft_err_b, width, label='FFT', alpha=0.8, color='blue')\n",
    "ax4.bar(x + width/2, fd_err_b, width, label='Finite Diff', alpha=0.8, color='green')\n",
    "ax4.set_xlabel('Denoising Method')\n",
    "ax4.set_ylabel('Average |Error in b|')\n",
    "ax4.set_title('Average Absolute Error in Coefficient b', fontweight='bold')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(methods, rotation=45, ha='right')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('denoising_average_performance.png', dpi=300, bbox_inches='tight')\n",
    "print(\"    Saved: denoising_average_performance.png\")\n",
    "\n",
    "\n",
    "# ===== VISUALIZATION 3: Heatmap Comparison =====\n",
    "fig3, axes3 = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# FFT RMSE Heatmap\n",
    "pivot_fft_rmse = fft_results.pivot(index='method', columns='sigma', values='rmse')\n",
    "sns.heatmap(pivot_fft_rmse, annot=True, fmt='.4f', cmap='RdYlGn_r', ax=axes3[0, 0],\n",
    "            cbar_kws={'label': 'RMSE'})\n",
    "axes3[0, 0].set_title('FFT: RMSE by Method and Noise Level', fontsize=12, fontweight='bold')\n",
    "axes3[0, 0].set_xlabel('Noise Level (σ)')\n",
    "axes3[0, 0].set_ylabel('Denoising Method')\n",
    "\n",
    "# Finite Diff RMSE Heatmap\n",
    "pivot_fd_rmse = fd_results.pivot(index='method', columns='sigma', values='rmse')\n",
    "sns.heatmap(pivot_fd_rmse, annot=True, fmt='.4f', cmap='RdYlGn_r', ax=axes3[0, 1],\n",
    "            cbar_kws={'label': 'RMSE'})\n",
    "axes3[0, 1].set_title('Finite Diff: RMSE by Method and Noise Level', fontsize=12, fontweight='bold')\n",
    "axes3[0, 1].set_xlabel('Noise Level (σ)')\n",
    "axes3[0, 1].set_ylabel('Denoising Method')\n",
    "\n",
    "# FFT R² Heatmap\n",
    "pivot_fft_r2 = fft_results.pivot(index='method', columns='sigma', values='r2')\n",
    "sns.heatmap(pivot_fft_r2, annot=True, fmt='.4f', cmap='RdYlGn', ax=axes3[1, 0],\n",
    "            cbar_kws={'label': 'R² Score'})\n",
    "axes3[1, 0].set_title('FFT: R² Score by Method and Noise Level', fontsize=12, fontweight='bold')\n",
    "axes3[1, 0].set_xlabel('Noise Level (σ)')\n",
    "axes3[1, 0].set_ylabel('Denoising Method')\n",
    "\n",
    "# Finite Diff R² Heatmap\n",
    "pivot_fd_r2 = fd_results.pivot(index='method', columns='sigma', values='r2')\n",
    "sns.heatmap(pivot_fd_r2, annot=True, fmt='.4f', cmap='RdYlGn', ax=axes3[1, 1],\n",
    "            cbar_kws={'label': 'R² Score'})\n",
    "axes3[1, 1].set_title('Finite Diff: R² Score by Method and Noise Level', fontsize=12, fontweight='bold')\n",
    "axes3[1, 1].set_xlabel('Noise Level (σ)')\n",
    "axes3[1, 1].set_ylabel('Denoising Method')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('denoising_heatmap_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"    Saved: denoising_heatmap_comparison.png\")\n",
    "\n",
    "\n",
    "# ===== Save Comprehensive Comparison =====\n",
    "comparison_data = []\n",
    "for sigma in noise_levels:\n",
    "    for method in methods:\n",
    "        fft_row = fft_results[(fft_results['sigma'] == sigma) & (fft_results['method'] == method)]\n",
    "        fd_row = fd_results[(fd_results['sigma'] == sigma) & (fd_results['method'] == method)]\n",
    "        \n",
    "        if not fft_row.empty and not fd_row.empty:\n",
    "            comparison_data.append({\n",
    "                'sigma': sigma,\n",
    "                'method': method,\n",
    "                'fft_a': fft_row.iloc[0]['a'],\n",
    "                'fd_a': fd_row.iloc[0]['a'],\n",
    "                'fft_b': fft_row.iloc[0]['b'],\n",
    "                'fd_b': fd_row.iloc[0]['b'],\n",
    "                'fft_c': fft_row.iloc[0]['c'],\n",
    "                'fd_c': fd_row.iloc[0]['c'],\n",
    "                'fft_rmse': fft_row.iloc[0]['rmse'],\n",
    "                'fd_rmse': fd_row.iloc[0]['rmse'],\n",
    "                'fft_r2': fft_row.iloc[0]['r2'],\n",
    "                'fd_r2': fd_row.iloc[0]['r2']\n",
    "            })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "df_comparison.to_csv('denoising_fft_vs_fd_comparison.csv', index=False)\n",
    "print(\"    Saved: denoising_fft_vs_fd_comparison.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"   COMPREHENSIVE COMPARISON COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  1. denoising_fft_vs_fd_comparison.png - Side-by-side comparisons\")\n",
    "print(\"  2. denoising_average_performance.png - Average performance metrics\")\n",
    "print(\"  3. denoising_heatmap_comparison.png - Heatmap comparisons\")\n",
    "print(\"  4. denoising_fft_vs_fd_comparison.csv - Complete comparison data\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb61c06",
   "metadata": {},
   "source": [
    "Phase 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66e2fae",
   "metadata": {},
   "source": [
    "Cross-correlation registration + FFT derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92e88d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from scipy.fft import fft, ifft, fftfreq\n",
    "from scipy.signal import correlate\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 3A: SPATIAL TRANSLATION - CROSS-CORRELATION REGISTRATION + FFT SINDY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ===== Spectral derivative functions =====\n",
    "def spectral_derivative(u_snapshot, k):\n",
    "    \"\"\"First derivative using FFT\"\"\"\n",
    "    u_hat = fft(u_snapshot)\n",
    "    du_hat = 1j * k * u_hat\n",
    "    return np.real(ifft(du_hat))\n",
    "\n",
    "def spectral_second_derivative(u_snapshot, k):\n",
    "    \"\"\"Second derivative using FFT\"\"\"\n",
    "    u_hat = fft(u_snapshot)\n",
    "    d2u_hat = -(k**2) * u_hat\n",
    "    return np.real(ifft(d2u_hat))\n",
    "\n",
    "def spectral_fourth_derivative(u_snapshot, k):\n",
    "    \"\"\"Fourth derivative using FFT\"\"\"\n",
    "    u_hat = fft(u_snapshot)\n",
    "    d4u_hat = (k**4) * u_hat\n",
    "    return np.real(ifft(d4u_hat))\n",
    "\n",
    "\n",
    "# ===== Cross-Correlation Registration =====\n",
    "def find_shift_crosscorr(frame, reference):\n",
    "    \"\"\"\n",
    "    Find the shift between frame and reference using cross-correlation\n",
    "    Returns: optimal shift in pixels\n",
    "    \"\"\"\n",
    "    # Compute cross-correlation\n",
    "    correlation = correlate(frame, reference, mode='same', method='fft')\n",
    "    \n",
    "    # Find peak location\n",
    "    max_idx = np.argmax(correlation)\n",
    "    center = len(frame) // 2\n",
    "    shift = max_idx - center\n",
    "    \n",
    "    return shift\n",
    "\n",
    "\n",
    "def align_data_crosscorr(u_data, reference_frame):\n",
    "    \"\"\"\n",
    "    Align all frames to reference using cross-correlation\n",
    "    Returns: aligned data, detected shifts\n",
    "    \"\"\"\n",
    "    aligned_data = np.zeros_like(u_data)\n",
    "    detected_shifts = []\n",
    "    \n",
    "    for i in range(u_data.shape[0]):\n",
    "        shift = find_shift_crosscorr(u_data[i], reference_frame)\n",
    "        aligned_data[i] = np.roll(u_data[i], -shift)  # Undo the shift\n",
    "        detected_shifts.append(shift)\n",
    "    \n",
    "    return aligned_data, np.array(detected_shifts)\n",
    "\n",
    "\n",
    "# ===== Registration Quality Metrics =====\n",
    "def compute_registration_metrics(detected_shifts, aligned_data, reference_frame):\n",
    "    \"\"\"\n",
    "    Compute metrics to evaluate registration quality\n",
    "    \"\"\"\n",
    "    # Shift statistics\n",
    "    mean_shift = np.mean(np.abs(detected_shifts))\n",
    "    std_shift = np.std(detected_shifts)\n",
    "    max_shift = np.max(np.abs(detected_shifts))\n",
    "    \n",
    "    # Alignment quality: correlation with reference\n",
    "    correlations = []\n",
    "    for i in range(aligned_data.shape[0]):\n",
    "        corr = np.corrcoef(aligned_data[i], reference_frame)[0, 1]\n",
    "        correlations.append(corr)\n",
    "    \n",
    "    mean_correlation = np.mean(correlations)\n",
    "    \n",
    "    # Residual variance (lower is better)\n",
    "    residuals = aligned_data - reference_frame\n",
    "    residual_variance = np.var(residuals)\n",
    "    \n",
    "    return {\n",
    "        'mean_absolute_shift': mean_shift,\n",
    "        'std_shift': std_shift,\n",
    "        'max_shift': max_shift,\n",
    "        'mean_correlation': mean_correlation,\n",
    "        'residual_variance': residual_variance,\n",
    "        'detected_shifts': detected_shifts\n",
    "    }\n",
    "\n",
    "\n",
    "# ===== Evaluation Function =====\n",
    "def evaluate_sindy_fft(u, x, t, method_name=\"Unknown\"):\n",
    "    \"\"\"\n",
    "    Apply FFT-based SINDy and compute evaluation metrics\n",
    "    Returns: dict with coefficients, R², RMSE, errors\n",
    "    \"\"\"\n",
    "    Nx = len(x)\n",
    "    dx = x[1] - x[0]\n",
    "    dt = t[1] - t[0]\n",
    "    \n",
    "    # Define wavenumbers\n",
    "    k = 2 * np.pi * fftfreq(Nx, d=dx)\n",
    "    \n",
    "    # Compute temporal derivative (central difference)\n",
    "    u_t = (u[2:] - u[:-2]) / (2 * dt)\n",
    "    u_mid = u[1:-1]\n",
    "    \n",
    "    # Build feature library using FFT\n",
    "    Theta = []\n",
    "    ut_flat = []\n",
    "    \n",
    "    for snapshot, ut_snapshot in zip(u_mid, u_t):\n",
    "        ux = spectral_derivative(snapshot, k)\n",
    "        uxx = spectral_second_derivative(snapshot, k)\n",
    "        uxxxx = spectral_fourth_derivative(snapshot, k)\n",
    "        \n",
    "        # Feature terms: [u_xx, u_xxxx, u*u_x]\n",
    "        Theta_snapshot = np.vstack([\n",
    "            uxx,\n",
    "            uxxxx,\n",
    "            snapshot * ux\n",
    "        ]).T  # shape (Nx, 3)\n",
    "        \n",
    "        Theta.append(Theta_snapshot)\n",
    "        ut_flat.append(ut_snapshot)\n",
    "    \n",
    "    Theta = np.vstack(Theta)\n",
    "    ut_flat = np.hstack(ut_flat)\n",
    "    \n",
    "    # Normalize features\n",
    "    Theta_mean = Theta.mean(axis=0)\n",
    "    Theta_std = Theta.std(axis=0)\n",
    "    Theta_norm = (Theta - Theta_mean) / Theta_std\n",
    "    \n",
    "    # Lasso regression\n",
    "    lasso = Lasso(alpha=1e-4, fit_intercept=False, max_iter=50000)\n",
    "    lasso.fit(Theta_norm, ut_flat)\n",
    "    coeffs = lasso.coef_ / Theta_std\n",
    "    \n",
    "    # Extract coefficients\n",
    "    a = coeffs[0]  # u_xx\n",
    "    b = coeffs[1]  # u_xxxx\n",
    "    c = coeffs[2]  # u*u_x\n",
    "    \n",
    "    # Predict u_t\n",
    "    u_t_pred = Theta @ coeffs\n",
    "    \n",
    "    # Compute metrics\n",
    "    rmse = np.sqrt(np.mean((ut_flat - u_t_pred)**2))\n",
    "    r2 = r2_score(ut_flat, u_t_pred)\n",
    "    \n",
    "    # True coefficients\n",
    "    a_true, b_true, c_true = -1.0, -1.0, -1.0\n",
    "    \n",
    "    # Coefficient errors\n",
    "    error_a = a - a_true\n",
    "    error_b = b - b_true\n",
    "    error_c = c - c_true\n",
    "    \n",
    "    abs_error_a = abs(error_a)\n",
    "    abs_error_b = abs(error_b)\n",
    "    abs_error_c = abs(error_c)\n",
    "    \n",
    "    return {\n",
    "        'method': method_name,\n",
    "        'a': a,\n",
    "        'b': b,\n",
    "        'c': c,\n",
    "        'error_a': error_a,\n",
    "        'error_b': error_b,\n",
    "        'error_c': error_c,\n",
    "        'abs_error_a': abs_error_a,\n",
    "        'abs_error_b': abs_error_b,\n",
    "        'abs_error_c': abs_error_c,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2\n",
    "    }\n",
    "\n",
    "\n",
    "# ===== Main Processing =====\n",
    "\n",
    "print(\"\\n📂 Loading spatial translation data...\")\n",
    "with h5py.File(\"test_solving_euler_ks_spatial_shift.h5\", \"r\") as f:\n",
    "    u_shifted = np.array(f[\"u\"])\n",
    "    x = np.array(f[\"x\"])\n",
    "    t = np.array(f[\"t\"])\n",
    "\n",
    "print(f\"Loaded: test_solving_euler_ks_spatial_shift.h5\")\n",
    "print(f\"Shape: {u_shifted.shape}\")\n",
    "\n",
    "# ===== Compute Mean Frame as Reference =====\n",
    "print(\"\\n🎯 Computing mean frame as reference...\")\n",
    "reference_frame = np.mean(u_shifted, axis=0)\n",
    "print(f\"Reference frame shape: {reference_frame.shape}\")\n",
    "\n",
    "# ===== Method 1: No Registration (Baseline) =====\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"METHOD 1: NO REGISTRATION (BASELINE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "result_baseline = evaluate_sindy_fft(u_shifted, x, t, method_name=\"No Registration\")\n",
    "\n",
    "print(f\"\\nRecovered Coefficients:\")\n",
    "print(f\"  a (u_xx):   {result_baseline['a']:.6f}  (true: -1.0)\")\n",
    "print(f\"  b (u_xxxx): {result_baseline['b']:.6f}  (true: -1.0)\")\n",
    "print(f\"  c (u*u_x):  {result_baseline['c']:.6f}  (true: -1.0)\")\n",
    "print(f\"\\nEvaluation Metrics:\")\n",
    "print(f\"  RMSE: {result_baseline['rmse']:.6f}\")\n",
    "print(f\"  R²:   {result_baseline['r2']:.6f}\")\n",
    "\n",
    "# ===== Method 2: Cross-Correlation Registration =====\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"METHOD 2: CROSS-CORRELATION REGISTRATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"Aligning frames using cross-correlation...\")\n",
    "u_aligned, detected_shifts = align_data_crosscorr(u_shifted, reference_frame)\n",
    "\n",
    "print(f\"    Alignment complete!\")\n",
    "print(f\"  Detected shifts: min={detected_shifts.min()}, max={detected_shifts.max()}\")\n",
    "print(f\"  Mean absolute shift: {np.mean(np.abs(detected_shifts)):.2f} pixels\")\n",
    "\n",
    "# Compute registration quality metrics\n",
    "reg_metrics = compute_registration_metrics(detected_shifts, u_aligned, reference_frame)\n",
    "\n",
    "print(f\"\\nRegistration Quality Metrics:\")\n",
    "print(f\"  Mean Absolute Shift: {reg_metrics['mean_absolute_shift']:.4f} pixels\")\n",
    "print(f\"  Std Shift:           {reg_metrics['std_shift']:.4f} pixels\")\n",
    "print(f\"  Max Shift:           {reg_metrics['max_shift']:.1f} pixels\")\n",
    "print(f\"  Mean Correlation:    {reg_metrics['mean_correlation']:.6f}\")\n",
    "print(f\"  Residual Variance:   {reg_metrics['residual_variance']:.6f}\")\n",
    "\n",
    "# Apply SINDy to aligned data\n",
    "result_registered = evaluate_sindy_fft(u_aligned, x, t, method_name=\"Cross-Correlation\")\n",
    "\n",
    "print(f\"\\nRecovered Coefficients (After Registration):\")\n",
    "print(f\"  a (u_xx):   {result_registered['a']:.6f}  (true: -1.0)\")\n",
    "print(f\"  b (u_xxxx): {result_registered['b']:.6f}  (true: -1.0)\")\n",
    "print(f\"  c (u*u_x):  {result_registered['c']:.6f}  (true: -1.0)\")\n",
    "print(f\"\\nEvaluation Metrics:\")\n",
    "print(f\"  RMSE: {result_registered['rmse']:.6f}\")\n",
    "print(f\"  R²:   {result_registered['r2']:.6f}\")\n",
    "\n",
    "# ===== Load Clean Data for Comparison =====\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING CLEAN DATA FOR COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with h5py.File(\"test_solving_euler_ks_clean.h5\", \"r\") as f:\n",
    "    u_clean = np.array(f[\"u\"])\n",
    "\n",
    "result_clean = evaluate_sindy_fft(u_clean, x, t, method_name=\"Clean Data\")\n",
    "\n",
    "print(f\"\\nRecovered Coefficients (Clean Data):\")\n",
    "print(f\"  a (u_xx):   {result_clean['a']:.6f}  (true: -1.0)\")\n",
    "print(f\"  b (u_xxxx): {result_clean['b']:.6f}  (true: -1.0)\")\n",
    "print(f\"  c (u*u_x):  {result_clean['c']:.6f}  (true: -1.0)\")\n",
    "print(f\"\\nEvaluation Metrics:\")\n",
    "print(f\"  RMSE: {result_clean['rmse']:.6f}\")\n",
    "print(f\"  R²:   {result_clean['r2']:.6f}\")\n",
    "\n",
    "# ===== Summary Comparison =====\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = [result_clean, result_baseline, result_registered]\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(f\"\\n{'Method':<25} {'a (u_xx)':>12} {'b (u_xxxx)':>12} {'c (u*u_x)':>12} {'RMSE':>12} {'R²':>12}\")\n",
    "print(\"-\"*80)\n",
    "for _, row in df_results.iterrows():\n",
    "    print(f\"{row['method']:<25} {row['a']:>12.6f} {row['b']:>12.6f} {row['c']:>12.6f} \"\n",
    "          f\"{row['rmse']:>12.6f} {row['r2']:>12.6f}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'TRUE VALUES:':<25} {-1.0:>12.6f} {-1.0:>12.6f} {-1.0:>12.6f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ===== Improvement Analysis =====\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IMPROVEMENT FROM REGISTRATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "improvement_rmse = (result_baseline['rmse'] - result_registered['rmse']) / result_baseline['rmse'] * 100\n",
    "improvement_r2 = (result_registered['r2'] - result_baseline['r2']) / max(abs(result_baseline['r2']), 1e-10) * 100\n",
    "\n",
    "print(f\"\\nRMSE Improvement:  {improvement_rmse:+.2f}%\")\n",
    "print(f\"R² Improvement:    {improvement_r2:+.2f}%\")\n",
    "print(f\"\\nCoefficient Error Reduction:\")\n",
    "print(f\"  a: {result_baseline['abs_error_a']:.6f} → {result_registered['abs_error_a']:.6f} \"\n",
    "      f\"({(result_baseline['abs_error_a'] - result_registered['abs_error_a'])/result_baseline['abs_error_a']*100:+.1f}%)\")\n",
    "print(f\"  b: {result_baseline['abs_error_b']:.6f} → {result_registered['abs_error_b']:.6f} \"\n",
    "      f\"({(result_baseline['abs_error_b'] - result_registered['abs_error_b'])/result_baseline['abs_error_b']*100:+.1f}%)\")\n",
    "print(f\"  c: {result_baseline['abs_error_c']:.6f} → {result_registered['abs_error_c']:.6f} \"\n",
    "      f\"({(result_baseline['abs_error_c'] - result_registered['abs_error_c'])/result_baseline['abs_error_c']*100:+.1f}%)\")\n",
    "\n",
    "# ===== VISUALIZATIONS =====\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"GENERATING VISUALIZATIONS...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Plot 1: Detected Shifts Histogram\n",
    "ax1 = plt.subplot(3, 3, 1)\n",
    "plt.hist(detected_shifts, bins=range(int(detected_shifts.min())-1, int(detected_shifts.max())+2), \n",
    "         edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Detected Shift (pixels)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Detected Shifts\\n(Cross-Correlation)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axvline(0, color='red', linestyle='--', linewidth=2, label='Zero shift')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 2: Shifts Over Time\n",
    "ax2 = plt.subplot(3, 3, 2)\n",
    "plt.plot(detected_shifts, 'o-', markersize=2, linewidth=0.5)\n",
    "plt.xlabel('Frame Index')\n",
    "plt.ylabel('Detected Shift (pixels)')\n",
    "plt.title('Detected Shifts Over Time')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "# Plot 3: Sample Frame Comparison\n",
    "ax3 = plt.subplot(3, 3, 3)\n",
    "sample_idx = len(u_shifted) // 2\n",
    "plt.plot(x, u_shifted[sample_idx], 'r-', label='Before Registration', alpha=0.7, linewidth=2)\n",
    "plt.plot(x, u_aligned[sample_idx], 'b-', label='After Registration', alpha=0.7, linewidth=2)\n",
    "plt.plot(x, reference_frame, 'k--', label='Reference (Mean)', alpha=0.5, linewidth=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('u(x)')\n",
    "plt.title(f'Sample Frame Alignment\\n(Frame {sample_idx})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Coefficient Comparison\n",
    "ax4 = plt.subplot(3, 3, 4)\n",
    "methods = ['Clean', 'No Reg', 'Cross-Corr']\n",
    "a_vals = [result_clean['a'], result_baseline['a'], result_registered['a']]\n",
    "b_vals = [result_clean['b'], result_baseline['b'], result_registered['b']]\n",
    "c_vals = [result_clean['c'], result_baseline['c'], result_registered['c']]\n",
    "x_pos = np.arange(len(methods))\n",
    "width = 0.25\n",
    "plt.bar(x_pos - width, a_vals, width, label='a (u_xx)', alpha=0.8)\n",
    "plt.bar(x_pos, b_vals, width, label='b (u_xxxx)', alpha=0.8)\n",
    "plt.bar(x_pos + width, c_vals, width, label='c (u*u_x)', alpha=0.8)\n",
    "plt.axhline(y=-1.0, color='red', linestyle='--', linewidth=2, label='True')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('Recovered Coefficients Comparison')\n",
    "plt.xticks(x_pos, methods)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 5: RMSE Comparison\n",
    "ax5 = plt.subplot(3, 3, 5)\n",
    "rmse_vals = [result_clean['rmse'], result_baseline['rmse'], result_registered['rmse']]\n",
    "colors = ['green', 'red', 'blue']\n",
    "plt.bar(methods, rmse_vals, color=colors, alpha=0.7, edgecolor='black')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE Comparison')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.yscale('log')\n",
    "\n",
    "# Plot 6: R² Comparison\n",
    "ax6 = plt.subplot(3, 3, 6)\n",
    "r2_vals = [result_clean['r2'], result_baseline['r2'], result_registered['r2']]\n",
    "plt.bar(methods, r2_vals, color=colors, alpha=0.7, edgecolor='black')\n",
    "plt.ylabel('R² Score')\n",
    "plt.title('R² Comparison')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.axhline(y=1.0, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "# Plot 7: Absolute Errors Comparison\n",
    "ax7 = plt.subplot(3, 3, 7)\n",
    "err_a = [result_clean['abs_error_a'], result_baseline['abs_error_a'], result_registered['abs_error_a']]\n",
    "err_b = [result_clean['abs_error_b'], result_baseline['abs_error_b'], result_registered['abs_error_b']]\n",
    "err_c = [result_clean['abs_error_c'], result_baseline['abs_error_c'], result_registered['abs_error_c']]\n",
    "x_pos = np.arange(len(methods))\n",
    "width = 0.25\n",
    "plt.bar(x_pos - width, err_a, width, label='|Err a|', alpha=0.8)\n",
    "plt.bar(x_pos, err_b, width, label='|Err b|', alpha=0.8)\n",
    "plt.bar(x_pos + width, err_c, width, label='|Err c|', alpha=0.8)\n",
    "plt.ylabel('Absolute Error')\n",
    "plt.title('Absolute Coefficient Errors')\n",
    "plt.xticks(x_pos, methods)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.yscale('log')\n",
    "\n",
    "# Plot 8: Spatiotemporal plot - Before Registration\n",
    "ax8 = plt.subplot(3, 3, 8)\n",
    "im1 = plt.imshow(u_shifted[:500].T, cmap='RdBu', aspect='auto', origin='lower',\n",
    "                 extent=(0, 500*t[1], 0, x[-1]))\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Space (x)')\n",
    "plt.title('Before Registration\\n(First 500 frames)')\n",
    "plt.colorbar(im1, ax=ax8)\n",
    "\n",
    "# Plot 9: Spatiotemporal plot - After Registration\n",
    "ax9 = plt.subplot(3, 3, 9)\n",
    "im2 = plt.imshow(u_aligned[:500].T, cmap='RdBu', aspect='auto', origin='lower',\n",
    "                 extent=(0, 500*t[1], 0, x[-1]))\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Space (x)')\n",
    "plt.title('After Registration\\n(First 500 frames)')\n",
    "plt.colorbar(im2, ax=ax9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('registration_crosscorr_fft_results.png', dpi=300, bbox_inches='tight')\n",
    "print(\"    Saved: registration_crosscorr_fft_results.png\")\n",
    "\n",
    "# ===== Save Results =====\n",
    "results_dict = {\n",
    "    'method': ['Clean Data', 'No Registration', 'Cross-Correlation'],\n",
    "    'a': [result_clean['a'], result_baseline['a'], result_registered['a']],\n",
    "    'b': [result_clean['b'], result_baseline['b'], result_registered['b']],\n",
    "    'c': [result_clean['c'], result_baseline['c'], result_registered['c']],\n",
    "    'rmse': [result_clean['rmse'], result_baseline['rmse'], result_registered['rmse']],\n",
    "    'r2': [result_clean['r2'], result_baseline['r2'], result_registered['r2']],\n",
    "    'abs_error_a': [result_clean['abs_error_a'], result_baseline['abs_error_a'], result_registered['abs_error_a']],\n",
    "    'abs_error_b': [result_clean['abs_error_b'], result_baseline['abs_error_b'], result_registered['abs_error_b']],\n",
    "    'abs_error_c': [result_clean['abs_error_c'], result_baseline['abs_error_c'], result_registered['abs_error_c']]\n",
    "}\n",
    "\n",
    "# Add registration metrics for cross-correlation method\n",
    "results_dict['mean_shift'] = [0.0, np.nan, reg_metrics['mean_absolute_shift']]\n",
    "results_dict['std_shift'] = [0.0, np.nan, reg_metrics['std_shift']]\n",
    "results_dict['max_shift'] = [0.0, np.nan, reg_metrics['max_shift']]\n",
    "results_dict['mean_correlation'] = [1.0, np.nan, reg_metrics['mean_correlation']]\n",
    "\n",
    "df_results_full = pd.DataFrame(results_dict)\n",
    "df_results_full.to_csv('registration_crosscorr_fft_results.csv', index=False)\n",
    "print(\"    Saved: registration_crosscorr_fft_results.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"   CROSS-CORRELATION REGISTRATION + FFT SINDY COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  1. registration_crosscorr_fft_results.png - Visualization\")\n",
    "print(\"  2. registration_crosscorr_fft_results.csv - Results data\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7b001a",
   "metadata": {},
   "source": [
    "Cross-correlation registration + Finite Difference derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5f07ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from scipy.signal import correlate\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 3A: SPATIAL TRANSLATION - CROSS-CORRELATION REGISTRATION + FINITE DIFFERENCE SINDY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ===== Cross-Correlation Registration =====\n",
    "def find_shift_crosscorr(frame, reference):\n",
    "    \"\"\"Find the shift between frame and reference using cross-correlation\"\"\"\n",
    "    correlation = correlate(frame, reference, mode='same', method='fft')\n",
    "    max_idx = np.argmax(correlation)\n",
    "    center = len(frame) // 2\n",
    "    shift = max_idx - center\n",
    "    return shift\n",
    "\n",
    "def align_data_crosscorr(u_data, reference_frame):\n",
    "    \"\"\"Align all frames to reference using cross-correlation\"\"\"\n",
    "    aligned_data = np.zeros_like(u_data)\n",
    "    detected_shifts = []\n",
    "    \n",
    "    for i in range(u_data.shape[0]):\n",
    "        shift = find_shift_crosscorr(u_data[i], reference_frame)\n",
    "        aligned_data[i] = np.roll(u_data[i], -shift)\n",
    "        detected_shifts.append(shift)\n",
    "    \n",
    "    return aligned_data, np.array(detected_shifts)\n",
    "\n",
    "# ===== Registration Quality Metrics =====\n",
    "def compute_registration_metrics(detected_shifts, aligned_data, reference_frame):\n",
    "    \"\"\"Compute metrics to evaluate registration quality\"\"\"\n",
    "    mean_shift = np.mean(np.abs(detected_shifts))\n",
    "    std_shift = np.std(detected_shifts)\n",
    "    max_shift = np.max(np.abs(detected_shifts))\n",
    "    \n",
    "    correlations = []\n",
    "    for i in range(aligned_data.shape[0]):\n",
    "        corr = np.corrcoef(aligned_data[i], reference_frame)[0, 1]\n",
    "        correlations.append(corr)\n",
    "    \n",
    "    mean_correlation = np.mean(correlations)\n",
    "    residuals = aligned_data - reference_frame\n",
    "    residual_variance = np.var(residuals)\n",
    "    \n",
    "    return {\n",
    "        'mean_absolute_shift': mean_shift,\n",
    "        'std_shift': std_shift,\n",
    "        'max_shift': max_shift,\n",
    "        'mean_correlation': mean_correlation,\n",
    "        'residual_variance': residual_variance,\n",
    "        'detected_shifts': detected_shifts\n",
    "    }\n",
    "\n",
    "# ===== Evaluation Function with Finite Difference =====\n",
    "def evaluate_sindy_finite_diff(u, x, t, method_name=\"Unknown\"):\n",
    "    \"\"\"Apply Finite Difference SINDy and compute evaluation metrics\"\"\"\n",
    "    dx = float(x[1] - x[0])\n",
    "    dt = float(t[1] - t[0])\n",
    "    \n",
    "    # Finite difference operators\n",
    "    def d1(U): \n",
    "        return (np.roll(U, -1, 1) - np.roll(U, 1, 1)) / (2*dx)\n",
    "    \n",
    "    def d2(U): \n",
    "        return (np.roll(U, -1, 1) - 2*U + np.roll(U, 1, 1)) / (dx**2)\n",
    "    \n",
    "    def d4(U):\n",
    "        return (np.roll(U, -2, 1) - 4*np.roll(U, -1, 1) + 6*U - \n",
    "                4*np.roll(U, 1, 1) + np.roll(U, 2, 1)) / (dx**4)\n",
    "    \n",
    "    # Compute derivatives\n",
    "    U_t = (u[1:] - u[:-1]) / dt\n",
    "    U = u[:-1]\n",
    "    U_x = d1(U)\n",
    "    U_xx = d2(U)\n",
    "    U_xxxx = d4(U)\n",
    "    U2_x = d1(U**2)\n",
    "    \n",
    "    # Build feature matrix\n",
    "    Theta = np.column_stack([\n",
    "        np.ones_like(U).ravel(),\n",
    "        U.ravel(),\n",
    "        U_x.ravel(),\n",
    "        U_xx.ravel(),\n",
    "        U_xxxx.ravel(),\n",
    "        U2_x.ravel()\n",
    "    ])\n",
    "    y = U_t.ravel()\n",
    "    names = [\"1\", \"u\", \"u_x\", \"u_xx\", \"u_xxxx\", \"(u^2)_x\"]\n",
    "    \n",
    "    # Normalize\n",
    "    Theta_mean = Theta.mean(0)\n",
    "    Theta_std = Theta.std(0) + 1e-8\n",
    "    Theta_n = (Theta - Theta_mean) / Theta_std\n",
    "    \n",
    "    # Lasso regression\n",
    "    lasso = Lasso(alpha=1e-4, fit_intercept=False, max_iter=50000, tol=1e-6)\n",
    "    lasso.fit(Theta_n, y)\n",
    "    coef = lasso.coef_ / Theta_std\n",
    "    \n",
    "    # Extract coefficients\n",
    "    a = coef[names.index(\"u_xx\")]\n",
    "    b = coef[names.index(\"u_xxxx\")]\n",
    "    c_phys = coef[names.index(\"(u^2)_x\")]\n",
    "    \n",
    "    # Predict u_t\n",
    "    y_pred = Theta @ coef\n",
    "    \n",
    "    # Compute metrics\n",
    "    rmse = np.sqrt(np.mean((y - y_pred)**2))\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    \n",
    "    # True coefficients (physical form)\n",
    "    a_true, b_true, c_true = -1.0, -1.0, -0.5\n",
    "    \n",
    "    # Coefficient errors\n",
    "    error_a = a - a_true\n",
    "    error_b = b - b_true\n",
    "    error_c = c_phys - c_true\n",
    "    \n",
    "    abs_error_a = abs(error_a)\n",
    "    abs_error_b = abs(error_b)\n",
    "    abs_error_c = abs(error_c)\n",
    "    \n",
    "    return {\n",
    "        'method': method_name,\n",
    "        'a': a,\n",
    "        'b': b,\n",
    "        'c': c_phys,\n",
    "        'error_a': error_a,\n",
    "        'error_b': error_b,\n",
    "        'error_c': error_c,\n",
    "        'abs_error_a': abs_error_a,\n",
    "        'abs_error_b': abs_error_b,\n",
    "        'abs_error_c': abs_error_c,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2\n",
    "    }\n",
    "\n",
    "# ===== Main Processing =====\n",
    "print(\"\\n📂 Loading spatial translation data...\")\n",
    "with h5py.File(\"test_solving_euler_ks_spatial_shift.h5\", \"r\") as f:\n",
    "    u_shifted = np.array(f[\"u\"], dtype=np.float32)\n",
    "    x = np.array(f[\"x\"], dtype=np.float32)\n",
    "    t = np.array(f[\"t\"], dtype=np.float32)\n",
    "\n",
    "print(f\"Loaded: test_solving_euler_ks_spatial_shift.h5\")\n",
    "print(f\"Shape: {u_shifted.shape}\")\n",
    "\n",
    "# Compute Mean Frame as Reference\n",
    "print(\"\\n🎯 Computing mean frame as reference...\")\n",
    "reference_frame = np.mean(u_shifted, axis=0)\n",
    "\n",
    "# Method 1: No Registration (Baseline)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"METHOD 1: NO REGISTRATION (BASELINE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "result_baseline = evaluate_sindy_finite_diff(u_shifted, x, t, method_name=\"No Registration\")\n",
    "\n",
    "print(f\"\\nRecovered Coefficients:\")\n",
    "print(f\"  a (u_xx):    {result_baseline['a']:.6f}  (true: -1.0)\")\n",
    "print(f\"  b (u_xxxx):  {result_baseline['b']:.6f}  (true: -1.0)\")\n",
    "print(f\"  c ((u²)_x):  {result_baseline['c']:.6f}  (true: -0.5)\")\n",
    "print(f\"\\nEvaluation Metrics:\")\n",
    "print(f\"  RMSE: {result_baseline['rmse']:.6f}\")\n",
    "print(f\"  R²:   {result_baseline['r2']:.6f}\")\n",
    "\n",
    "# Method 2: Cross-Correlation Registration\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"METHOD 2: CROSS-CORRELATION REGISTRATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"Aligning frames using cross-correlation...\")\n",
    "u_aligned, detected_shifts = align_data_crosscorr(u_shifted, reference_frame)\n",
    "\n",
    "print(f\"    Alignment complete!\")\n",
    "print(f\"  Detected shifts: min={detected_shifts.min()}, max={detected_shifts.max()}\")\n",
    "print(f\"  Mean absolute shift: {np.mean(np.abs(detected_shifts)):.2f} pixels\")\n",
    "\n",
    "# Compute registration quality metrics\n",
    "reg_metrics = compute_registration_metrics(detected_shifts, u_aligned, reference_frame)\n",
    "\n",
    "print(f\"\\nRegistration Quality Metrics:\")\n",
    "print(f\"  Mean Absolute Shift: {reg_metrics['mean_absolute_shift']:.4f} pixels\")\n",
    "print(f\"  Std Shift:           {reg_metrics['std_shift']:.4f} pixels\")\n",
    "print(f\"  Max Shift:           {reg_metrics['max_shift']:.1f} pixels\")\n",
    "print(f\"  Mean Correlation:    {reg_metrics['mean_correlation']:.6f}\")\n",
    "print(f\"  Residual Variance:   {reg_metrics['residual_variance']:.6f}\")\n",
    "\n",
    "# Apply SINDy to aligned data\n",
    "result_registered = evaluate_sindy_finite_diff(u_aligned, x, t, method_name=\"Cross-Correlation\")\n",
    "\n",
    "print(f\"\\nRecovered Coefficients (After Registration):\")\n",
    "print(f\"  a (u_xx):    {result_registered['a']:.6f}  (true: -1.0)\")\n",
    "print(f\"  b (u_xxxx):  {result_registered['b']:.6f}  (true: -1.0)\")\n",
    "print(f\"  c ((u²)_x):  {result_registered['c']:.6f}  (true: -0.5)\")\n",
    "print(f\"\\nEvaluation Metrics:\")\n",
    "print(f\"  RMSE: {result_registered['rmse']:.6f}\")\n",
    "print(f\"  R²:   {result_registered['r2']:.6f}\")\n",
    "\n",
    "# Load Clean Data for Comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING CLEAN DATA FOR COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with h5py.File(\"test_solving_euler_ks_clean.h5\", \"r\") as f:\n",
    "    u_clean = np.array(f[\"u\"], dtype=np.float32)\n",
    "\n",
    "result_clean = evaluate_sindy_finite_diff(u_clean, x, t, method_name=\"Clean Data\")\n",
    "\n",
    "print(f\"\\nRecovered Coefficients (Clean Data):\")\n",
    "print(f\"  a (u_xx):    {result_clean['a']:.6f}  (true: -1.0)\")\n",
    "print(f\"  b (u_xxxx):  {result_clean['b']:.6f}  (true: -1.0)\")\n",
    "print(f\"  c ((u²)_x):  {result_clean['c']:.6f}  (true: -0.5)\")\n",
    "print(f\"\\nEvaluation Metrics:\")\n",
    "print(f\"  RMSE: {result_clean['rmse']:.6f}\")\n",
    "print(f\"  R²:   {result_clean['r2']:.6f}\")\n",
    "\n",
    "# Summary Comparison\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'Method':<25} {'a (u_xx)':>12} {'b (u_xxxx)':>12} {'c ((u²)_x)':>12} {'RMSE':>12} {'R²':>12}\")\n",
    "print(\"-\"*80)\n",
    "results = [result_clean, result_baseline, result_registered]\n",
    "for r in results:\n",
    "    print(f\"{r['method']:<25} {r['a']:>12.6f} {r['b']:>12.6f} {r['c']:>12.6f} \"\n",
    "          f\"{r['rmse']:>12.6f} {r['r2']:>12.6f}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'TRUE VALUES:':<25} {-1.0:>12.6f} {-1.0:>12.6f} {-0.5:>12.6f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Improvement Analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IMPROVEMENT FROM REGISTRATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "improvement_rmse = (result_baseline['rmse'] - result_registered['rmse']) / result_baseline['rmse'] * 100\n",
    "improvement_r2 = (result_registered['r2'] - result_baseline['r2']) / max(abs(result_baseline['r2']), 1e-10) * 100\n",
    "\n",
    "print(f\"\\nRMSE Improvement:  {improvement_rmse:+.2f}%\")\n",
    "print(f\"R² Improvement:    {improvement_r2:+.2f}%\")\n",
    "print(f\"\\nCoefficient Error Reduction:\")\n",
    "print(f\"  a: {result_baseline['abs_error_a']:.6f} → {result_registered['abs_error_a']:.6f}\")\n",
    "print(f\"  b: {result_baseline['abs_error_b']:.6f} → {result_registered['abs_error_b']:.6f}\")\n",
    "print(f\"  c: {result_baseline['abs_error_c']:.6f} → {result_registered['abs_error_c']:.6f}\")\n",
    "\n",
    "# Save Results\n",
    "results_dict = {\n",
    "    'method': ['Clean Data', 'No Registration', 'Cross-Correlation'],\n",
    "    'a': [result_clean['a'], result_baseline['a'], result_registered['a']],\n",
    "    'b': [result_clean['b'], result_baseline['b'], result_registered['b']],\n",
    "    'c': [result_clean['c'], result_baseline['c'], result_registered['c']],\n",
    "    'rmse': [result_clean['rmse'], result_baseline['rmse'], result_registered['rmse']],\n",
    "    'r2': [result_clean['r2'], result_baseline['r2'], result_registered['r2']],\n",
    "    'abs_error_a': [result_clean['abs_error_a'], result_baseline['abs_error_a'], result_registered['abs_error_a']],\n",
    "    'abs_error_b': [result_clean['abs_error_b'], result_baseline['abs_error_b'], result_registered['abs_error_b']],\n",
    "    'abs_error_c': [result_clean['abs_error_c'], result_baseline['abs_error_c'], result_registered['abs_error_c']],\n",
    "    'mean_shift': [0.0, np.nan, reg_metrics['mean_absolute_shift']],\n",
    "    'std_shift': [0.0, np.nan, reg_metrics['std_shift']],\n",
    "    'max_shift': [0.0, np.nan, reg_metrics['max_shift']],\n",
    "    'mean_correlation': [1.0, np.nan, reg_metrics['mean_correlation']]\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(results_dict)\n",
    "df_results.to_csv('registration_crosscorr_fd_results.csv', index=False)\n",
    "print(\"\\n    Saved: registration_crosscorr_fd_results.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"  CROSS-CORRELATION REGISTRATION + FINITE DIFFERENCE SINDY COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec357b23",
   "metadata": {},
   "source": [
    "Phase correlation registration + FFT derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0705d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from scipy.fft import fft, ifft, fftfreq, fft2, ifft2\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 3A: SPATIAL TRANSLATION - PHASE CORRELATION REGISTRATION + FFT SINDY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ===== Spectral derivative functions =====\n",
    "def spectral_derivative(u_snapshot, k):\n",
    "    \"\"\"First derivative using FFT\"\"\"\n",
    "    u_hat = fft(u_snapshot)\n",
    "    du_hat = 1j * k * u_hat\n",
    "    return np.real(ifft(du_hat))\n",
    "\n",
    "def spectral_second_derivative(u_snapshot, k):\n",
    "    \"\"\"Second derivative using FFT\"\"\"\n",
    "    u_hat = fft(u_snapshot)\n",
    "    d2u_hat = -(k**2) * u_hat\n",
    "    return np.real(ifft(d2u_hat))\n",
    "\n",
    "def spectral_fourth_derivative(u_snapshot, k):\n",
    "    \"\"\"Fourth derivative using FFT\"\"\"\n",
    "    u_hat = fft(u_snapshot)\n",
    "    d4u_hat = (k**4) * u_hat\n",
    "    return np.real(ifft(d4u_hat))\n",
    "\n",
    "# ===== Phase Correlation Registration =====\n",
    "def find_shift_phasecorr(frame, reference):\n",
    "    \"\"\"\n",
    "    Find the shift between frame and reference using phase correlation\n",
    "    More robust to noise than standard cross-correlation\n",
    "    \"\"\"\n",
    "    # FFT of both signals\n",
    "    F1 = fft(frame)\n",
    "    F2 = fft(reference)\n",
    "    \n",
    "    # Cross-power spectrum\n",
    "    cross_power = F1 * np.conj(F2)\n",
    "    # Normalize to get phase correlation\n",
    "    cross_power = cross_power / (np.abs(cross_power) + 1e-10)\n",
    "    \n",
    "    # Inverse FFT to get correlation\n",
    "    correlation = np.real(ifft(cross_power))\n",
    "    \n",
    "    # Find peak\n",
    "    max_idx = np.argmax(correlation)\n",
    "    \n",
    "    # Convert to shift (handle wrap-around)\n",
    "    N = len(frame)\n",
    "    if max_idx > N // 2:\n",
    "        shift = max_idx - N\n",
    "    else:\n",
    "        shift = max_idx\n",
    "    \n",
    "    return shift\n",
    "\n",
    "def align_data_phasecorr(u_data, reference_frame):\n",
    "    \"\"\"Align all frames to reference using phase correlation\"\"\"\n",
    "    aligned_data = np.zeros_like(u_data)\n",
    "    detected_shifts = []\n",
    "    \n",
    "    for i in range(u_data.shape[0]):\n",
    "        shift = find_shift_phasecorr(u_data[i], reference_frame)\n",
    "        aligned_data[i] = np.roll(u_data[i], -shift)\n",
    "        detected_shifts.append(shift)\n",
    "    \n",
    "    return aligned_data, np.array(detected_shifts)\n",
    "\n",
    "# ===== Registration Quality Metrics =====\n",
    "def compute_registration_metrics(detected_shifts, aligned_data, reference_frame):\n",
    "    \"\"\"Compute metrics to evaluate registration quality\"\"\"\n",
    "    mean_shift = np.mean(np.abs(detected_shifts))\n",
    "    std_shift = np.std(detected_shifts)\n",
    "    max_shift = np.max(np.abs(detected_shifts))\n",
    "    \n",
    "    correlations = []\n",
    "    for i in range(aligned_data.shape[0]):\n",
    "        corr = np.corrcoef(aligned_data[i], reference_frame)[0, 1]\n",
    "        correlations.append(corr)\n",
    "    \n",
    "    mean_correlation = np.mean(correlations)\n",
    "    residuals = aligned_data - reference_frame\n",
    "    residual_variance = np.var(residuals)\n",
    "    \n",
    "    return {\n",
    "        'mean_absolute_shift': mean_shift,\n",
    "        'std_shift': std_shift,\n",
    "        'max_shift': max_shift,\n",
    "        'mean_correlation': mean_correlation,\n",
    "        'residual_variance': residual_variance,\n",
    "        'detected_shifts': detected_shifts\n",
    "    }\n",
    "\n",
    "# ===== Evaluation Function =====\n",
    "def evaluate_sindy_fft(u, x, t, method_name=\"Unknown\"):\n",
    "    \"\"\"Apply FFT-based SINDy and compute evaluation metrics\"\"\"\n",
    "    Nx = len(x)\n",
    "    dx = x[1] - x[0]\n",
    "    dt = t[1] - t[0]\n",
    "    \n",
    "    # Define wavenumbers\n",
    "    k = 2 * np.pi * fftfreq(Nx, d=dx)\n",
    "    \n",
    "    # Compute temporal derivative (central difference)\n",
    "    u_t = (u[2:] - u[:-2]) / (2 * dt)\n",
    "    u_mid = u[1:-1]\n",
    "    \n",
    "    # Build feature library using FFT\n",
    "    Theta = []\n",
    "    ut_flat = []\n",
    "    \n",
    "    for snapshot, ut_snapshot in zip(u_mid, u_t):\n",
    "        ux = spectral_derivative(snapshot, k)\n",
    "        uxx = spectral_second_derivative(snapshot, k)\n",
    "        uxxxx = spectral_fourth_derivative(snapshot, k)\n",
    "        \n",
    "        # Feature terms: [u_xx, u_xxxx, u*u_x]\n",
    "        Theta_snapshot = np.vstack([\n",
    "            uxx,\n",
    "            uxxxx,\n",
    "            snapshot * ux\n",
    "        ]).T\n",
    "        \n",
    "        Theta.append(Theta_snapshot)\n",
    "        ut_flat.append(ut_snapshot)\n",
    "    \n",
    "    Theta = np.vstack(Theta)\n",
    "    ut_flat = np.hstack(ut_flat)\n",
    "    \n",
    "    # Normalize features\n",
    "    Theta_mean = Theta.mean(axis=0)\n",
    "    Theta_std = Theta.std(axis=0)\n",
    "    Theta_norm = (Theta - Theta_mean) / Theta_std\n",
    "    \n",
    "    # Lasso regression\n",
    "    lasso = Lasso(alpha=1e-4, fit_intercept=False, max_iter=50000)\n",
    "    lasso.fit(Theta_norm, ut_flat)\n",
    "    coeffs = lasso.coef_ / Theta_std\n",
    "    \n",
    "    # Extract coefficients\n",
    "    a = coeffs[0]\n",
    "    b = coeffs[1]\n",
    "    c = coeffs[2]\n",
    "    \n",
    "    # Predict u_t\n",
    "    u_t_pred = Theta @ coeffs\n",
    "    \n",
    "    # Compute metrics\n",
    "    rmse = np.sqrt(np.mean((ut_flat - u_t_pred)**2))\n",
    "    r2 = r2_score(ut_flat, u_t_pred)\n",
    "    \n",
    "    # True coefficients\n",
    "    a_true, b_true, c_true = -1.0, -1.0, -1.0\n",
    "    \n",
    "    # Coefficient errors\n",
    "    error_a = a - a_true\n",
    "    error_b = b - b_true\n",
    "    error_c = c - c_true\n",
    "    \n",
    "    abs_error_a = abs(error_a)\n",
    "    abs_error_b = abs(error_b)\n",
    "    abs_error_c = abs(error_c)\n",
    "    \n",
    "    return {\n",
    "        'method': method_name,\n",
    "        'a': a,\n",
    "        'b': b,\n",
    "        'c': c,\n",
    "        'error_a': error_a,\n",
    "        'error_b': error_b,\n",
    "        'error_c': error_c,\n",
    "        'abs_error_a': abs_error_a,\n",
    "        'abs_error_b': abs_error_b,\n",
    "        'abs_error_c': abs_error_c,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2\n",
    "    }\n",
    "\n",
    "# ===== Main Processing =====\n",
    "print(\"\\n📂 Loading spatial translation data...\")\n",
    "with h5py.File(\"test_solving_euler_ks_spatial_shift.h5\", \"r\") as f:\n",
    "    u_shifted = np.array(f[\"u\"])\n",
    "    x = np.array(f[\"x\"])\n",
    "    t = np.array(f[\"t\"])\n",
    "\n",
    "print(f\"Loaded: test_solving_euler_ks_spatial_shift.h5\")\n",
    "print(f\"Shape: {u_shifted.shape}\")\n",
    "\n",
    "# Compute Mean Frame as Reference\n",
    "print(\"\\n🎯 Computing mean frame as reference...\")\n",
    "reference_frame = np.mean(u_shifted, axis=0)\n",
    "\n",
    "# Method 1: No Registration (Baseline)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"METHOD 1: NO REGISTRATION (BASELINE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "result_baseline = evaluate_sindy_fft(u_shifted, x, t, method_name=\"No Registration\")\n",
    "\n",
    "print(f\"\\nRecovered Coefficients:\")\n",
    "print(f\"  a (u_xx):   {result_baseline['a']:.6f}  (true: -1.0)\")\n",
    "print(f\"  b (u_xxxx): {result_baseline['b']:.6f}  (true: -1.0)\")\n",
    "print(f\"  c (u*u_x):  {result_baseline['c']:.6f}  (true: -1.0)\")\n",
    "print(f\"\\nEvaluation Metrics:\")\n",
    "print(f\"  RMSE: {result_baseline['rmse']:.6f}\")\n",
    "print(f\"  R²:   {result_baseline['r2']:.6f}\")\n",
    "\n",
    "# Method 2: Phase Correlation Registration\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"METHOD 2: PHASE CORRELATION REGISTRATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"Aligning frames using phase correlation...\")\n",
    "u_aligned, detected_shifts = align_data_phasecorr(u_shifted, reference_frame)\n",
    "\n",
    "print(f\"    Alignment complete!\")\n",
    "print(f\"  Detected shifts: min={detected_shifts.min()}, max={detected_shifts.max()}\")\n",
    "print(f\"  Mean absolute shift: {np.mean(np.abs(detected_shifts)):.2f} pixels\")\n",
    "\n",
    "# Compute registration quality metrics\n",
    "reg_metrics = compute_registration_metrics(detected_shifts, u_aligned, reference_frame)\n",
    "\n",
    "print(f\"\\nRegistration Quality Metrics:\")\n",
    "print(f\"  Mean Absolute Shift: {reg_metrics['mean_absolute_shift']:.4f} pixels\")\n",
    "print(f\"  Std Shift:           {reg_metrics['std_shift']:.4f} pixels\")\n",
    "print(f\"  Max Shift:           {reg_metrics['max_shift']:.1f} pixels\")\n",
    "print(f\"  Mean Correlation:    {reg_metrics['mean_correlation']:.6f}\")\n",
    "print(f\"  Residual Variance:   {reg_metrics['residual_variance']:.6f}\")\n",
    "\n",
    "# Apply SINDy to aligned data\n",
    "result_registered = evaluate_sindy_fft(u_aligned, x, t, method_name=\"Phase Correlation\")\n",
    "\n",
    "print(f\"\\nRecovered Coefficients (After Registration):\")\n",
    "print(f\"  a (u_xx):   {result_registered['a']:.6f}  (true: -1.0)\")\n",
    "print(f\"  b (u_xxxx): {result_registered['b']:.6f}  (true: -1.0)\")\n",
    "print(f\"  c (u*u_x):  {result_registered['c']:.6f}  (true: -1.0)\")\n",
    "print(f\"\\nEvaluation Metrics:\")\n",
    "print(f\"  RMSE: {result_registered['rmse']:.6f}\")\n",
    "print(f\"  R²:   {result_registered['r2']:.6f}\")\n",
    "\n",
    "# Load Clean Data for Comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING CLEAN DATA FOR COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with h5py.File(\"test_solving_euler_ks_clean.h5\", \"r\") as f:\n",
    "    u_clean = np.array(f[\"u\"])\n",
    "\n",
    "result_clean = evaluate_sindy_fft(u_clean, x, t, method_name=\"Clean Data\")\n",
    "\n",
    "print(f\"\\nRecovered Coefficients (Clean Data):\")\n",
    "print(f\"  a (u_xx):   {result_clean['a']:.6f}  (true: -1.0)\")\n",
    "print(f\"  b (u_xxxx): {result_clean['b']:.6f}  (true: -1.0)\")\n",
    "print(f\"  c (u*u_x):  {result_clean['c']:.6f}  (true: -1.0)\")\n",
    "print(f\"\\nEvaluation Metrics:\")\n",
    "print(f\"  RMSE: {result_clean['rmse']:.6f}\")\n",
    "print(f\"  R²:   {result_clean['r2']:.6f}\")\n",
    "\n",
    "# Summary Comparison\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'Method':<25} {'a (u_xx)':>12} {'b (u_xxxx)':>12} {'c (u*u_x)':>12} {'RMSE':>12} {'R²':>12}\")\n",
    "print(\"-\"*80)\n",
    "results = [result_clean, result_baseline, result_registered]\n",
    "for r in results:\n",
    "    print(f\"{r['method']:<25} {r['a']:>12.6f} {r['b']:>12.6f} {r['c']:>12.6f} \"\n",
    "          f\"{r['rmse']:>12.6f} {r['r2']:>12.6f}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'TRUE VALUES:':<25} {-1.0:>12.6f} {-1.0:>12.6f} {-1.0:>12.6f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Improvement Analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IMPROVEMENT FROM REGISTRATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "improvement_rmse = (result_baseline['rmse'] - result_registered['rmse']) / result_baseline['rmse'] * 100\n",
    "improvement_r2 = (result_registered['r2'] - result_baseline['r2']) / max(abs(result_baseline['r2']), 1e-10) * 100\n",
    "\n",
    "print(f\"\\nRMSE Improvement:  {improvement_rmse:+.2f}%\")\n",
    "print(f\"R² Improvement:    {improvement_r2:+.2f}%\")\n",
    "\n",
    "# Save Results\n",
    "results_dict = {\n",
    "    'method': ['Clean Data', 'No Registration', 'Phase Correlation'],\n",
    "    'a': [result_clean['a'], result_baseline['a'], result_registered['a']],\n",
    "    'b': [result_clean['b'], result_baseline['b'], result_registered['b']],\n",
    "    'c': [result_clean['c'], result_baseline['c'], result_registered['c']],\n",
    "    'rmse': [result_clean['rmse'], result_baseline['rmse'], result_registered['rmse']],\n",
    "    'r2': [result_clean['r2'], result_baseline['r2'], result_registered['r2']],\n",
    "    'abs_error_a': [result_clean['abs_error_a'], result_baseline['abs_error_a'], result_registered['abs_error_a']],\n",
    "    'abs_error_b': [result_clean['abs_error_b'], result_baseline['abs_error_b'], result_registered['abs_error_b']],\n",
    "    'abs_error_c': [result_clean['abs_error_c'], result_baseline['abs_error_c'], result_registered['abs_error_c']],\n",
    "    'mean_shift': [0.0, np.nan, reg_metrics['mean_absolute_shift']],\n",
    "    'std_shift': [0.0, np.nan, reg_metrics['std_shift']],\n",
    "    'max_shift': [0.0, np.nan, reg_metrics['max_shift']],\n",
    "    'mean_correlation': [1.0, np.nan, reg_metrics['mean_correlation']]\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(results_dict)\n",
    "df_results.to_csv('registration_phasecorr_fft_results.csv', index=False)\n",
    "print(\"\\n    Saved: registration_phasecorr_fft_results.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"   PHASE CORRELATION REGISTRATION + FFT SINDY COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf75ec3d",
   "metadata": {},
   "source": [
    "Phase correlation registration + Finite Difference derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8237c8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from scipy.fft import fft, ifft\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 3A: SPATIAL TRANSLATION - PHASE CORRELATION REGISTRATION + FINITE DIFFERENCE SINDY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ===== Phase Correlation Registration =====\n",
    "def find_shift_phasecorr(frame, reference):\n",
    "    \"\"\"Find the shift between frame and reference using phase correlation\"\"\"\n",
    "    F1 = fft(frame)\n",
    "    F2 = fft(reference)\n",
    "    cross_power = F1 * np.conj(F2)\n",
    "    cross_power = cross_power / (np.abs(cross_power) + 1e-10)\n",
    "    correlation = np.real(ifft(cross_power))\n",
    "    max_idx = np.argmax(correlation)\n",
    "    N = len(frame)\n",
    "    if max_idx > N // 2:\n",
    "        shift = max_idx - N\n",
    "    else:\n",
    "        shift = max_idx\n",
    "    return shift\n",
    "\n",
    "def align_data_phasecorr(u_data, reference_frame):\n",
    "    \"\"\"Align all frames to reference using phase correlation\"\"\"\n",
    "    aligned_data = np.zeros_like(u_data)\n",
    "    detected_shifts = []\n",
    "    for i in range(u_data.shape[0]):\n",
    "        shift = find_shift_phasecorr(u_data[i], reference_frame)\n",
    "        aligned_data[i] = np.roll(u_data[i], -shift)\n",
    "        detected_shifts.append(shift)\n",
    "    return aligned_data, np.array(detected_shifts)\n",
    "\n",
    "# ===== Registration Quality Metrics =====\n",
    "def compute_registration_metrics(detected_shifts, aligned_data, reference_frame):\n",
    "    \"\"\"Compute metrics to evaluate registration quality\"\"\"\n",
    "    mean_shift = np.mean(np.abs(detected_shifts))\n",
    "    std_shift = np.std(detected_shifts)\n",
    "    max_shift = np.max(np.abs(detected_shifts))\n",
    "    correlations = []\n",
    "    for i in range(aligned_data.shape[0]):\n",
    "        corr = np.corrcoef(aligned_data[i], reference_frame)[0, 1]\n",
    "        correlations.append(corr)\n",
    "    mean_correlation = np.mean(correlations)\n",
    "    residuals = aligned_data - reference_frame\n",
    "    residual_variance = np.var(residuals)\n",
    "    return {\n",
    "        'mean_absolute_shift': mean_shift,\n",
    "        'std_shift': std_shift,\n",
    "        'max_shift': max_shift,\n",
    "        'mean_correlation': mean_correlation,\n",
    "        'residual_variance': residual_variance,\n",
    "        'detected_shifts': detected_shifts\n",
    "    }\n",
    "\n",
    "# ===== Evaluation Function with Finite Difference =====\n",
    "def evaluate_sindy_finite_diff(u, x, t, method_name=\"Unknown\"):\n",
    "    \"\"\"Apply Finite Difference SINDy and compute evaluation metrics\"\"\"\n",
    "    dx = float(x[1] - x[0])\n",
    "    dt = float(t[1] - t[0])\n",
    "    \n",
    "    def d1(U): \n",
    "        return (np.roll(U, -1, 1) - np.roll(U, 1, 1)) / (2*dx)\n",
    "    def d2(U): \n",
    "        return (np.roll(U, -1, 1) - 2*U + np.roll(U, 1, 1)) / (dx**2)\n",
    "    def d4(U):\n",
    "        return (np.roll(U, -2, 1) - 4*np.roll(U, -1, 1) + 6*U - \n",
    "                4*np.roll(U, 1, 1) + np.roll(U, 2, 1)) / (dx**4)\n",
    "    \n",
    "    U_t = (u[1:] - u[:-1]) / dt\n",
    "    U = u[:-1]\n",
    "    U_x = d1(U)\n",
    "    U_xx = d2(U)\n",
    "    U_xxxx = d4(U)\n",
    "    U2_x = d1(U**2)\n",
    "    \n",
    "    Theta = np.column_stack([\n",
    "        np.ones_like(U).ravel(),\n",
    "        U.ravel(),\n",
    "        U_x.ravel(),\n",
    "        U_xx.ravel(),\n",
    "        U_xxxx.ravel(),\n",
    "        U2_x.ravel()\n",
    "    ])\n",
    "    y = U_t.ravel()\n",
    "    names = [\"1\", \"u\", \"u_x\", \"u_xx\", \"u_xxxx\", \"(u^2)_x\"]\n",
    "    \n",
    "    Theta_mean = Theta.mean(0)\n",
    "    Theta_std = Theta.std(0) + 1e-8\n",
    "    Theta_n = (Theta - Theta_mean) / Theta_std\n",
    "    \n",
    "    lasso = Lasso(alpha=1e-4, fit_intercept=False, max_iter=50000, tol=1e-6)\n",
    "    lasso.fit(Theta_n, y)\n",
    "    coef = lasso.coef_ / Theta_std\n",
    "    \n",
    "    a = coef[names.index(\"u_xx\")]\n",
    "    b = coef[names.index(\"u_xxxx\")]\n",
    "    c_phys = coef[names.index(\"(u^2)_x\")]\n",
    "    \n",
    "    y_pred = Theta @ coef\n",
    "    rmse = np.sqrt(np.mean((y - y_pred)**2))\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    \n",
    "    a_true, b_true, c_true = -1.0, -1.0, -0.5\n",
    "    \n",
    "    return {\n",
    "        'method': method_name,\n",
    "        'a': a,\n",
    "        'b': b,\n",
    "        'c': c_phys,\n",
    "        'error_a': a - a_true,\n",
    "        'error_b': b - b_true,\n",
    "        'error_c': c_phys - c_true,\n",
    "        'abs_error_a': abs(a - a_true),\n",
    "        'abs_error_b': abs(b - b_true),\n",
    "        'abs_error_c': abs(c_phys - c_true),\n",
    "        'rmse': rmse,\n",
    "        'r2': r2\n",
    "    }\n",
    "\n",
    "# ===== Main Processing =====\n",
    "print(\"\\n📂 Loading spatial translation data...\")\n",
    "with h5py.File(\"test_solving_euler_ks_spatial_shift.h5\", \"r\") as f:\n",
    "    u_shifted = np.array(f[\"u\"], dtype=np.float32)\n",
    "    x = np.array(f[\"x\"], dtype=np.float32)\n",
    "    t = np.array(f[\"t\"], dtype=np.float32)\n",
    "\n",
    "print(f\"Loaded: test_solving_euler_ks_spatial_shift.h5\")\n",
    "print(f\"Shape: {u_shifted.shape}\")\n",
    "\n",
    "print(\"\\n🎯 Computing mean frame as reference...\")\n",
    "reference_frame = np.mean(u_shifted, axis=0)\n",
    "\n",
    "# Method 1: No Registration\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"METHOD 1: NO REGISTRATION (BASELINE)\")\n",
    "print(\"=\"*80)\n",
    "result_baseline = evaluate_sindy_finite_diff(u_shifted, x, t, method_name=\"No Registration\")\n",
    "print(f\"\\nRecovered: a={result_baseline['a']:.6f}, b={result_baseline['b']:.6f}, c={result_baseline['c']:.6f}\")\n",
    "print(f\"RMSE={result_baseline['rmse']:.6f}, R²={result_baseline['r2']:.6f}\")\n",
    "\n",
    "# Method 2: Phase Correlation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"METHOD 2: PHASE CORRELATION REGISTRATION\")\n",
    "print(\"=\"*80)\n",
    "u_aligned, detected_shifts = align_data_phasecorr(u_shifted, reference_frame)\n",
    "print(f\"    Alignment complete! Mean shift: {np.mean(np.abs(detected_shifts)):.2f} pixels\")\n",
    "\n",
    "reg_metrics = compute_registration_metrics(detected_shifts, u_aligned, reference_frame)\n",
    "print(f\"Registration metrics: mean_shift={reg_metrics['mean_absolute_shift']:.4f}, \"\n",
    "      f\"correlation={reg_metrics['mean_correlation']:.6f}\")\n",
    "\n",
    "result_registered = evaluate_sindy_finite_diff(u_aligned, x, t, method_name=\"Phase Correlation\")\n",
    "print(f\"\\nRecovered: a={result_registered['a']:.6f}, b={result_registered['b']:.6f}, c={result_registered['c']:.6f}\")\n",
    "print(f\"RMSE={result_registered['rmse']:.6f}, R²={result_registered['r2']:.6f}\")\n",
    "\n",
    "# Clean Data\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLEAN DATA COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "with h5py.File(\"test_solving_euler_ks_clean.h5\", \"r\") as f:\n",
    "    u_clean = np.array(f[\"u\"], dtype=np.float32)\n",
    "result_clean = evaluate_sindy_finite_diff(u_clean, x, t, method_name=\"Clean Data\")\n",
    "print(f\"Recovered: a={result_clean['a']:.6f}, b={result_clean['b']:.6f}, c={result_clean['c']:.6f}\")\n",
    "print(f\"RMSE={result_clean['rmse']:.6f}, R²={result_clean['r2']:.6f}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Method':<25} {'a':>12} {'b':>12} {'c':>12} {'RMSE':>12} {'R²':>12}\")\n",
    "print(\"-\"*80)\n",
    "for r in [result_clean, result_baseline, result_registered]:\n",
    "    print(f\"{r['method']:<25} {r['a']:>12.6f} {r['b']:>12.6f} {r['c']:>12.6f} {r['rmse']:>12.6f} {r['r2']:>12.6f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save\n",
    "results_dict = {\n",
    "    'method': ['Clean Data', 'No Registration', 'Phase Correlation'],\n",
    "    'a': [result_clean['a'], result_baseline['a'], result_registered['a']],\n",
    "    'b': [result_clean['b'], result_baseline['b'], result_registered['b']],\n",
    "    'c': [result_clean['c'], result_baseline['c'], result_registered['c']],\n",
    "    'rmse': [result_clean['rmse'], result_baseline['rmse'], result_registered['rmse']],\n",
    "    'r2': [result_clean['r2'], result_baseline['r2'], result_registered['r2']],\n",
    "    'abs_error_a': [result_clean['abs_error_a'], result_baseline['abs_error_a'], result_registered['abs_error_a']],\n",
    "    'abs_error_b': [result_clean['abs_error_b'], result_baseline['abs_error_b'], result_registered['abs_error_b']],\n",
    "    'abs_error_c': [result_clean['abs_error_c'], result_baseline['abs_error_c'], result_registered['abs_error_c']],\n",
    "    'mean_shift': [0.0, np.nan, reg_metrics['mean_absolute_shift']],\n",
    "    'std_shift': [0.0, np.nan, reg_metrics['std_shift']],\n",
    "    'max_shift': [0.0, np.nan, reg_metrics['max_shift']],\n",
    "    'mean_correlation': [1.0, np.nan, reg_metrics['mean_correlation']]\n",
    "}\n",
    "pd.DataFrame(results_dict).to_csv('registration_phasecorr_fd_results.csv', index=False)\n",
    "print(\"\\n    Saved: registration_phasecorr_fd_results.csv\")\n",
    "print(\"\\n   PHASE CORRELATION REGISTRATION + FINITE DIFFERENCE SINDY COMPLETE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a70208",
   "metadata": {},
   "source": [
    "Registration Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ba9c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.signal import correlate\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (18, 14)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 3A: REGISTRATION METHODS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ===== Load All Results =====\n",
    "print(\"\\n📂 Loading all registration results...\")\n",
    "\n",
    "try:\n",
    "    # Cross-Correlation Results\n",
    "    crosscorr_fft = pd.read_csv('registration_crosscorr_fft_results.csv')\n",
    "    crosscorr_fd = pd.read_csv('registration_crosscorr_fd_results.csv')\n",
    "    \n",
    "    # Phase Correlation Results\n",
    "    phasecorr_fft = pd.read_csv('registration_phasecorr_fft_results.csv')\n",
    "    phasecorr_fd = pd.read_csv('registration_phasecorr_fd_results.csv')\n",
    "    \n",
    "    print(\"    Successfully loaded all 4 result files\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"    Error: {e}\")\n",
    "    print(\"Please run all 4 registration scripts first.\")\n",
    "    exit()\n",
    "\n",
    "# ===== Load Data for Visual Comparison =====\n",
    "print(\"\\n📂 Loading spatial data for visual comparison...\")\n",
    "\n",
    "with h5py.File(\"test_solving_euler_ks_clean.h5\", \"r\") as f:\n",
    "    u_clean = np.array(f[\"u\"])\n",
    "    x = np.array(f[\"x\"])\n",
    "    t = np.array(f[\"t\"])\n",
    "\n",
    "with h5py.File(\"test_solving_euler_ks_spatial_shift.h5\", \"r\") as f:\n",
    "    u_shifted = np.array(f[\"u\"])\n",
    "\n",
    "# Reconstruct aligned data (using cross-correlation)\n",
    "def find_shift_crosscorr(frame, reference):\n",
    "    \"\"\"Find shift using cross-correlation\"\"\"\n",
    "    correlation = correlate(frame, reference, mode='same', method='fft')\n",
    "    max_idx = np.argmax(correlation)\n",
    "    center = len(frame) // 2\n",
    "    shift = max_idx - center\n",
    "    return shift\n",
    "\n",
    "print(\"Reconstructing aligned data...\")\n",
    "reference_frame = np.mean(u_shifted, axis=0)\n",
    "u_aligned = np.zeros_like(u_shifted)\n",
    "for i in range(u_shifted.shape[0]):\n",
    "    shift = find_shift_crosscorr(u_shifted[i], reference_frame)\n",
    "    u_aligned[i] = np.roll(u_shifted[i], -shift)\n",
    "\n",
    "print(\"    Data loaded successfully\")\n",
    "\n",
    "# ===== COMPARISON TABLE 1: All Methods Overview =====\n",
    "print(\"\\n\\n\" + \"=\"*100)\n",
    "print(\"COMPARISON TABLE 1: ALL REGISTRATION METHODS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "# FFT Methods\n",
    "for idx, row in crosscorr_fft.iterrows():\n",
    "    if row['method'] != 'Clean Data':\n",
    "        comparison_data.append({\n",
    "            'Derivative': 'FFT',\n",
    "            'Registration': row['method'],\n",
    "            'a': row['a'],\n",
    "            'b': row['b'],\n",
    "            'c': row['c'],\n",
    "            'rmse': row['rmse'],\n",
    "            'r2': row['r2'],\n",
    "            'abs_error_a': row['abs_error_a'],\n",
    "            'abs_error_b': row['abs_error_b'],\n",
    "            'abs_error_c': row['abs_error_c'],\n",
    "            'mean_shift': row['mean_shift']\n",
    "        })\n",
    "\n",
    "for idx, row in phasecorr_fft.iterrows():\n",
    "    if row['method'] not in ['Clean Data', 'No Registration']:\n",
    "        comparison_data.append({\n",
    "            'Derivative': 'FFT',\n",
    "            'Registration': row['method'],\n",
    "            'a': row['a'],\n",
    "            'b': row['b'],\n",
    "            'c': row['c'],\n",
    "            'rmse': row['rmse'],\n",
    "            'r2': row['r2'],\n",
    "            'abs_error_a': row['abs_error_a'],\n",
    "            'abs_error_b': row['abs_error_b'],\n",
    "            'abs_error_c': row['abs_error_c'],\n",
    "            'mean_shift': row['mean_shift']\n",
    "        })\n",
    "\n",
    "# Finite Difference Methods\n",
    "for idx, row in crosscorr_fd.iterrows():\n",
    "    if row['method'] != 'Clean Data':\n",
    "        comparison_data.append({\n",
    "            'Derivative': 'Finite Diff',\n",
    "            'Registration': row['method'],\n",
    "            'a': row['a'],\n",
    "            'b': row['b'],\n",
    "            'c': row['c'],\n",
    "            'rmse': row['rmse'],\n",
    "            'r2': row['r2'],\n",
    "            'abs_error_a': row['abs_error_a'],\n",
    "            'abs_error_b': row['abs_error_b'],\n",
    "            'abs_error_c': row['abs_error_c'],\n",
    "            'mean_shift': row['mean_shift']\n",
    "        })\n",
    "\n",
    "for idx, row in phasecorr_fd.iterrows():\n",
    "    if row['method'] not in ['Clean Data', 'No Registration']:\n",
    "        comparison_data.append({\n",
    "            'Derivative': 'Finite Diff',\n",
    "            'Registration': row['method'],\n",
    "            'a': row['a'],\n",
    "            'b': row['b'],\n",
    "            'c': row['c'],\n",
    "            'rmse': row['rmse'],\n",
    "            'r2': row['r2'],\n",
    "            'abs_error_a': row['abs_error_a'],\n",
    "            'abs_error_b': row['abs_error_b'],\n",
    "            'abs_error_c': row['abs_error_c'],\n",
    "            'mean_shift': row['mean_shift']\n",
    "        })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(f\"\\n{'Derivative':>12} {'Registration':>20} {'a':>12} {'b':>12} {'c':>12} {'RMSE':>12} {'R²':>10}\")\n",
    "print(\"-\"*100)\n",
    "for _, row in df_comparison.iterrows():\n",
    "    print(f\"{row['Derivative']:>12} {row['Registration']:>20} {row['a']:>12.6f} {row['b']:>12.6f} \"\n",
    "          f\"{row['c']:>12.6f} {row['rmse']:>12.6f} {row['r2']:>10.6f}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# ===== COMPARISON TABLE 2: Clean Data Baseline =====\n",
    "print(\"\\n\\n\" + \"=\"*100)\n",
    "print(\"COMPARISON TABLE 2: CLEAN DATA BASELINE\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "clean_fft = crosscorr_fft[crosscorr_fft['method'] == 'Clean Data'].iloc[0]\n",
    "clean_fd = crosscorr_fd[crosscorr_fd['method'] == 'Clean Data'].iloc[0]\n",
    "\n",
    "print(f\"\\n{'Derivative':>12} {'a':>12} {'b':>12} {'c':>12} {'RMSE':>12} {'R²':>10}\")\n",
    "print(\"-\"*100)\n",
    "print(f\"{'FFT':>12} {clean_fft['a']:>12.6f} {clean_fft['b']:>12.6f} {clean_fft['c']:>12.6f} \"\n",
    "      f\"{clean_fft['rmse']:>12.6f} {clean_fft['r2']:>10.6f}\")\n",
    "print(f\"{'Finite Diff':>12} {clean_fd['a']:>12.6f} {clean_fd['b']:>12.6f} {clean_fd['c']:>12.6f} \"\n",
    "      f\"{clean_fd['rmse']:>12.6f} {clean_fd['r2']:>10.6f}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# ===== BEST METHOD ANALYSIS =====\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"BEST METHODS BY METRIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Best by RMSE\n",
    "best_rmse = df_comparison.loc[df_comparison['rmse'].idxmin()]\n",
    "print(f\"\\n🏆 BEST RMSE: {best_rmse['Derivative']} + {best_rmse['Registration']}\")\n",
    "print(f\"   RMSE = {best_rmse['rmse']:.6f}\")\n",
    "print(f\"   R² = {best_rmse['r2']:.6f}\")\n",
    "\n",
    "# Best by R²\n",
    "best_r2 = df_comparison.loc[df_comparison['r2'].idxmax()]\n",
    "print(f\"\\n🏆 BEST R²: {best_r2['Derivative']} + {best_r2['Registration']}\")\n",
    "print(f\"   RMSE = {best_r2['rmse']:.6f}\")\n",
    "print(f\"   R² = {best_r2['r2']:.6f}\")\n",
    "\n",
    "# Best by coefficient accuracy (mean absolute error)\n",
    "df_comparison['mean_coef_error'] = (df_comparison['abs_error_a'] + \n",
    "                                     df_comparison['abs_error_b'] + \n",
    "                                     df_comparison['abs_error_c']) / 3\n",
    "best_coef = df_comparison.loc[df_comparison['mean_coef_error'].idxmin()]\n",
    "print(f\"\\n🏆 BEST COEFFICIENT ACCURACY: {best_coef['Derivative']} + {best_coef['Registration']}\")\n",
    "print(f\"   Mean Coef Error = {best_coef['mean_coef_error']:.6f}\")\n",
    "print(f\"   RMSE = {best_coef['rmse']:.6f}\")\n",
    "\n",
    "# ===== IMPROVEMENT ANALYSIS =====\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"IMPROVEMENT FROM REGISTRATION (vs No Registration Baseline)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "baseline_fft = crosscorr_fft[crosscorr_fft['method'] == 'No Registration'].iloc[0]\n",
    "baseline_fd = crosscorr_fd[crosscorr_fd['method'] == 'No Registration'].iloc[0]\n",
    "\n",
    "print(\"\\n--- FFT Methods ---\")\n",
    "for idx, row in df_comparison[df_comparison['Derivative'] == 'FFT'].iterrows():\n",
    "    if row['Registration'] != 'No Registration':\n",
    "        rmse_improv = (baseline_fft['rmse'] - row['rmse']) / baseline_fft['rmse'] * 100\n",
    "        r2_improv = (row['r2'] - baseline_fft['r2']) / max(abs(baseline_fft['r2']), 1e-10) * 100\n",
    "        print(f\"{row['Registration']:>20}: RMSE {rmse_improv:+.2f}%, R² {r2_improv:+.2f}%\")\n",
    "\n",
    "print(\"\\n--- Finite Difference Methods ---\")\n",
    "for idx, row in df_comparison[df_comparison['Derivative'] == 'Finite Diff'].iterrows():\n",
    "    if row['Registration'] != 'No Registration':\n",
    "        rmse_improv = (baseline_fd['rmse'] - row['rmse']) / baseline_fd['rmse'] * 100\n",
    "        r2_improv = (row['r2'] - baseline_fd['r2']) / max(abs(baseline_fd['r2']), 1e-10) * 100\n",
    "        print(f\"{row['Registration']:>20}: RMSE {rmse_improv:+.2f}%, R² {r2_improv:+.2f}%\")\n",
    "\n",
    "# ===== VISUALIZATION 1: VISUAL DATA COMPARISON =====\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"GENERATING VISUALIZATIONS...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig1 = plt.figure(figsize=(20, 12))\n",
    "\n",
    "# Sample frame index\n",
    "sample_idx = len(u_clean) // 2\n",
    "\n",
    "# Row 1: Spatiotemporal plots (first 500 frames)\n",
    "n_frames = 500\n",
    "\n",
    "ax1 = plt.subplot(3, 3, 1)\n",
    "im1 = plt.imshow(u_clean[:n_frames].T, cmap='RdBu', aspect='auto', origin='lower',\n",
    "                 extent=(0, n_frames*t[1], x[0], x[-1]), vmin=-2, vmax=2)\n",
    "plt.xlabel('Time', fontsize=10)\n",
    "plt.ylabel('Space (x)', fontsize=10)\n",
    "plt.title('Clean Data (Aligned)\\n(First 500 frames)', fontweight='bold', fontsize=11)\n",
    "plt.colorbar(im1, ax=ax1, label='u(x,t)')\n",
    "\n",
    "ax2 = plt.subplot(3, 3, 2)\n",
    "im2 = plt.imshow(u_shifted[:n_frames].T, cmap='RdBu', aspect='auto', origin='lower',\n",
    "                 extent=(0, n_frames*t[1], x[0], x[-1]), vmin=-2, vmax=2)\n",
    "plt.xlabel('Time', fontsize=10)\n",
    "plt.ylabel('Space (x)', fontsize=10)\n",
    "plt.title('No Registration (Unaligned)\\n(First 500 frames)', fontweight='bold', fontsize=11)\n",
    "plt.colorbar(im2, ax=ax2, label='u(x,t)')\n",
    "\n",
    "ax3 = plt.subplot(3, 3, 3)\n",
    "im3 = plt.imshow(u_aligned[:n_frames].T, cmap='RdBu', aspect='auto', origin='lower',\n",
    "                 extent=(0, n_frames*t[1], x[0], x[-1]), vmin=-2, vmax=2)\n",
    "plt.xlabel('Time', fontsize=10)\n",
    "plt.ylabel('Space (x)', fontsize=10)\n",
    "plt.title('With Registration (Aligned)\\n(First 500 frames)', fontweight='bold', fontsize=11)\n",
    "plt.colorbar(im3, ax=ax3, label='u(x,t)')\n",
    "\n",
    "# Row 2: Sample frame comparison\n",
    "ax4 = plt.subplot(3, 3, 4)\n",
    "plt.plot(x, u_clean[sample_idx], 'g-', label='Clean', linewidth=2.5, alpha=0.8)\n",
    "plt.plot(x, u_shifted[sample_idx], 'r--', label='Unaligned', linewidth=2, alpha=0.7)\n",
    "plt.plot(x, u_aligned[sample_idx], 'b:', label='Aligned', linewidth=2.5, alpha=0.8)\n",
    "plt.xlabel('x', fontsize=10)\n",
    "plt.ylabel('u(x)', fontsize=10)\n",
    "plt.title(f'Sample Frame Comparison\\n(Frame {sample_idx})', fontweight='bold', fontsize=11)\n",
    "plt.legend(fontsize=9)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Row 2: Zoomed region\n",
    "zoom_start, zoom_end = len(x)//3, len(x)//3 + 50\n",
    "ax5 = plt.subplot(3, 3, 5)\n",
    "plt.plot(x[zoom_start:zoom_end], u_clean[sample_idx, zoom_start:zoom_end], \n",
    "         'g-', label='Clean', linewidth=2.5, marker='o', markersize=4, alpha=0.8)\n",
    "plt.plot(x[zoom_start:zoom_end], u_shifted[sample_idx, zoom_start:zoom_end], \n",
    "         'r--', label='Unaligned', linewidth=2, marker='s', markersize=4, alpha=0.7)\n",
    "plt.plot(x[zoom_start:zoom_end], u_aligned[sample_idx, zoom_start:zoom_end], \n",
    "         'b:', label='Aligned', linewidth=2.5, marker='^', markersize=4, alpha=0.8)\n",
    "plt.xlabel('x', fontsize=10)\n",
    "plt.ylabel('u(x)', fontsize=10)\n",
    "plt.title('Zoomed Region\\n(Shows Spatial Shifts)', fontweight='bold', fontsize=11)\n",
    "plt.legend(fontsize=9)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Difference plots\n",
    "ax6 = plt.subplot(3, 3, 6)\n",
    "diff_unaligned = np.abs(u_shifted[sample_idx] - u_clean[sample_idx])\n",
    "diff_aligned = np.abs(u_aligned[sample_idx] - u_clean[sample_idx])\n",
    "plt.plot(x, diff_unaligned, 'r-', label='|Unaligned - Clean|', linewidth=2, alpha=0.7)\n",
    "plt.plot(x, diff_aligned, 'b-', label='|Aligned - Clean|', linewidth=2, alpha=0.7)\n",
    "plt.xlabel('x', fontsize=10)\n",
    "plt.ylabel('Absolute Difference', fontsize=10)\n",
    "plt.title('Spatial Error Comparison\\n(Lower is Better)', fontweight='bold', fontsize=11)\n",
    "plt.legend(fontsize=9)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Row 3: Coefficient comparison\n",
    "ax7 = plt.subplot(3, 3, 7)\n",
    "methods = ['No Reg\\nFFT', 'CrossCorr\\nFFT', 'PhaseCorr\\nFFT', \n",
    "           'No Reg\\nFD', 'CrossCorr\\nFD', 'PhaseCorr\\nFD']\n",
    "a_vals = [baseline_fft['a'], \n",
    "          crosscorr_fft[crosscorr_fft['method']=='Cross-Correlation'].iloc[0]['a'],\n",
    "          phasecorr_fft[phasecorr_fft['method']=='Phase Correlation'].iloc[0]['a'],\n",
    "          baseline_fd['a'],\n",
    "          crosscorr_fd[crosscorr_fd['method']=='Cross-Correlation'].iloc[0]['a'],\n",
    "          phasecorr_fd[phasecorr_fd['method']=='Phase Correlation'].iloc[0]['a']]\n",
    "\n",
    "b_vals = [baseline_fft['b'], \n",
    "          crosscorr_fft[crosscorr_fft['method']=='Cross-Correlation'].iloc[0]['b'],\n",
    "          phasecorr_fft[phasecorr_fft['method']=='Phase Correlation'].iloc[0]['b'],\n",
    "          baseline_fd['b'],\n",
    "          crosscorr_fd[crosscorr_fd['method']=='Cross-Correlation'].iloc[0]['b'],\n",
    "          phasecorr_fd[phasecorr_fd['method']=='Phase Correlation'].iloc[0]['b']]\n",
    "\n",
    "x_pos = np.arange(len(methods))\n",
    "width = 0.35\n",
    "plt.bar(x_pos - width/2, a_vals, width, label='a (u_xx)', alpha=0.8)\n",
    "plt.bar(x_pos + width/2, b_vals, width, label='b (u_xxxx)', alpha=0.8)\n",
    "plt.axhline(y=-1.0, color='red', linestyle='--', linewidth=2, label='True')\n",
    "plt.ylabel('Coefficient Value', fontsize=10)\n",
    "plt.title('Recovered Coefficients\\nAll Methods', fontweight='bold', fontsize=11)\n",
    "plt.xticks(x_pos, methods, rotation=45, ha='right', fontsize=8)\n",
    "plt.legend(fontsize=9)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# RMSE comparison\n",
    "ax8 = plt.subplot(3, 3, 8)\n",
    "rmse_vals = [baseline_fft['rmse'], \n",
    "             crosscorr_fft[crosscorr_fft['method']=='Cross-Correlation'].iloc[0]['rmse'],\n",
    "             phasecorr_fft[phasecorr_fft['method']=='Phase Correlation'].iloc[0]['rmse'],\n",
    "             baseline_fd['rmse'],\n",
    "             crosscorr_fd[crosscorr_fd['method']=='Cross-Correlation'].iloc[0]['rmse'],\n",
    "             phasecorr_fd[phasecorr_fd['method']=='Phase Correlation'].iloc[0]['rmse']]\n",
    "\n",
    "colors = ['red', 'blue', 'purple', 'red', 'blue', 'purple']\n",
    "plt.bar(methods, rmse_vals, color=colors, alpha=0.7, edgecolor='black')\n",
    "plt.ylabel('RMSE', fontsize=10)\n",
    "plt.title('RMSE Comparison\\n(Lower is Better)', fontweight='bold', fontsize=11)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.yscale('log')\n",
    "\n",
    "# R² comparison\n",
    "ax9 = plt.subplot(3, 3, 9)\n",
    "r2_vals = [baseline_fft['r2'], \n",
    "           crosscorr_fft[crosscorr_fft['method']=='Cross-Correlation'].iloc[0]['r2'],\n",
    "           phasecorr_fft[phasecorr_fft['method']=='Phase Correlation'].iloc[0]['r2'],\n",
    "           baseline_fd['r2'],\n",
    "           crosscorr_fd[crosscorr_fd['method']=='Cross-Correlation'].iloc[0]['r2'],\n",
    "           phasecorr_fd[phasecorr_fd['method']=='Phase Correlation'].iloc[0]['r2']]\n",
    "\n",
    "plt.bar(methods, r2_vals, color=colors, alpha=0.7, edgecolor='black')\n",
    "plt.ylabel('R² Score', fontsize=10)\n",
    "plt.title('R² Comparison\\n(Higher is Better)', fontweight='bold', fontsize=11)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.axhline(y=1.0, color='green', linestyle='--', linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('registration_visual_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"    Saved: registration_visual_comparison.png\")\n",
    "\n",
    "# ===== VISUALIZATION 2: Detailed Performance Heatmaps =====\n",
    "fig2, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Prepare data for heatmaps\n",
    "methods_clean = ['No Reg', 'CrossCorr', 'PhaseCorr']\n",
    "derivatives = ['FFT', 'Finite Diff']\n",
    "\n",
    "# RMSE Heatmap\n",
    "rmse_matrix = np.array([\n",
    "    [baseline_fft['rmse'], \n",
    "     crosscorr_fft[crosscorr_fft['method']=='Cross-Correlation'].iloc[0]['rmse'],\n",
    "     phasecorr_fft[phasecorr_fft['method']=='Phase Correlation'].iloc[0]['rmse']],\n",
    "    [baseline_fd['rmse'],\n",
    "     crosscorr_fd[crosscorr_fd['method']=='Cross-Correlation'].iloc[0]['rmse'],\n",
    "     phasecorr_fd[phasecorr_fd['method']=='Phase Correlation'].iloc[0]['rmse']]\n",
    "])\n",
    "\n",
    "sns.heatmap(rmse_matrix, annot=True, fmt='.4f', cmap='RdYlGn_r', \n",
    "            xticklabels=methods_clean, yticklabels=derivatives, ax=axes[0,0],\n",
    "            cbar_kws={'label': 'RMSE'})\n",
    "axes[0,0].set_title('RMSE by Method', fontweight='bold', fontsize=12)\n",
    "axes[0,0].set_xlabel('Registration Method')\n",
    "axes[0,0].set_ylabel('Derivative Type')\n",
    "\n",
    "# R² Heatmap\n",
    "r2_matrix = np.array([\n",
    "    [baseline_fft['r2'], \n",
    "     crosscorr_fft[crosscorr_fft['method']=='Cross-Correlation'].iloc[0]['r2'],\n",
    "     phasecorr_fft[phasecorr_fft['method']=='Phase Correlation'].iloc[0]['r2']],\n",
    "    [baseline_fd['r2'],\n",
    "     crosscorr_fd[crosscorr_fd['method']=='Cross-Correlation'].iloc[0]['r2'],\n",
    "     phasecorr_fd[phasecorr_fd['method']=='Phase Correlation'].iloc[0]['r2']]\n",
    "])\n",
    "\n",
    "sns.heatmap(r2_matrix, annot=True, fmt='.4f', cmap='RdYlGn', \n",
    "            xticklabels=methods_clean, yticklabels=derivatives, ax=axes[0,1],\n",
    "            cbar_kws={'label': 'R² Score'})\n",
    "axes[0,1].set_title('R² Score by Method', fontweight='bold', fontsize=12)\n",
    "axes[0,1].set_xlabel('Registration Method')\n",
    "axes[0,1].set_ylabel('Derivative Type')\n",
    "\n",
    "# Coefficient Error a\n",
    "error_a_matrix = np.array([\n",
    "    [baseline_fft['abs_error_a'], \n",
    "     crosscorr_fft[crosscorr_fft['method']=='Cross-Correlation'].iloc[0]['abs_error_a'],\n",
    "     phasecorr_fft[phasecorr_fft['method']=='Phase Correlation'].iloc[0]['abs_error_a']],\n",
    "    [baseline_fd['abs_error_a'],\n",
    "     crosscorr_fd[crosscorr_fd['method']=='Cross-Correlation'].iloc[0]['abs_error_a'],\n",
    "     phasecorr_fd[phasecorr_fd['method']=='Phase Correlation'].iloc[0]['abs_error_a']]\n",
    "])\n",
    "\n",
    "sns.heatmap(error_a_matrix, annot=True, fmt='.4f', cmap='RdYlGn_r', \n",
    "            xticklabels=methods_clean, yticklabels=derivatives, ax=axes[1,0],\n",
    "            cbar_kws={'label': '|Error in a|'})\n",
    "axes[1,0].set_title('Absolute Error in Coefficient a', fontweight='bold', fontsize=12)\n",
    "axes[1,0].set_xlabel('Registration Method')\n",
    "axes[1,0].set_ylabel('Derivative Type')\n",
    "\n",
    "# Coefficient Error b\n",
    "error_b_matrix = np.array([\n",
    "    [baseline_fft['abs_error_b'], \n",
    "     crosscorr_fft[crosscorr_fft['method']=='Cross-Correlation'].iloc[0]['abs_error_b'],\n",
    "     phasecorr_fft[phasecorr_fft['method']=='Phase Correlation'].iloc[0]['abs_error_b']],\n",
    "    [baseline_fd['abs_error_b'],\n",
    "     crosscorr_fd[crosscorr_fd['method']=='Cross-Correlation'].iloc[0]['abs_error_b'],\n",
    "     phasecorr_fd[phasecorr_fd['method']=='Phase Correlation'].iloc[0]['abs_error_b']]\n",
    "])\n",
    "\n",
    "sns.heatmap(error_b_matrix, annot=True, fmt='.4f', cmap='RdYlGn_r', \n",
    "            xticklabels=methods_clean, yticklabels=derivatives, ax=axes[1,1],\n",
    "            cbar_kws={'label': '|Error in b|'})\n",
    "axes[1,1].set_title('Absolute Error in Coefficient b', fontweight='bold', fontsize=12)\n",
    "axes[1,1].set_xlabel('Registration Method')\n",
    "axes[1,1].set_ylabel('Derivative Type')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('registration_heatmap_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"    Saved: registration_heatmap_comparison.png\")\n",
    "\n",
    "# ===== Save Comprehensive Comparison =====\n",
    "df_comparison.to_csv('registration_all_methods_comparison.csv', index=False)\n",
    "print(\"    Saved: registration_all_methods_comparison.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"   REGISTRATION COMPARISON COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  1. registration_visual_comparison.png - Visual data & performance comparison\")\n",
    "print(\"  2. registration_heatmap_comparison.png - Performance heatmaps\")\n",
    "print(\"  3. registration_all_methods_comparison.csv - Complete comparison data\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b980384",
   "metadata": {},
   "source": [
    "###  phase 3b Varying IC Problem\n",
    "\n",
    "Each frame comes from different initial condition\n",
    "Frames represent an ensemble of trajectories\n",
    "Trajectories diverge due to chaotic dynamics\n",
    "Cannot \"undo\" this - it's fundamental trajectory uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba48f032",
   "metadata": {},
   "source": [
    "Baseline (no treatment) + FFT derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec07cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from scipy.fft import fft, ifft, fftfreq\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 3B: VARYING INITIAL CONDITIONS - BASELINE (NO TREATMENT) + FFT SINDY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ===== Spectral derivative functions =====\n",
    "def spectral_derivative(u_snapshot, k):\n",
    "    \"\"\"First derivative using FFT\"\"\"\n",
    "    u_hat = fft(u_snapshot)\n",
    "    du_hat = 1j * k * u_hat\n",
    "    return np.real(ifft(du_hat))\n",
    "\n",
    "def spectral_second_derivative(u_snapshot, k):\n",
    "    \"\"\"Second derivative using FFT\"\"\"\n",
    "    u_hat = fft(u_snapshot)\n",
    "    d2u_hat = -(k**2) * u_hat\n",
    "    return np.real(ifft(d2u_hat))\n",
    "\n",
    "def spectral_fourth_derivative(u_snapshot, k):\n",
    "    \"\"\"Fourth derivative using FFT\"\"\"\n",
    "    u_hat = fft(u_snapshot)\n",
    "    d4u_hat = (k**4) * u_hat\n",
    "    return np.real(ifft(d4u_hat))\n",
    "\n",
    "# ===== Evaluation Function =====\n",
    "def evaluate_sindy_fft(u, x, t, method_name=\"Unknown\"):\n",
    "    \"\"\"Apply FFT-based SINDy and compute evaluation metrics\"\"\"\n",
    "    Nx = len(x)\n",
    "    dx = x[1] - x[0]\n",
    "    dt = t[1] - t[0]\n",
    "    \n",
    "    # Define wavenumbers\n",
    "    k = 2 * np.pi * fftfreq(Nx, d=dx)\n",
    "    \n",
    "    # Compute temporal derivative (central difference)\n",
    "    u_t = (u[2:] - u[:-2]) / (2 * dt)\n",
    "    u_mid = u[1:-1]\n",
    "    \n",
    "    # Build feature library using FFT\n",
    "    Theta = []\n",
    "    ut_flat = []\n",
    "    \n",
    "    for snapshot, ut_snapshot in zip(u_mid, u_t):\n",
    "        ux = spectral_derivative(snapshot, k)\n",
    "        uxx = spectral_second_derivative(snapshot, k)\n",
    "        uxxxx = spectral_fourth_derivative(snapshot, k)\n",
    "        \n",
    "        # Feature terms: [u_xx, u_xxxx, u*u_x]\n",
    "        Theta_snapshot = np.vstack([\n",
    "            uxx,\n",
    "            uxxxx,\n",
    "            snapshot * ux\n",
    "        ]).T\n",
    "        \n",
    "        Theta.append(Theta_snapshot)\n",
    "        ut_flat.append(ut_snapshot)\n",
    "    \n",
    "    Theta = np.vstack(Theta)\n",
    "    ut_flat = np.hstack(ut_flat)\n",
    "    \n",
    "    # Normalize features\n",
    "    Theta_mean = Theta.mean(axis=0)\n",
    "    Theta_std = Theta.std(axis=0)\n",
    "    Theta_norm = (Theta - Theta_mean) / Theta_std\n",
    "    \n",
    "    # Lasso regression\n",
    "    lasso = Lasso(alpha=1e-4, fit_intercept=False, max_iter=50000)\n",
    "    lasso.fit(Theta_norm, ut_flat)\n",
    "    coeffs = lasso.coef_ / Theta_std\n",
    "    \n",
    "    # Extract coefficients\n",
    "    a = coeffs[0]\n",
    "    b = coeffs[1]\n",
    "    c = coeffs[2]\n",
    "    \n",
    "    # Predict u_t\n",
    "    u_t_pred = Theta @ coeffs\n",
    "    \n",
    "    # Compute metrics\n",
    "    rmse = np.sqrt(np.mean((ut_flat - u_t_pred)**2))\n",
    "    r2 = r2_score(ut_flat, u_t_pred)\n",
    "    \n",
    "    # True coefficients\n",
    "    a_true, b_true, c_true = -1.0, -1.0, -1.0\n",
    "    \n",
    "    # Coefficient errors\n",
    "    error_a = a - a_true\n",
    "    error_b = b - b_true\n",
    "    error_c = c - c_true\n",
    "    \n",
    "    abs_error_a = abs(error_a)\n",
    "    abs_error_b = abs(error_b)\n",
    "    abs_error_c = abs(error_c)\n",
    "    \n",
    "    return {\n",
    "        'method': method_name,\n",
    "        'a': a,\n",
    "        'b': b,\n",
    "        'c': c,\n",
    "        'error_a': error_a,\n",
    "        'error_b': error_b,\n",
    "        'error_c': error_c,\n",
    "        'abs_error_a': abs_error_a,\n",
    "        'abs_error_b': abs_error_b,\n",
    "        'abs_error_c': abs_error_c,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2\n",
    "    }\n",
    "\n",
    "# ===== Trajectory Ensemble Statistics =====\n",
    "def compute_ensemble_statistics(u_data):\n",
    "    \"\"\"\n",
    "    Compute statistics about trajectory ensemble\n",
    "    Measures trajectory divergence and uncertainty\n",
    "    \"\"\"\n",
    "    # Frame-wise statistics\n",
    "    mean_traj = np.mean(u_data, axis=0)\n",
    "    std_traj = np.std(u_data, axis=0)\n",
    "    \n",
    "    # Temporal variance (how much frames differ)\n",
    "    frame_variance = np.var(u_data, axis=1)\n",
    "    mean_frame_var = np.mean(frame_variance)\n",
    "    \n",
    "    # Trajectory divergence (distance from mean)\n",
    "    divergence = np.sqrt(np.mean((u_data - mean_traj)**2, axis=1))\n",
    "    mean_divergence = np.mean(divergence)\n",
    "    max_divergence = np.max(divergence)\n",
    "    \n",
    "    # Spatial coherence (correlation between frames)\n",
    "    n_frames = u_data.shape[0]\n",
    "    sample_size = min(100, n_frames)  # Sample for efficiency\n",
    "    indices = np.random.choice(n_frames, sample_size, replace=False)\n",
    "    \n",
    "    correlations = []\n",
    "    for i in range(len(indices)-1):\n",
    "        corr = np.corrcoef(u_data[indices[i]], u_data[indices[i+1]])[0, 1]\n",
    "        correlations.append(corr)\n",
    "    \n",
    "    mean_correlation = np.mean(correlations)\n",
    "    \n",
    "    return {\n",
    "        'mean_frame_variance': mean_frame_var,\n",
    "        'mean_divergence': mean_divergence,\n",
    "        'max_divergence': max_divergence,\n",
    "        'mean_correlation': mean_correlation,\n",
    "        'std_spatial_mean': np.mean(std_traj)\n",
    "    }\n",
    "\n",
    "# ===== Main Processing =====\n",
    "print(\"\\n📂 Loading varying initial condition data...\")\n",
    "with h5py.File(\"test_solving_euler_ks_varied_ic.h5\", \"r\") as f:\n",
    "    u_varied = np.array(f[\"u\"])\n",
    "    x = np.array(f[\"x\"])\n",
    "    t = np.array(f[\"t\"])\n",
    "\n",
    "print(f\"Loaded: test_solving_euler_ks_varied_ic.h5\")\n",
    "print(f\"Shape: {u_varied.shape}\")\n",
    "\n",
    "# Compute ensemble statistics\n",
    "print(\"\\n📊 Computing trajectory ensemble statistics...\")\n",
    "ensemble_stats = compute_ensemble_statistics(u_varied)\n",
    "\n",
    "print(\"\\nEnsemble Statistics:\")\n",
    "print(f\"  Mean Frame Variance:     {ensemble_stats['mean_frame_variance']:.6f}\")\n",
    "print(f\"  Mean Trajectory Divergence: {ensemble_stats['mean_divergence']:.6f}\")\n",
    "print(f\"  Max Trajectory Divergence:  {ensemble_stats['max_divergence']:.6f}\")\n",
    "print(f\"  Mean Frame Correlation:     {ensemble_stats['mean_correlation']:.6f}\")\n",
    "print(f\"  Spatial Std (mean):         {ensemble_stats['std_spatial_mean']:.6f}\")\n",
    "\n",
    "# Method 1: Baseline - Use data as-is (no treatment)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"METHOD 1: NO TREATMENT (BASELINE)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Applying SINDy to varying IC data without any filtering or treatment...\")\n",
    "\n",
    "result_baseline = evaluate_sindy_fft(u_varied, x, t, method_name=\"No Treatment\")\n",
    "\n",
    "print(f\"\\nRecovered Coefficients:\")\n",
    "print(f\"  a (u_xx):   {result_baseline['a']:.6f}  (true: -1.0)\")\n",
    "print(f\"  b (u_xxxx): {result_baseline['b']:.6f}  (true: -1.0)\")\n",
    "print(f\"  c (u*u_x):  {result_baseline['c']:.6f}  (true: -1.0)\")\n",
    "print(f\"\\nEvaluation Metrics:\")\n",
    "print(f\"  RMSE: {result_baseline['rmse']:.6f}\")\n",
    "print(f\"  R²:   {result_baseline['r2']:.6f}\")\n",
    "\n",
    "# Load Clean Data for Comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING CLEAN DATA FOR COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with h5py.File(\"test_solving_euler_ks_clean.h5\", \"r\") as f:\n",
    "    u_clean = np.array(f[\"u\"])\n",
    "\n",
    "result_clean = evaluate_sindy_fft(u_clean, x, t, method_name=\"Clean Data\")\n",
    "\n",
    "print(f\"\\nRecovered Coefficients (Clean Data):\")\n",
    "print(f\"  a (u_xx):   {result_clean['a']:.6f}  (true: -1.0)\")\n",
    "print(f\"  b (u_xxxx): {result_clean['b']:.6f}  (true: -1.0)\")\n",
    "print(f\"  c (u*u_x):  {result_clean['c']:.6f}  (true: -1.0)\")\n",
    "print(f\"\\nEvaluation Metrics:\")\n",
    "print(f\"  RMSE: {result_clean['rmse']:.6f}\")\n",
    "print(f\"  R²:   {result_clean['r2']:.6f}\")\n",
    "\n",
    "# Summary Comparison\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'Method':<25} {'a (u_xx)':>12} {'b (u_xxxx)':>12} {'c (u*u_x)':>12} {'RMSE':>12} {'R²':>12}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{result_clean['method']:<25} {result_clean['a']:>12.6f} {result_clean['b']:>12.6f} \"\n",
    "      f\"{result_clean['c']:>12.6f} {result_clean['rmse']:>12.6f} {result_clean['r2']:>12.6f}\")\n",
    "print(f\"{result_baseline['method']:<25} {result_baseline['a']:>12.6f} {result_baseline['b']:>12.6f} \"\n",
    "      f\"{result_baseline['c']:>12.6f} {result_baseline['rmse']:>12.6f} {result_baseline['r2']:>12.6f}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'TRUE VALUES:':<25} {-1.0:>12.6f} {-1.0:>12.6f} {-1.0:>12.6f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Degradation Analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DEGRADATION FROM VARYING INITIAL CONDITIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "degradation_rmse = (result_baseline['rmse'] - result_clean['rmse']) / result_clean['rmse'] * 100\n",
    "degradation_r2 = (result_clean['r2'] - result_baseline['r2']) / result_clean['r2'] * 100\n",
    "\n",
    "print(f\"\\nRMSE Degradation:  {degradation_rmse:+.2f}%\")\n",
    "print(f\"R² Degradation:    {degradation_r2:+.2f}%\")\n",
    "print(f\"\\nCoefficient Error Increase:\")\n",
    "print(f\"  a: {result_clean['abs_error_a']:.6f} → {result_baseline['abs_error_a']:.6f} \"\n",
    "      f\"({(result_baseline['abs_error_a'] - result_clean['abs_error_a'])/result_clean['abs_error_a']*100:+.1f}%)\")\n",
    "print(f\"  b: {result_clean['abs_error_b']:.6f} → {result_baseline['abs_error_b']:.6f} \"\n",
    "      f\"({(result_baseline['abs_error_b'] - result_clean['abs_error_b'])/result_clean['abs_error_b']*100:+.1f}%)\")\n",
    "print(f\"  c: {result_clean['abs_error_c']:.6f} → {result_baseline['abs_error_c']:.6f} \"\n",
    "      f\"({(result_baseline['abs_error_c'] - result_clean['abs_error_c'])/result_clean['abs_error_c']*100:+.1f}%)\")\n",
    "\n",
    "# Visualizations\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"GENERATING VISUALIZATIONS...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Plot 1: Spatiotemporal - Clean\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "im1 = plt.imshow(u_clean[:500].T, cmap='RdBu', aspect='auto', origin='lower',\n",
    "                 extent=(0, 500*t[1], x[0], x[-1]), vmin=-2, vmax=2)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Space (x)')\n",
    "plt.title('Clean Data\\n(Consistent Trajectory)', fontweight='bold')\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "# Plot 2: Spatiotemporal - Varying IC\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "im2 = plt.imshow(u_varied[:500].T, cmap='RdBu', aspect='auto', origin='lower',\n",
    "                 extent=(0, 500*t[1], x[0], x[-1]), vmin=-2, vmax=2)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Space (x)')\n",
    "plt.title('Varying Initial Conditions\\n(Ensemble of Trajectories)', fontweight='bold')\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "# Plot 3: Sample frames comparison\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "sample_idx = len(u_clean) // 2\n",
    "plt.plot(x, u_clean[sample_idx], 'g-', label='Clean', linewidth=2.5, alpha=0.8)\n",
    "plt.plot(x, u_varied[sample_idx], 'r--', label='Varying IC', linewidth=2, alpha=0.7)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('u(x)')\n",
    "plt.title(f'Sample Frame Comparison\\n(Frame {sample_idx})', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Trajectory divergence over time\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "mean_traj = np.mean(u_varied, axis=0)\n",
    "divergence = np.sqrt(np.mean((u_varied - mean_traj)**2, axis=1))\n",
    "plt.plot(t, divergence, 'b-', linewidth=2)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Trajectory Divergence')\n",
    "plt.title('Trajectory Divergence from Mean\\n(Shows IC Uncertainty)', fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Coefficient comparison\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "methods = ['Clean', 'Varying IC']\n",
    "a_vals = [result_clean['a'], result_baseline['a']]\n",
    "b_vals = [result_clean['b'], result_baseline['b']]\n",
    "c_vals = [result_clean['c'], result_baseline['c']]\n",
    "x_pos = np.arange(len(methods))\n",
    "width = 0.25\n",
    "plt.bar(x_pos - width, a_vals, width, label='a (u_xx)', alpha=0.8)\n",
    "plt.bar(x_pos, b_vals, width, label='b (u_xxxx)', alpha=0.8)\n",
    "plt.bar(x_pos + width, c_vals, width, label='c (u*u_x)', alpha=0.8)\n",
    "plt.axhline(y=-1.0, color='red', linestyle='--', linewidth=2, label='True')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('Recovered Coefficients', fontweight='bold')\n",
    "plt.xticks(x_pos, methods)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 6: RMSE and R² comparison\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "metrics = ['RMSE', 'R²']\n",
    "clean_vals = [result_clean['rmse'], result_clean['r2']]\n",
    "varied_vals = [result_baseline['rmse'], result_baseline['r2']]\n",
    "x_pos = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "bars1 = plt.bar(x_pos - width/2, clean_vals, width, label='Clean', alpha=0.8, color='green')\n",
    "bars2 = plt.bar(x_pos + width/2, varied_vals, width, label='Varying IC', alpha=0.8, color='red')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Metrics Comparison', fontweight='bold')\n",
    "plt.xticks(x_pos, metrics)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.4f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('varying_ic_baseline_fft_results.png', dpi=300, bbox_inches='tight')\n",
    "print(\"    Saved: varying_ic_baseline_fft_results.png\")\n",
    "\n",
    "# Save Results\n",
    "results_dict = {\n",
    "    'method': ['Clean Data', 'Varying IC (No Treatment)'],\n",
    "    'a': [result_clean['a'], result_baseline['a']],\n",
    "    'b': [result_clean['b'], result_baseline['b']],\n",
    "    'c': [result_clean['c'], result_baseline['c']],\n",
    "    'rmse': [result_clean['rmse'], result_baseline['rmse']],\n",
    "    'r2': [result_clean['r2'], result_baseline['r2']],\n",
    "    'abs_error_a': [result_clean['abs_error_a'], result_baseline['abs_error_a']],\n",
    "    'abs_error_b': [result_clean['abs_error_b'], result_baseline['abs_error_b']],\n",
    "    'abs_error_c': [result_clean['abs_error_c'], result_baseline['abs_error_c']],\n",
    "    'mean_frame_variance': [0.0, ensemble_stats['mean_frame_variance']],\n",
    "    'mean_divergence': [0.0, ensemble_stats['mean_divergence']],\n",
    "    'max_divergence': [0.0, ensemble_stats['max_divergence']]\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(results_dict)\n",
    "df_results.to_csv('varying_ic_baseline_fft_results.csv', index=False)\n",
    "print(\"    Saved: varying_ic_baseline_fft_results.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"   VARYING IC BASELINE + FFT SINDY COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  1. varying_ic_baseline_fft_results.png - Visualization\")\n",
    "print(\"  2. varying_ic_baseline_fft_results.csv - Results data\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7c0570",
   "metadata": {},
   "source": [
    "Baseline (no treatment) + Finite Difference derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7e5d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 3B: VARYING INITIAL CONDITIONS - BASELINE (NO TREATMENT) + FINITE DIFFERENCE SINDY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ===== Evaluation Function with Finite Difference =====\n",
    "def evaluate_sindy_finite_diff(u, x, t, method_name=\"Unknown\"):\n",
    "    \"\"\"Apply Finite Difference SINDy and compute evaluation metrics\"\"\"\n",
    "    dx = float(x[1] - x[0])\n",
    "    dt = float(t[1] - t[0])\n",
    "    \n",
    "    def d1(U): \n",
    "        return (np.roll(U, -1, 1) - np.roll(U, 1, 1)) / (2*dx)\n",
    "    def d2(U): \n",
    "        return (np.roll(U, -1, 1) - 2*U + np.roll(U, 1, 1)) / (dx**2)\n",
    "    def d4(U):\n",
    "        return (np.roll(U, -2, 1) - 4*np.roll(U, -1, 1) + 6*U - \n",
    "                4*np.roll(U, 1, 1) + np.roll(U, 2, 1)) / (dx**4)\n",
    "    \n",
    "    U_t = (u[1:] - u[:-1]) / dt\n",
    "    U = u[:-1]\n",
    "    U_x = d1(U)\n",
    "    U_xx = d2(U)\n",
    "    U_xxxx = d4(U)\n",
    "    U2_x = d1(U**2)\n",
    "    \n",
    "    Theta = np.column_stack([\n",
    "        np.ones_like(U).ravel(),\n",
    "        U.ravel(),\n",
    "        U_x.ravel(),\n",
    "        U_xx.ravel(),\n",
    "        U_xxxx.ravel(),\n",
    "        U2_x.ravel()\n",
    "    ])\n",
    "    y = U_t.ravel()\n",
    "    names = [\"1\", \"u\", \"u_x\", \"u_xx\", \"u_xxxx\", \"(u^2)_x\"]\n",
    "    \n",
    "    Theta_mean = Theta.mean(0)\n",
    "    Theta_std = Theta.std(0) + 1e-8\n",
    "    Theta_n = (Theta - Theta_mean) / Theta_std\n",
    "    \n",
    "    lasso = Lasso(alpha=1e-4, fit_intercept=False, max_iter=50000, tol=1e-6)\n",
    "    lasso.fit(Theta_n, y)\n",
    "    coef = lasso.coef_ / Theta_std\n",
    "    \n",
    "    a = coef[names.index(\"u_xx\")]\n",
    "    b = coef[names.index(\"u_xxxx\")]\n",
    "    c_phys = coef[names.index(\"(u^2)_x\")]\n",
    "    \n",
    "    y_pred = Theta @ coef\n",
    "    rmse = np.sqrt(np.mean((y - y_pred)**2))\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    \n",
    "    a_true, b_true, c_true = -1.0, -1.0, -0.5\n",
    "    \n",
    "    return {\n",
    "        'method': method_name,\n",
    "        'a': a,\n",
    "        'b': b,\n",
    "        'c': c_phys,\n",
    "        'error_a': a - a_true,\n",
    "        'error_b': b - b_true,\n",
    "        'error_c': c_phys - c_true,\n",
    "        'abs_error_a': abs(a - a_true),\n",
    "        'abs_error_b': abs(b - b_true),\n",
    "        'abs_error_c': abs(c_phys - c_true),\n",
    "        'rmse': rmse,\n",
    "        'r2': r2\n",
    "    }\n",
    "\n",
    "# ===== Trajectory Ensemble Statistics =====\n",
    "def compute_ensemble_statistics(u_data):\n",
    "    \"\"\"Compute statistics about trajectory ensemble\"\"\"\n",
    "    mean_traj = np.mean(u_data, axis=0)\n",
    "    std_traj = np.std(u_data, axis=0)\n",
    "    frame_variance = np.var(u_data, axis=1)\n",
    "    mean_frame_var = np.mean(frame_variance)\n",
    "    divergence = np.sqrt(np.mean((u_data - mean_traj)**2, axis=1))\n",
    "    mean_divergence = np.mean(divergence)\n",
    "    max_divergence = np.max(divergence)\n",
    "    \n",
    "    n_frames = u_data.shape[0]\n",
    "    sample_size = min(100, n_frames)\n",
    "    indices = np.random.choice(n_frames, sample_size, replace=False)\n",
    "    correlations = []\n",
    "    for i in range(len(indices)-1):\n",
    "        corr = np.corrcoef(u_data[indices[i]], u_data[indices[i+1]])[0, 1]\n",
    "        correlations.append(corr)\n",
    "    mean_correlation = np.mean(correlations)\n",
    "    \n",
    "    return {\n",
    "        'mean_frame_variance': mean_frame_var,\n",
    "        'mean_divergence': mean_divergence,\n",
    "        'max_divergence': max_divergence,\n",
    "        'mean_correlation': mean_correlation,\n",
    "        'std_spatial_mean': np.mean(std_traj)\n",
    "    }\n",
    "\n",
    "# ===== Main Processing =====\n",
    "print(\"\\n📂 Loading varying initial condition data...\")\n",
    "with h5py.File(\"test_solving_euler_ks_varied_ic.h5\", \"r\") as f:\n",
    "    u_varied = np.array(f[\"u\"], dtype=np.float32)\n",
    "    x = np.array(f[\"x\"], dtype=np.float32)\n",
    "    t = np.array(f[\"t\"], dtype=np.float32)\n",
    "\n",
    "print(f\"Loaded: test_solving_euler_ks_varied_ic.h5\")\n",
    "print(f\"Shape: {u_varied.shape}\")\n",
    "\n",
    "# Compute ensemble statistics\n",
    "print(\"\\n📊 Computing trajectory ensemble statistics...\")\n",
    "ensemble_stats = compute_ensemble_statistics(u_varied)\n",
    "print(f\"\\nEnsemble Statistics:\")\n",
    "print(f\"  Mean Frame Variance:     {ensemble_stats['mean_frame_variance']:.6f}\")\n",
    "print(f\"  Mean Trajectory Divergence: {ensemble_stats['mean_divergence']:.6f}\")\n",
    "print(f\"  Max Trajectory Divergence:  {ensemble_stats['max_divergence']:.6f}\")\n",
    "print(f\"  Mean Frame Correlation:     {ensemble_stats['mean_correlation']:.6f}\")\n",
    "\n",
    "# Method 1: Baseline - No treatment\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"METHOD 1: NO TREATMENT (BASELINE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "result_baseline = evaluate_sindy_finite_diff(u_varied, x, t, method_name=\"No Treatment\")\n",
    "print(f\"\\nRecovered: a={result_baseline['a']:.6f}, b={result_baseline['b']:.6f}, c={result_baseline['c']:.6f}\")\n",
    "print(f\"RMSE={result_baseline['rmse']:.6f}, R²={result_baseline['r2']:.6f}\")\n",
    "\n",
    "# Load Clean Data\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLEAN DATA COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with h5py.File(\"test_solving_euler_ks_clean.h5\", \"r\") as f:\n",
    "    u_clean = np.array(f[\"u\"], dtype=np.float32)\n",
    "\n",
    "result_clean = evaluate_sindy_finite_diff(u_clean, x, t, method_name=\"Clean Data\")\n",
    "print(f\"Recovered: a={result_clean['a']:.6f}, b={result_clean['b']:.6f}, c={result_clean['c']:.6f}\")\n",
    "print(f\"RMSE={result_clean['rmse']:.6f}, R²={result_clean['r2']:.6f}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Method':<25} {'a':>12} {'b':>12} {'c':>12} {'RMSE':>12} {'R²':>12}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{result_clean['method']:<25} {result_clean['a']:>12.6f} {result_clean['b']:>12.6f} \"\n",
    "      f\"{result_clean['c']:>12.6f} {result_clean['rmse']:>12.6f} {result_clean['r2']:>12.6f}\")\n",
    "print(f\"{result_baseline['method']:<25} {result_baseline['a']:>12.6f} {result_baseline['b']:>12.6f} \"\n",
    "      f\"{result_baseline['c']:>12.6f} {result_baseline['rmse']:>12.6f} {result_baseline['r2']:>12.6f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save Results\n",
    "results_dict = {\n",
    "    'method': ['Clean Data', 'Varying IC (No Treatment)'],\n",
    "    'a': [result_clean['a'], result_baseline['a']],\n",
    "    'b': [result_clean['b'], result_baseline['b']],\n",
    "    'c': [result_clean['c'], result_baseline['c']],\n",
    "    'rmse': [result_clean['rmse'], result_baseline['rmse']],\n",
    "    'r2': [result_clean['r2'], result_baseline['r2']],\n",
    "    'abs_error_a': [result_clean['abs_error_a'], result_baseline['abs_error_a']],\n",
    "    'abs_error_b': [result_clean['abs_error_b'], result_baseline['abs_error_b']],\n",
    "    'abs_error_c': [result_clean['abs_error_c'], result_baseline['abs_error_c']],\n",
    "    'mean_frame_variance': [0.0, ensemble_stats['mean_frame_variance']],\n",
    "    'mean_divergence': [0.0, ensemble_stats['mean_divergence']],\n",
    "    'max_divergence': [0.0, ensemble_stats['max_divergence']]\n",
    "}\n",
    "\n",
    "pd.DataFrame(results_dict).to_csv('varying_ic_baseline_fd_results.csv', index=False)\n",
    "print(\"\\n    Saved: varying_ic_baseline_fd_results.csv\")\n",
    "print(\"\\n   VARYING IC BASELINE + FINITE DIFFERENCE SINDY COMPLETE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4a133b",
   "metadata": {},
   "source": [
    "Robust regression (Huber loss) + FFT derivatives\n",
    "Tests 2 epsilon values (1.35, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33703f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from scipy.fft import fft, ifft, fftfreq\n",
    "from sklearn.linear_model import Lasso, HuberRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 3B: VARYING INITIAL CONDITIONS - ROBUST REGRESSION + FFT SINDY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ===== Spectral derivative functions =====\n",
    "def spectral_derivative(u_snapshot, k):\n",
    "    \"\"\"First derivative using FFT\"\"\"\n",
    "    u_hat = fft(u_snapshot)\n",
    "    du_hat = 1j * k * u_hat\n",
    "    return np.real(ifft(du_hat))\n",
    "\n",
    "def spectral_second_derivative(u_snapshot, k):\n",
    "    \"\"\"Second derivative using FFT\"\"\"\n",
    "    u_hat = fft(u_snapshot)\n",
    "    d2u_hat = -(k**2) * u_hat\n",
    "    return np.real(ifft(d2u_hat))\n",
    "\n",
    "def spectral_fourth_derivative(u_snapshot, k):\n",
    "    \"\"\"Fourth derivative using FFT\"\"\"\n",
    "    u_hat = fft(u_snapshot)\n",
    "    d4u_hat = (k**4) * u_hat\n",
    "    return np.real(ifft(d4u_hat))\n",
    "\n",
    "# ===== Evaluation Functions =====\n",
    "def evaluate_sindy_fft_standard(u, x, t, method_name=\"Unknown\"):\n",
    "    \"\"\"Standard Lasso regression (L1 penalty)\"\"\"\n",
    "    Nx = len(x)\n",
    "    dx = x[1] - x[0]\n",
    "    dt = t[1] - t[0]\n",
    "    k = 2 * np.pi * fftfreq(Nx, d=dx)\n",
    "    \n",
    "    u_t = (u[2:] - u[:-2]) / (2 * dt)\n",
    "    u_mid = u[1:-1]\n",
    "    \n",
    "    Theta = []\n",
    "    ut_flat = []\n",
    "    \n",
    "    for snapshot, ut_snapshot in zip(u_mid, u_t):\n",
    "        ux = spectral_derivative(snapshot, k)\n",
    "        uxx = spectral_second_derivative(snapshot, k)\n",
    "        uxxxx = spectral_fourth_derivative(snapshot, k)\n",
    "        Theta_snapshot = np.vstack([uxx, uxxxx, snapshot * ux]).T\n",
    "        Theta.append(Theta_snapshot)\n",
    "        ut_flat.append(ut_snapshot)\n",
    "    \n",
    "    Theta = np.vstack(Theta)\n",
    "    ut_flat = np.hstack(ut_flat)\n",
    "    \n",
    "    Theta_mean = Theta.mean(axis=0)\n",
    "    Theta_std = Theta.std(axis=0)\n",
    "    Theta_norm = (Theta - Theta_mean) / Theta_std\n",
    "    \n",
    "    lasso = Lasso(alpha=1e-4, fit_intercept=False, max_iter=50000)\n",
    "    lasso.fit(Theta_norm, ut_flat)\n",
    "    coeffs = lasso.coef_ / Theta_std\n",
    "    \n",
    "    a, b, c = coeffs[0], coeffs[1], coeffs[2]\n",
    "    u_t_pred = Theta @ coeffs\n",
    "    rmse = np.sqrt(np.mean((ut_flat - u_t_pred)**2))\n",
    "    r2 = r2_score(ut_flat, u_t_pred)\n",
    "    \n",
    "    a_true, b_true, c_true = -1.0, -1.0, -1.0\n",
    "    \n",
    "    return {\n",
    "        'method': method_name,\n",
    "        'a': a, 'b': b, 'c': c,\n",
    "        'error_a': a - a_true,\n",
    "        'error_b': b - b_true,\n",
    "        'error_c': c - c_true,\n",
    "        'abs_error_a': abs(a - a_true),\n",
    "        'abs_error_b': abs(b - b_true),\n",
    "        'abs_error_c': abs(c - c_true),\n",
    "        'rmse': rmse,\n",
    "        'r2': r2\n",
    "    }\n",
    "\n",
    "def evaluate_sindy_fft_robust(u, x, t, method_name=\"Unknown\", epsilon=1.35):\n",
    "    \"\"\"\n",
    "    Robust regression using Huber loss\n",
    "    Epsilon parameter controls outlier threshold (default 1.35 is standard)\n",
    "    Smaller epsilon = more aggressive outlier downweighting\n",
    "    \"\"\"\n",
    "    Nx = len(x)\n",
    "    dx = x[1] - x[0]\n",
    "    dt = t[1] - t[0]\n",
    "    k = 2 * np.pi * fftfreq(Nx, d=dx)\n",
    "    \n",
    "    u_t = (u[2:] - u[:-2]) / (2 * dt)\n",
    "    u_mid = u[1:-1]\n",
    "    \n",
    "    Theta = []\n",
    "    ut_flat = []\n",
    "    \n",
    "    for snapshot, ut_snapshot in zip(u_mid, u_t):\n",
    "        ux = spectral_derivative(snapshot, k)\n",
    "        uxx = spectral_second_derivative(snapshot, k)\n",
    "        uxxxx = spectral_fourth_derivative(snapshot, k)\n",
    "        Theta_snapshot = np.vstack([uxx, uxxxx, snapshot * ux]).T\n",
    "        Theta.append(Theta_snapshot)\n",
    "        ut_flat.append(ut_snapshot)\n",
    "    \n",
    "    Theta = np.vstack(Theta)\n",
    "    ut_flat = np.hstack(ut_flat)\n",
    "    \n",
    "    Theta_mean = Theta.mean(axis=0)\n",
    "    Theta_std = Theta.std(axis=0)\n",
    "    Theta_norm = (Theta - Theta_mean) / Theta_std\n",
    "    \n",
    "    # Huber Regressor (robust to outliers)\n",
    "    huber = HuberRegressor(epsilon=epsilon, fit_intercept=False, max_iter=1000, alpha=1e-4)\n",
    "    huber.fit(Theta_norm, ut_flat)\n",
    "    coeffs = huber.coef_ / Theta_std\n",
    "    \n",
    "    # Count outliers detected\n",
    "    residuals = ut_flat - huber.predict(Theta_norm)\n",
    "    threshold = epsilon * np.median(np.abs(residuals))\n",
    "    n_outliers = np.sum(np.abs(residuals) > threshold)\n",
    "    outlier_fraction = n_outliers / len(residuals)\n",
    "    \n",
    "    a, b, c = coeffs[0], coeffs[1], coeffs[2]\n",
    "    u_t_pred = Theta @ coeffs\n",
    "    rmse = np.sqrt(np.mean((ut_flat - u_t_pred)**2))\n",
    "    r2 = r2_score(ut_flat, u_t_pred)\n",
    "    \n",
    "    a_true, b_true, c_true = -1.0, -1.0, -1.0\n",
    "    \n",
    "    return {\n",
    "        'method': method_name,\n",
    "        'a': a, 'b': b, 'c': c,\n",
    "        'error_a': a - a_true,\n",
    "        'error_b': b - b_true,\n",
    "        'error_c': c - c_true,\n",
    "        'abs_error_a': abs(a - a_true),\n",
    "        'abs_error_b': abs(b - b_true),\n",
    "        'abs_error_c': abs(c - c_true),\n",
    "        'rmse': rmse,\n",
    "        'r2': r2,\n",
    "        'n_outliers': n_outliers,\n",
    "        'outlier_fraction': outlier_fraction\n",
    "    }\n",
    "\n",
    "# ===== Main Processing =====\n",
    "print(\"\\n📂 Loading varying initial condition data...\")\n",
    "with h5py.File(\"test_solving_euler_ks_varied_ic.h5\", \"r\") as f:\n",
    "    u_varied = np.array(f[\"u\"])\n",
    "    x = np.array(f[\"x\"])\n",
    "    t = np.array(f[\"t\"])\n",
    "\n",
    "print(f\"Loaded: test_solving_euler_ks_varied_ic.h5\")\n",
    "print(f\"Shape: {u_varied.shape}\")\n",
    "\n",
    "# Method 1: Standard Lasso (baseline)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"METHOD 1: STANDARD LASSO (BASELINE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "result_standard = evaluate_sindy_fft_standard(u_varied, x, t, method_name=\"Standard Lasso\")\n",
    "print(f\"\\nRecovered: a={result_standard['a']:.6f}, b={result_standard['b']:.6f}, c={result_standard['c']:.6f}\")\n",
    "print(f\"RMSE={result_standard['rmse']:.6f}, R²={result_standard['r2']:.6f}\")\n",
    "\n",
    "# Method 2: Robust Regression (epsilon=1.35, standard)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"METHOD 2: ROBUST REGRESSION (Huber Loss, ε=1.35)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "result_robust = evaluate_sindy_fft_robust(u_varied, x, t, method_name=\"Robust (ε=1.35)\", epsilon=1.35)\n",
    "print(f\"\\nRecovered: a={result_robust['a']:.6f}, b={result_robust['b']:.6f}, c={result_robust['c']:.6f}\")\n",
    "print(f\"RMSE={result_robust['rmse']:.6f}, R²={result_robust['r2']:.6f}\")\n",
    "print(f\"Outliers detected: {result_robust['n_outliers']} ({result_robust['outlier_fraction']*100:.2f}%)\")\n",
    "\n",
    "# Method 3: More aggressive outlier rejection (epsilon=1.0)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"METHOD 3: AGGRESSIVE ROBUST REGRESSION (Huber Loss, ε=1.0)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "result_robust_aggressive = evaluate_sindy_fft_robust(u_varied, x, t, method_name=\"Robust (ε=1.0)\", epsilon=1.0)\n",
    "print(f\"\\nRecovered: a={result_robust_aggressive['a']:.6f}, b={result_robust_aggressive['b']:.6f}, c={result_robust_aggressive['c']:.6f}\")\n",
    "print(f\"RMSE={result_robust_aggressive['rmse']:.6f}, R²={result_robust_aggressive['r2']:.6f}\")\n",
    "print(f\"Outliers detected: {result_robust_aggressive['n_outliers']} ({result_robust_aggressive['outlier_fraction']*100:.2f}%)\")\n",
    "\n",
    "# Load Clean Data\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLEAN DATA COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with h5py.File(\"test_solving_euler_ks_clean.h5\", \"r\") as f:\n",
    "    u_clean = np.array(f[\"u\"])\n",
    "\n",
    "result_clean = evaluate_sindy_fft_standard(u_clean, x, t, method_name=\"Clean Data\")\n",
    "print(f\"Recovered: a={result_clean['a']:.6f}, b={result_clean['b']:.6f}, c={result_clean['c']:.6f}\")\n",
    "print(f\"RMSE={result_clean['rmse']:.6f}, R²={result_clean['r2']:.6f}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Method':<25} {'a':>12} {'b':>12} {'c':>12} {'RMSE':>12} {'R²':>10}\")\n",
    "print(\"-\"*80)\n",
    "for r in [result_clean, result_standard, result_robust, result_robust_aggressive]:\n",
    "    print(f\"{r['method']:<25} {r['a']:>12.6f} {r['b']:>12.6f} {r['c']:>12.6f} \"\n",
    "          f\"{r['rmse']:>12.6f} {r['r2']:>10.6f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Improvement Analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IMPROVEMENT FROM ROBUST REGRESSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "improv_robust = (result_standard['rmse'] - result_robust['rmse']) / result_standard['rmse'] * 100\n",
    "improv_aggressive = (result_standard['rmse'] - result_robust_aggressive['rmse']) / result_standard['rmse'] * 100\n",
    "\n",
    "print(f\"\\nRobust (ε=1.35):  RMSE {improv_robust:+.2f}%, R² change {(result_robust['r2']-result_standard['r2'])*100:+.2f}%\")\n",
    "print(f\"Robust (ε=1.0):   RMSE {improv_aggressive:+.2f}%, R² change {(result_robust_aggressive['r2']-result_standard['r2'])*100:+.2f}%\")\n",
    "\n",
    "# Save Results\n",
    "results_dict = {\n",
    "    'method': ['Clean Data', 'Standard Lasso', 'Robust (ε=1.35)', 'Robust (ε=1.0)'],\n",
    "    'a': [result_clean['a'], result_standard['a'], result_robust['a'], result_robust_aggressive['a']],\n",
    "    'b': [result_clean['b'], result_standard['b'], result_robust['b'], result_robust_aggressive['b']],\n",
    "    'c': [result_clean['c'], result_standard['c'], result_robust['c'], result_robust_aggressive['c']],\n",
    "    'rmse': [result_clean['rmse'], result_standard['rmse'], result_robust['rmse'], result_robust_aggressive['rmse']],\n",
    "    'r2': [result_clean['r2'], result_standard['r2'], result_robust['r2'], result_robust_aggressive['r2']],\n",
    "    'abs_error_a': [result_clean['abs_error_a'], result_standard['abs_error_a'], result_robust['abs_error_a'], result_robust_aggressive['abs_error_a']],\n",
    "    'abs_error_b': [result_clean['abs_error_b'], result_standard['abs_error_b'], result_robust['abs_error_b'], result_robust_aggressive['abs_error_b']],\n",
    "    'abs_error_c': [result_clean['abs_error_c'], result_standard['abs_error_c'], result_robust['abs_error_c'], result_robust_aggressive['abs_error_c']],\n",
    "    'outliers_detected': [0, 0, result_robust['n_outliers'], result_robust_aggressive['n_outliers']]\n",
    "}\n",
    "\n",
    "pd.DataFrame(results_dict).to_csv('varying_ic_robust_fft_results.csv', index=False)\n",
    "print(\"\\n    Saved: varying_ic_robust_fft_results.csv\")\n",
    "print(\"\\n   VARYING IC ROBUST REGRESSION + FFT SINDY COMPLETE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3892c18a",
   "metadata": {},
   "source": [
    "Robust regression (Huber loss) + Finite Difference derivatives\n",
    "Tests 2 epsilon values (1.35, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8e8a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso, HuberRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 3B: VARYING INITIAL CONDITIONS - ROBUST REGRESSION + FINITE DIFFERENCE SINDY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ===== Evaluation Functions =====\n",
    "def evaluate_sindy_fd_standard(u, x, t, method_name=\"Unknown\"):\n",
    "    \"\"\"Standard Lasso regression\"\"\"\n",
    "    dx = float(x[1] - x[0])\n",
    "    dt = float(t[1] - t[0])\n",
    "    \n",
    "    def d1(U): return (np.roll(U, -1, 1) - np.roll(U, 1, 1)) / (2*dx)\n",
    "    def d2(U): return (np.roll(U, -1, 1) - 2*U + np.roll(U, 1, 1)) / (dx**2)\n",
    "    def d4(U): return (np.roll(U, -2, 1) - 4*np.roll(U, -1, 1) + 6*U - 4*np.roll(U, 1, 1) + np.roll(U, 2, 1)) / (dx**4)\n",
    "    \n",
    "    U_t = (u[1:] - u[:-1]) / dt\n",
    "    U = u[:-1]\n",
    "    Theta = np.column_stack([np.ones_like(U).ravel(), U.ravel(), d1(U).ravel(), d2(U).ravel(), d4(U).ravel(), d1(U**2).ravel()])\n",
    "    y = U_t.ravel()\n",
    "    names = [\"1\", \"u\", \"u_x\", \"u_xx\", \"u_xxxx\", \"(u^2)_x\"]\n",
    "    \n",
    "    Theta_mean = Theta.mean(0)\n",
    "    Theta_std = Theta.std(0) + 1e-8\n",
    "    Theta_n = (Theta - Theta_mean) / Theta_std\n",
    "    \n",
    "    lasso = Lasso(alpha=1e-4, fit_intercept=False, max_iter=50000, tol=1e-6)\n",
    "    lasso.fit(Theta_n, y)\n",
    "    coef = lasso.coef_ / Theta_std\n",
    "    \n",
    "    a = coef[names.index(\"u_xx\")]\n",
    "    b = coef[names.index(\"u_xxxx\")]\n",
    "    c = coef[names.index(\"(u^2)_x\")]\n",
    "    \n",
    "    y_pred = Theta @ coef\n",
    "    rmse = np.sqrt(np.mean((y - y_pred)**2))\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    \n",
    "    a_true, b_true, c_true = -1.0, -1.0, -0.5\n",
    "    \n",
    "    return {\n",
    "        'method': method_name, 'a': a, 'b': b, 'c': c,\n",
    "        'error_a': a - a_true, 'error_b': b - b_true, 'error_c': c - c_true,\n",
    "        'abs_error_a': abs(a - a_true), 'abs_error_b': abs(b - b_true), 'abs_error_c': abs(c - c_true),\n",
    "        'rmse': rmse, 'r2': r2\n",
    "    }\n",
    "\n",
    "def evaluate_sindy_fd_robust(u, x, t, method_name=\"Unknown\", epsilon=1.35):\n",
    "    \"\"\"Robust regression using Huber loss\"\"\"\n",
    "    dx = float(x[1] - x[0])\n",
    "    dt = float(t[1] - t[0])\n",
    "    \n",
    "    def d1(U): return (np.roll(U, -1, 1) - np.roll(U, 1, 1)) / (2*dx)\n",
    "    def d2(U): return (np.roll(U, -1, 1) - 2*U + np.roll(U, 1, 1)) / (dx**2)\n",
    "    def d4(U): return (np.roll(U, -2, 1) - 4*np.roll(U, -1, 1) + 6*U - 4*np.roll(U, 1, 1) + np.roll(U, 2, 1)) / (dx**4)\n",
    "    \n",
    "    U_t = (u[1:] - u[:-1]) / dt\n",
    "    U = u[:-1]\n",
    "    Theta = np.column_stack([np.ones_like(U).ravel(), U.ravel(), d1(U).ravel(), d2(U).ravel(), d4(U).ravel(), d1(U**2).ravel()])\n",
    "    y = U_t.ravel()\n",
    "    names = [\"1\", \"u\", \"u_x\", \"u_xx\", \"u_xxxx\", \"(u^2)_x\"]\n",
    "    \n",
    "    Theta_mean = Theta.mean(0)\n",
    "    Theta_std = Theta.std(0) + 1e-8\n",
    "    Theta_n = (Theta - Theta_mean) / Theta_std\n",
    "    \n",
    "    huber = HuberRegressor(epsilon=epsilon, fit_intercept=False, max_iter=1000, alpha=1e-4)\n",
    "    huber.fit(Theta_n, y)\n",
    "    coef = huber.coef_ / Theta_std\n",
    "    \n",
    "    residuals = y - huber.predict(Theta_n)\n",
    "    threshold = epsilon * np.median(np.abs(residuals))\n",
    "    n_outliers = np.sum(np.abs(residuals) > threshold)\n",
    "    outlier_fraction = n_outliers / len(residuals)\n",
    "    \n",
    "    a = coef[names.index(\"u_xx\")]\n",
    "    b = coef[names.index(\"u_xxxx\")]\n",
    "    c = coef[names.index(\"(u^2)_x\")]\n",
    "    \n",
    "    y_pred = Theta @ coef\n",
    "    rmse = np.sqrt(np.mean((y - y_pred)**2))\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    \n",
    "    a_true, b_true, c_true = -1.0, -1.0, -0.5\n",
    "    \n",
    "    return {\n",
    "        'method': method_name, 'a': a, 'b': b, 'c': c,\n",
    "        'error_a': a - a_true, 'error_b': b - b_true, 'error_c': c - c_true,\n",
    "        'abs_error_a': abs(a - a_true), 'abs_error_b': abs(b - b_true), 'abs_error_c': abs(c - c_true),\n",
    "        'rmse': rmse, 'r2': r2, 'n_outliers': n_outliers, 'outlier_fraction': outlier_fraction\n",
    "    }\n",
    "\n",
    "# ===== Main Processing =====\n",
    "print(\"\\n📂 Loading varying initial condition data...\")\n",
    "with h5py.File(\"test_solving_euler_ks_varied_ic.h5\", \"r\") as f:\n",
    "    u_varied = np.array(f[\"u\"], dtype=np.float32)\n",
    "    x = np.array(f[\"x\"], dtype=np.float32)\n",
    "    t = np.array(f[\"t\"], dtype=np.float32)\n",
    "\n",
    "print(f\"Loaded: {u_varied.shape}\")\n",
    "\n",
    "# Method 1: Standard Lasso\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"METHOD 1: STANDARD LASSO\")\n",
    "print(\"=\"*80)\n",
    "result_standard = evaluate_sindy_fd_standard(u_varied, x, t, \"Standard Lasso\")\n",
    "print(f\"Recovered: a={result_standard['a']:.6f}, b={result_standard['b']:.6f}, c={result_standard['c']:.6f}\")\n",
    "print(f\"RMSE={result_standard['rmse']:.6f}, R²={result_standard['r2']:.6f}\")\n",
    "\n",
    "# Method 2: Robust (ε=1.35)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"METHOD 2: ROBUST REGRESSION (ε=1.35)\")\n",
    "print(\"=\"*80)\n",
    "result_robust = evaluate_sindy_fd_robust(u_varied, x, t, \"Robust (ε=1.35)\", 1.35)\n",
    "print(f\"Recovered: a={result_robust['a']:.6f}, b={result_robust['b']:.6f}, c={result_robust['c']:.6f}\")\n",
    "print(f\"RMSE={result_robust['rmse']:.6f}, R²={result_robust['r2']:.6f}\")\n",
    "print(f\"Outliers: {result_robust['n_outliers']} ({result_robust['outlier_fraction']*100:.2f}%)\")\n",
    "\n",
    "# Method 3: Aggressive (ε=1.0)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"METHOD 3: AGGRESSIVE ROBUST (ε=1.0)\")\n",
    "print(\"=\"*80)\n",
    "result_robust_agg = evaluate_sindy_fd_robust(u_varied, x, t, \"Robust (ε=1.0)\", 1.0)\n",
    "print(f\"Recovered: a={result_robust_agg['a']:.6f}, b={result_robust_agg['b']:.6f}, c={result_robust_agg['c']:.6f}\")\n",
    "print(f\"RMSE={result_robust_agg['rmse']:.6f}, R²={result_robust_agg['r2']:.6f}\")\n",
    "print(f\"Outliers: {result_robust_agg['n_outliers']} ({result_robust_agg['outlier_fraction']*100:.2f}%)\")\n",
    "\n",
    "# Clean Data\n",
    "with h5py.File(\"test_solving_euler_ks_clean.h5\", \"r\") as f:\n",
    "    u_clean = np.array(f[\"u\"], dtype=np.float32)\n",
    "result_clean = evaluate_sindy_fd_standard(u_clean, x, t, \"Clean Data\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Method':<25} {'a':>12} {'b':>12} {'c':>12} {'RMSE':>12} {'R²':>10}\")\n",
    "print(\"-\"*80)\n",
    "for r in [result_clean, result_standard, result_robust, result_robust_agg]:\n",
    "    print(f\"{r['method']:<25} {r['a']:>12.6f} {r['b']:>12.6f} {r['c']:>12.6f} {r['rmse']:>12.6f} {r['r2']:>10.6f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save\n",
    "pd.DataFrame({\n",
    "    'method': ['Clean Data', 'Standard Lasso', 'Robust (ε=1.35)', 'Robust (ε=1.0)'],\n",
    "    'a': [result_clean['a'], result_standard['a'], result_robust['a'], result_robust_agg['a']],\n",
    "    'b': [result_clean['b'], result_standard['b'], result_robust['b'], result_robust_agg['b']],\n",
    "    'c': [result_clean['c'], result_standard['c'], result_robust['c'], result_robust_agg['c']],\n",
    "    'rmse': [result_clean['rmse'], result_standard['rmse'], result_robust['rmse'], result_robust_agg['rmse']],\n",
    "    'r2': [result_clean['r2'], result_standard['r2'], result_robust['r2'], result_robust_agg['r2']],\n",
    "    'abs_error_a': [result_clean['abs_error_a'], result_standard['abs_error_a'], result_robust['abs_error_a'], result_robust_agg['abs_error_a']],\n",
    "    'abs_error_b': [result_clean['abs_error_b'], result_standard['abs_error_b'], result_robust['abs_error_b'], result_robust_agg['abs_error_b']],\n",
    "    'abs_error_c': [result_clean['abs_error_c'], result_standard['abs_error_c'], result_robust['abs_error_c'], result_robust_agg['abs_error_c']],\n",
    "    'outliers_detected': [0, 0, result_robust['n_outliers'], result_robust_agg['n_outliers']]\n",
    "}).to_csv('varying_ic_robust_fd_results.csv', index=False)\n",
    "\n",
    "print(\"\\n    Saved: varying_ic_robust_fd_results.csv\")\n",
    "print(\"\\n   VARYING IC ROBUST REGRESSION + FINITE DIFFERENCE SINDY COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b559be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Jupyter, after running the script:\n",
    "import pandas as pd\n",
    "df = pd.read_csv('varying_ic_baseline_fft_results.csv')\n",
    "print(df[['method', 'a', 'b', 'c']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e685a0",
   "metadata": {},
   "source": [
    "Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42e695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (20, 14)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 3B: VARYING INITIAL CONDITIONS - COMPREHENSIVE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ===== Load All Results =====\n",
    "print(\"\\n📂 Loading all varying IC results...\")\n",
    "\n",
    "try:\n",
    "    baseline_fft = pd.read_csv('varying_ic_baseline_fft_results.csv')\n",
    "    baseline_fd = pd.read_csv('varying_ic_baseline_fd_results.csv')\n",
    "    robust_fft = pd.read_csv('varying_ic_robust_fft_results.csv')\n",
    "    robust_fd = pd.read_csv('varying_ic_robust_fd_results.csv')\n",
    "    print(\"    Successfully loaded all 4 result files\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"    Error: {e}\")\n",
    "    print(\"Please run all 4 varying IC scripts first.\")\n",
    "    exit()\n",
    "\n",
    "# ===== Load Data for Visual Comparison =====\n",
    "print(\"\\n📂 Loading spatial data for visual comparison...\")\n",
    "\n",
    "with h5py.File(\"test_solving_euler_ks_clean.h5\", \"r\") as f:\n",
    "    u_clean = np.array(f[\"u\"])\n",
    "    x = np.array(f[\"x\"])\n",
    "    t = np.array(f[\"t\"])\n",
    "\n",
    "with h5py.File(\"test_solving_euler_ks_varied_ic.h5\", \"r\") as f:\n",
    "    u_varied = np.array(f[\"u\"])\n",
    "\n",
    "print(\"    Data loaded successfully\")\n",
    "\n",
    "# ===== Build Comparison DataFrame =====\n",
    "comparison_data = []\n",
    "\n",
    "# FFT Methods\n",
    "for idx, row in baseline_fft.iterrows():\n",
    "    if row['method'] != 'Clean Data':\n",
    "        comparison_data.append({\n",
    "            'Derivative': 'FFT', 'Treatment': 'No Treatment',\n",
    "            'a': row['a'], 'b': row['b'], 'c': row['c'],\n",
    "            'rmse': row['rmse'], 'r2': row['r2'],\n",
    "            'abs_error_a': row['abs_error_a'], 'abs_error_b': row['abs_error_b'], 'abs_error_c': row['abs_error_c']\n",
    "        })\n",
    "\n",
    "for idx, row in robust_fft.iterrows():\n",
    "    if row['method'] not in ['Clean Data', 'Standard Lasso']:\n",
    "        comparison_data.append({\n",
    "            'Derivative': 'FFT', 'Treatment': row['method'],\n",
    "            'a': row['a'], 'b': row['b'], 'c': row['c'],\n",
    "            'rmse': row['rmse'], 'r2': row['r2'],\n",
    "            'abs_error_a': row['abs_error_a'], 'abs_error_b': row['abs_error_b'], 'abs_error_c': row['abs_error_c']\n",
    "        })\n",
    "\n",
    "# Finite Difference Methods\n",
    "for idx, row in baseline_fd.iterrows():\n",
    "    if row['method'] != 'Clean Data':\n",
    "        comparison_data.append({\n",
    "            'Derivative': 'Finite Diff', 'Treatment': 'No Treatment',\n",
    "            'a': row['a'], 'b': row['b'], 'c': row['c'],\n",
    "            'rmse': row['rmse'], 'r2': row['r2'],\n",
    "            'abs_error_a': row['abs_error_a'], 'abs_error_b': row['abs_error_b'], 'abs_error_c': row['abs_error_c']\n",
    "        })\n",
    "\n",
    "for idx, row in robust_fd.iterrows():\n",
    "    if row['method'] not in ['Clean Data', 'Standard Lasso']:\n",
    "        comparison_data.append({\n",
    "            'Derivative': 'Finite Diff', 'Treatment': row['method'],\n",
    "            'a': row['a'], 'b': row['b'], 'c': row['c'],\n",
    "            'rmse': row['rmse'], 'r2': row['r2'],\n",
    "            'abs_error_a': row['abs_error_a'], 'abs_error_b': row['abs_error_b'], 'abs_error_c': row['abs_error_c']\n",
    "        })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "# ===== Extract Clean Data Baselines =====\n",
    "clean_fft = baseline_fft[baseline_fft['method'] == 'Clean Data'].iloc[0]\n",
    "clean_fd = baseline_fd[baseline_fd['method'] == 'Clean Data'].iloc[0]\n",
    "\n",
    "baseline_fft_notreat = baseline_fft[baseline_fft['method'] == 'Varying IC (No Treatment)'].iloc[0]\n",
    "baseline_fd_notreat = baseline_fd[baseline_fd['method'] == 'Varying IC (No Treatment)'].iloc[0]\n",
    "\n",
    "# ===== SUMMARY TABLES =====\n",
    "print(\"\\n\\n\" + \"=\"*100)\n",
    "print(\"COMPARISON TABLE 1: ALL METHODS\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\n{'Derivative':>12} {'Treatment':>20} {'a':>12} {'b':>12} {'c':>12} {'RMSE':>12} {'R²':>10}\")\n",
    "print(\"-\"*100)\n",
    "for _, row in df_comparison.iterrows():\n",
    "    print(f\"{row['Derivative']:>12} {row['Treatment']:>20} {row['a']:>12.6f} {row['b']:>12.6f} \"\n",
    "          f\"{row['c']:>12.6f} {row['rmse']:>12.6f} {row['r2']:>10.6f}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*100)\n",
    "print(\"COMPARISON TABLE 2: CLEAN DATA BASELINE\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\n{'Derivative':>12} {'a':>12} {'b':>12} {'c':>12} {'RMSE':>12} {'R²':>10}\")\n",
    "print(\"-\"*100)\n",
    "print(f\"{'FFT':>12} {clean_fft['a']:>12.6f} {clean_fft['b']:>12.6f} {clean_fft['c']:>12.6f} \"\n",
    "      f\"{clean_fft['rmse']:>12.6f} {clean_fft['r2']:>10.6f}\")\n",
    "print(f\"{'Finite Diff':>12} {clean_fd['a']:>12.6f} {clean_fd['b']:>12.6f} {clean_fd['c']:>12.6f} \"\n",
    "      f\"{clean_fd['rmse']:>12.6f} {clean_fd['r2']:>10.6f}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# ===== BEST METHOD ANALYSIS =====\n",
    "df_comparison['mean_coef_error'] = (df_comparison['abs_error_a'] + \n",
    "                                     df_comparison['abs_error_b'] + \n",
    "                                     df_comparison['abs_error_c']) / 3\n",
    "\n",
    "best_rmse = df_comparison.loc[df_comparison['rmse'].idxmin()]\n",
    "best_r2 = df_comparison.loc[df_comparison['r2'].idxmax()]\n",
    "best_coef = df_comparison.loc[df_comparison['mean_coef_error'].idxmin()]\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"BEST METHODS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n🏆 BEST RMSE: {best_rmse['Derivative']} + {best_rmse['Treatment']}\")\n",
    "print(f\"   RMSE={best_rmse['rmse']:.6f}, R²={best_rmse['r2']:.6f}\")\n",
    "print(f\"\\n🏆 BEST R²: {best_r2['Derivative']} + {best_r2['Treatment']}\")\n",
    "print(f\"   RMSE={best_r2['rmse']:.6f}, R²={best_r2['r2']:.6f}\")\n",
    "print(f\"\\n🏆 BEST COEFFICIENT ACCURACY: {best_coef['Derivative']} + {best_coef['Treatment']}\")\n",
    "print(f\"   Mean Error={best_coef['mean_coef_error']:.6f}\")\n",
    "\n",
    "# ===== VISUALIZATION 1: Main Comparison =====\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"GENERATING VISUALIZATIONS...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig1 = plt.figure(figsize=(20, 14))\n",
    "\n",
    "n_frames = 500\n",
    "sample_idx = len(u_clean) // 2\n",
    "\n",
    "# Row 1: Spatiotemporal Comparison\n",
    "ax1 = plt.subplot(3, 4, 1)\n",
    "im1 = plt.imshow(u_clean[:n_frames].T, cmap='RdBu', aspect='auto', origin='lower',\n",
    "                 extent=(0, n_frames*t[1], x[0], x[-1]), vmin=-2, vmax=2)\n",
    "plt.xlabel('Time', fontsize=10)\n",
    "plt.ylabel('Space (x)', fontsize=10)\n",
    "plt.title('Clean Data\\n(Single Trajectory)', fontweight='bold', fontsize=11)\n",
    "plt.colorbar(im1, ax=ax1, label='u(x,t)')\n",
    "\n",
    "ax2 = plt.subplot(3, 4, 2)\n",
    "im2 = plt.imshow(u_varied[:n_frames].T, cmap='RdBu', aspect='auto', origin='lower',\n",
    "                 extent=(0, n_frames*t[1], x[0], x[-1]), vmin=-2, vmax=2)\n",
    "plt.xlabel('Time', fontsize=10)\n",
    "plt.ylabel('Space (x)', fontsize=10)\n",
    "plt.title('Varying IC (No Treatment)\\n(Ensemble of Trajectories)', fontweight='bold', fontsize=11)\n",
    "plt.colorbar(im2, ax=ax2, label='u(x,t)')\n",
    "\n",
    "# Row 1: Sample Frame Comparison\n",
    "ax3 = plt.subplot(3, 4, 3)\n",
    "plt.plot(x, u_clean[sample_idx], 'g-', label='Clean', linewidth=2.5, alpha=0.8)\n",
    "plt.plot(x, u_varied[sample_idx], 'r--', label='Varying IC', linewidth=2, alpha=0.7)\n",
    "plt.xlabel('x', fontsize=10)\n",
    "plt.ylabel('u(x)', fontsize=10)\n",
    "plt.title(f'Sample Frame Comparison\\n(Frame {sample_idx})', fontweight='bold', fontsize=11)\n",
    "plt.legend(fontsize=9)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Row 1: Trajectory Divergence\n",
    "ax4 = plt.subplot(3, 4, 4)\n",
    "mean_traj = np.mean(u_varied, axis=0)\n",
    "divergence = np.sqrt(np.mean((u_varied - mean_traj)**2, axis=1))\n",
    "plt.plot(t, divergence, 'b-', linewidth=2)\n",
    "plt.xlabel('Time', fontsize=10)\n",
    "plt.ylabel('Trajectory Divergence', fontsize=10)\n",
    "plt.title('Trajectory Divergence from Mean\\n(Shows IC Uncertainty)', fontweight='bold', fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Row 2: Coefficient Recovery - FFT\n",
    "ax5 = plt.subplot(3, 4, 5)\n",
    "methods_fft = ['No Treat', 'Robust\\n(ε=1.35)', 'Robust\\n(ε=1.0)']\n",
    "a_vals_fft = [\n",
    "    baseline_fft_notreat['a'],\n",
    "    robust_fft[robust_fft['method']=='Robust (ε=1.35)'].iloc[0]['a'],\n",
    "    robust_fft[robust_fft['method']=='Robust (ε=1.0)'].iloc[0]['a']\n",
    "]\n",
    "b_vals_fft = [\n",
    "    baseline_fft_notreat['b'],\n",
    "    robust_fft[robust_fft['method']=='Robust (ε=1.35)'].iloc[0]['b'],\n",
    "    robust_fft[robust_fft['method']=='Robust (ε=1.0)'].iloc[0]['b']\n",
    "]\n",
    "c_vals_fft = [\n",
    "    baseline_fft_notreat['c'],\n",
    "    robust_fft[robust_fft['method']=='Robust (ε=1.35)'].iloc[0]['c'],\n",
    "    robust_fft[robust_fft['method']=='Robust (ε=1.0)'].iloc[0]['c']\n",
    "]\n",
    "x_pos = np.arange(len(methods_fft))\n",
    "width = 0.25\n",
    "plt.bar(x_pos - width, a_vals_fft, width, label='a (u_xx)', alpha=0.8)\n",
    "plt.bar(x_pos, b_vals_fft, width, label='b (u_xxxx)', alpha=0.8)\n",
    "plt.bar(x_pos + width, c_vals_fft, width, label='c (u*u_x)', alpha=0.8)\n",
    "plt.axhline(y=-1.0, color='red', linestyle='--', linewidth=2, label='True')\n",
    "plt.ylabel('Coefficient Value', fontsize=10)\n",
    "plt.title('FFT: Coefficient Recovery', fontweight='bold', fontsize=11)\n",
    "plt.xticks(x_pos, methods_fft, fontsize=8)\n",
    "plt.legend(fontsize=8)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Row 2: Coefficient Recovery - FD\n",
    "ax6 = plt.subplot(3, 4, 6)\n",
    "a_vals_fd = [\n",
    "    baseline_fd_notreat['a'],\n",
    "    robust_fd[robust_fd['method']=='Robust (ε=1.35)'].iloc[0]['a'],\n",
    "    robust_fd[robust_fd['method']=='Robust (ε=1.0)'].iloc[0]['a']\n",
    "]\n",
    "b_vals_fd = [\n",
    "    baseline_fd_notreat['b'],\n",
    "    robust_fd[robust_fd['method']=='Robust (ε=1.35)'].iloc[0]['b'],\n",
    "    robust_fd[robust_fd['method']=='Robust (ε=1.0)'].iloc[0]['b']\n",
    "]\n",
    "c_vals_fd = [\n",
    "    baseline_fd_notreat['c'],\n",
    "    robust_fd[robust_fd['method']=='Robust (ε=1.35)'].iloc[0]['c'],\n",
    "    robust_fd[robust_fd['method']=='Robust (ε=1.0)'].iloc[0]['c']\n",
    "]\n",
    "plt.bar(x_pos - width, a_vals_fd, width, label='a (u_xx)', alpha=0.8)\n",
    "plt.bar(x_pos, b_vals_fd, width, label='b (u_xxxx)', alpha=0.8)\n",
    "plt.bar(x_pos + width, c_vals_fd, width, label='c ((u²)_x)', alpha=0.8)\n",
    "plt.axhline(y=-1.0, color='red', linestyle='--', linewidth=2)\n",
    "plt.axhline(y=-0.5, color='orange', linestyle='--', linewidth=2, label='True (c)')\n",
    "plt.ylabel('Coefficient Value', fontsize=10)\n",
    "plt.title('Finite Diff: Coefficient Recovery', fontweight='bold', fontsize=11)\n",
    "plt.xticks(x_pos, methods_fft, fontsize=8)\n",
    "plt.legend(fontsize=8)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Row 2: RMSE Comparison\n",
    "ax7 = plt.subplot(3, 4, 7)\n",
    "rmse_fft = [\n",
    "    baseline_fft_notreat['rmse'],\n",
    "    robust_fft[robust_fft['method']=='Robust (ε=1.35)'].iloc[0]['rmse'],\n",
    "    robust_fft[robust_fft['method']=='Robust (ε=1.0)'].iloc[0]['rmse']\n",
    "]\n",
    "rmse_fd = [\n",
    "    baseline_fd_notreat['rmse'],\n",
    "    robust_fd[robust_fd['method']=='Robust (ε=1.35)'].iloc[0]['rmse'],\n",
    "    robust_fd[robust_fd['method']=='Robust (ε=1.0)'].iloc[0]['rmse']\n",
    "]\n",
    "width2 = 0.35\n",
    "plt.bar(x_pos - width2/2, rmse_fft, width2, label='FFT', alpha=0.8, color='blue')\n",
    "plt.bar(x_pos + width2/2, rmse_fd, width2, label='Finite Diff', alpha=0.8, color='green')\n",
    "plt.axhline(y=clean_fft['rmse'], color='blue', linestyle='--', linewidth=1, alpha=0.5)\n",
    "plt.axhline(y=clean_fd['rmse'], color='green', linestyle='--', linewidth=1, alpha=0.5)\n",
    "plt.ylabel('RMSE', fontsize=10)\n",
    "plt.title('RMSE Comparison\\n(Dashed = Clean)', fontweight='bold', fontsize=11)\n",
    "plt.xticks(x_pos, methods_fft, fontsize=8)\n",
    "plt.legend(fontsize=8)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.yscale('log')\n",
    "\n",
    "# Row 2: R² Comparison\n",
    "ax8 = plt.subplot(3, 4, 8)\n",
    "r2_fft = [\n",
    "    baseline_fft_notreat['r2'],\n",
    "    robust_fft[robust_fft['method']=='Robust (ε=1.35)'].iloc[0]['r2'],\n",
    "    robust_fft[robust_fft['method']=='Robust (ε=1.0)'].iloc[0]['r2']\n",
    "]\n",
    "r2_fd = [\n",
    "    baseline_fd_notreat['r2'],\n",
    "    robust_fd[robust_fd['method']=='Robust (ε=1.35)'].iloc[0]['r2'],\n",
    "    robust_fd[robust_fd['method']=='Robust (ε=1.0)'].iloc[0]['r2']\n",
    "]\n",
    "plt.bar(x_pos - width2/2, r2_fft, width2, label='FFT', alpha=0.8, color='blue')\n",
    "plt.bar(x_pos + width2/2, r2_fd, width2, label='Finite Diff', alpha=0.8, color='green')\n",
    "plt.axhline(y=clean_fft['r2'], color='blue', linestyle='--', linewidth=1, alpha=0.5)\n",
    "plt.axhline(y=clean_fd['r2'], color='green', linestyle='--', linewidth=1, alpha=0.5)\n",
    "plt.axhline(y=1.0, color='black', linestyle=':', linewidth=1)\n",
    "plt.ylabel('R² Score', fontsize=10)\n",
    "plt.title('R² Comparison\\n(Dashed = Clean)', fontweight='bold', fontsize=11)\n",
    "plt.xticks(x_pos, methods_fft, fontsize=8)\n",
    "plt.legend(fontsize=8)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Row 3: Absolute Errors - FFT\n",
    "ax9 = plt.subplot(3, 4, 9)\n",
    "err_a_fft = [baseline_fft_notreat['abs_error_a'],\n",
    "             robust_fft[robust_fft['method']=='Robust (ε=1.35)'].iloc[0]['abs_error_a'],\n",
    "             robust_fft[robust_fft['method']=='Robust (ε=1.0)'].iloc[0]['abs_error_a']]\n",
    "err_b_fft = [baseline_fft_notreat['abs_error_b'],\n",
    "             robust_fft[robust_fft['method']=='Robust (ε=1.35)'].iloc[0]['abs_error_b'],\n",
    "             robust_fft[robust_fft['method']=='Robust (ε=1.0)'].iloc[0]['abs_error_b']]\n",
    "err_c_fft = [baseline_fft_notreat['abs_error_c'],\n",
    "             robust_fft[robust_fft['method']=='Robust (ε=1.35)'].iloc[0]['abs_error_c'],\n",
    "             robust_fft[robust_fft['method']=='Robust (ε=1.0)'].iloc[0]['abs_error_c']]\n",
    "plt.bar(x_pos - width, err_a_fft, width, label='|Err a|', alpha=0.8)\n",
    "plt.bar(x_pos, err_b_fft, width, label='|Err b|', alpha=0.8)\n",
    "plt.bar(x_pos + width, err_c_fft, width, label='|Err c|', alpha=0.8)\n",
    "plt.ylabel('Absolute Error', fontsize=10)\n",
    "plt.title('FFT: Absolute Coefficient Errors', fontweight='bold', fontsize=11)\n",
    "plt.xticks(x_pos, methods_fft, fontsize=8)\n",
    "plt.legend(fontsize=8)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.yscale('log')\n",
    "\n",
    "# Row 3: Absolute Errors - FD\n",
    "ax10 = plt.subplot(3, 4, 10)\n",
    "err_a_fd = [baseline_fd_notreat['abs_error_a'],\n",
    "            robust_fd[robust_fd['method']=='Robust (ε=1.35)'].iloc[0]['abs_error_a'],\n",
    "            robust_fd[robust_fd['method']=='Robust (ε=1.0)'].iloc[0]['abs_error_a']]\n",
    "err_b_fd = [baseline_fd_notreat['abs_error_b'],\n",
    "            robust_fd[robust_fd['method']=='Robust (ε=1.35)'].iloc[0]['abs_error_b'],\n",
    "            robust_fd[robust_fd['method']=='Robust (ε=1.0)'].iloc[0]['abs_error_b']]\n",
    "err_c_fd = [baseline_fd_notreat['abs_error_c'],\n",
    "            robust_fd[robust_fd['method']=='Robust (ε=1.35)'].iloc[0]['abs_error_c'],\n",
    "            robust_fd[robust_fd['method']=='Robust (ε=1.0)'].iloc[0]['abs_error_c']]\n",
    "plt.bar(x_pos - width, err_a_fd, width, label='|Err a|', alpha=0.8)\n",
    "plt.bar(x_pos, err_b_fd, width, label='|Err b|', alpha=0.8)\n",
    "plt.bar(x_pos + width, err_c_fd, width, label='|Err c|', alpha=0.8)\n",
    "plt.ylabel('Absolute Error', fontsize=10)\n",
    "plt.title('Finite Diff: Absolute Coefficient Errors', fontweight='bold', fontsize=11)\n",
    "plt.xticks(x_pos, methods_fft, fontsize=8)\n",
    "plt.legend(fontsize=8)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.yscale('log')\n",
    "\n",
    "# Row 3: Outliers Detected - FFT\n",
    "ax11 = plt.subplot(3, 4, 11)\n",
    "if 'outliers_detected' in robust_fft.columns:\n",
    "    outliers_fft = [0,\n",
    "                    robust_fft[robust_fft['method']=='Robust (ε=1.35)'].iloc[0]['outliers_detected'],\n",
    "                    robust_fft[robust_fft['method']=='Robust (ε=1.0)'].iloc[0]['outliers_detected']]\n",
    "    colors = ['red', 'blue', 'green']\n",
    "    plt.bar(x_pos, outliers_fft, color=colors, alpha=0.7, edgecolor='black')\n",
    "    plt.ylabel('Number of Outliers', fontsize=10)\n",
    "    plt.title('FFT: Outliers Detected', fontweight='bold', fontsize=11)\n",
    "    plt.xticks(x_pos, methods_fft, fontsize=8)\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Row 3: Outliers Detected - FD\n",
    "ax12 = plt.subplot(3, 4, 12)\n",
    "if 'outliers_detected' in robust_fd.columns:\n",
    "    outliers_fd = [0,\n",
    "                   robust_fd[robust_fd['method']=='Robust (ε=1.35)'].iloc[0]['outliers_detected'],\n",
    "                   robust_fd[robust_fd['method']=='Robust (ε=1.0)'].iloc[0]['outliers_detected']]\n",
    "    plt.bar(x_pos, outliers_fd, color=colors, alpha=0.7, edgecolor='black')\n",
    "    plt.ylabel('Number of Outliers', fontsize=10)\n",
    "    plt.title('Finite Diff: Outliers Detected', fontweight='bold', fontsize=11)\n",
    "    plt.xticks(x_pos, methods_fft, fontsize=8)\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('varying_ic_visual_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"    Saved: varying_ic_visual_comparison.png\")\n",
    "\n",
    "# ===== VISUALIZATION 2: Heatmaps =====\n",
    "fig2, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "methods_clean = ['No Treat', 'Robust (ε=1.35)', 'Robust (ε=1.0)']\n",
    "derivatives = ['FFT', 'Finite Diff']\n",
    "\n",
    "# RMSE Heatmap\n",
    "rmse_matrix = np.array([\n",
    "    rmse_fft,\n",
    "    rmse_fd\n",
    "])\n",
    "sns.heatmap(rmse_matrix, annot=True, fmt='.4f', cmap='RdYlGn_r',\n",
    "            xticklabels=methods_clean, yticklabels=derivatives, ax=axes[0,0],\n",
    "            cbar_kws={'label': 'RMSE'})\n",
    "axes[0,0].set_title('RMSE by Method', fontweight='bold', fontsize=12)\n",
    "axes[0,0].set_xlabel('Treatment Method')\n",
    "axes[0,0].set_ylabel('Derivative Type')\n",
    "\n",
    "# R² Heatmap\n",
    "r2_matrix = np.array([\n",
    "    r2_fft,\n",
    "    r2_fd\n",
    "])\n",
    "sns.heatmap(r2_matrix, annot=True, fmt='.4f', cmap='RdYlGn',\n",
    "            xticklabels=methods_clean, yticklabels=derivatives, ax=axes[0,1],\n",
    "            cbar_kws={'label': 'R² Score'})\n",
    "axes[0,1].set_title('R² Score by Method', fontweight='bold', fontsize=12)\n",
    "axes[0,1].set_xlabel('Treatment Method')\n",
    "axes[0,1].set_ylabel('Derivative Type')\n",
    "\n",
    "# Coefficient Error a Heatmap\n",
    "err_a_matrix = np.array([\n",
    "    err_a_fft,\n",
    "    err_a_fd\n",
    "])\n",
    "sns.heatmap(err_a_matrix, annot=True, fmt='.4f', cmap='RdYlGn_r',\n",
    "            xticklabels=methods_clean, yticklabels=derivatives, ax=axes[1,0],\n",
    "            cbar_kws={'label': '|Error in a|'})\n",
    "axes[1,0].set_title('Absolute Error in Coefficient a', fontweight='bold', fontsize=12)\n",
    "axes[1,0].set_xlabel('Treatment Method')\n",
    "axes[1,0].set_ylabel('Derivative Type')\n",
    "\n",
    "# Coefficient Error b Heatmap\n",
    "err_b_matrix = np.array([\n",
    "    err_b_fft,\n",
    "    err_b_fd\n",
    "])\n",
    "sns.heatmap(err_b_matrix, annot=True, fmt='.4f', cmap='RdYlGn_r',\n",
    "            xticklabels=methods_clean, yticklabels=derivatives, ax=axes[1,1],\n",
    "            cbar_kws={'label': '|Error in b|'})\n",
    "axes[1,1].set_title('Absolute Error in Coefficient b', fontweight='bold', fontsize=12)\n",
    "axes[1,1].set_xlabel('Treatment Method')\n",
    "axes[1,1].set_ylabel('Derivative Type')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('varying_ic_heatmap_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"    Saved: varying_ic_heatmap_comparison.png\")\n",
    "\n",
    "# ===== Save Comparison Data =====\n",
    "df_comparison.to_csv('varying_ic_all_methods_comparison.csv', index=False)\n",
    "print(\"    Saved: varying_ic_all_methods_comparison.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"   VARYING IC COMPREHENSIVE COMPARISON COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  1. varying_ic_visual_comparison.png - Visual data & performance comparison (12 subplots)\")\n",
    "print(\"  2. varying_ic_heatmap_comparison.png - Performance heatmaps (4 heatmaps)\")\n",
    "print(\"  3. varying_ic_all_methods_comparison.csv - Complete comparison data\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8d3251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.fft import fft, ifft, fftfreq\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (20, 14)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 3B: VARYING INITIAL CONDITIONS - ENHANCED COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ===== Load All Results =====\n",
    "print(\"\\n📂 Loading all varying IC results...\")\n",
    "\n",
    "try:\n",
    "    baseline_fft = pd.read_csv('varying_ic_baseline_fft_results.csv')\n",
    "    baseline_fd = pd.read_csv('varying_ic_baseline_fd_results.csv')\n",
    "    robust_fft = pd.read_csv('varying_ic_robust_fft_results.csv')\n",
    "    robust_fd = pd.read_csv('varying_ic_robust_fd_results.csv')\n",
    "    print(\"    Successfully loaded all 4 result files\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\" Error: {e}\")\n",
    "    print(\"Please run all 4 varying IC scripts first.\")\n",
    "    exit()\n",
    "\n",
    "# ===== Load Data for Visual Comparison =====\n",
    "print(\"\\n📂 Loading spatial data for outlier detection...\")\n",
    "\n",
    "with h5py.File(\"test_solving_euler_ks_clean.h5\", \"r\") as f:\n",
    "    u_clean = np.array(f[\"u\"])\n",
    "    x = np.array(f[\"x\"])\n",
    "    t = np.array(f[\"t\"])\n",
    "\n",
    "with h5py.File(\"test_solving_euler_ks_varied_ic.h5\", \"r\") as f:\n",
    "    u_varied = np.array(f[\"u\"])\n",
    "\n",
    "print(\"    Data loaded successfully\")\n",
    "\n",
    "# Print data information\n",
    "dt = t[1] - t[0]\n",
    "n_frames_viz = 500\n",
    "time_duration = n_frames_viz * dt\n",
    "sample_idx = len(u_clean) // 2\n",
    "sample_time = t[sample_idx]\n",
    "\n",
    "print(f\"\\n📊 Data Information:\")\n",
    "print(f\"  Total time steps: {len(t)}\")\n",
    "print(f\"  Time step size (dt): {dt:.6f}\")\n",
    "print(f\"  Spatiotemporal plots show: {n_frames_viz} frames (Time: 0 to {time_duration:.3f} units)\")\n",
    "print(f\"  Sample frame index: {sample_idx} (Time: {sample_time:.3f} units)\")\n",
    "print(f\"  Spatial domain: x ∈ [{x[0]:.2f}, {x[-1]:.2f}]\")\n",
    "\n",
    "# ===== Detect Outliers using Robust Regression =====\n",
    "print(\"\\n🔍 Computing outlier detection maps...\")\n",
    "\n",
    "def detect_outliers_fft(u, x, t, epsilon=1.35):\n",
    "    \"\"\"Detect outliers using Huber regression with FFT derivatives\"\"\"\n",
    "    Nx = len(x)\n",
    "    dx = x[1] - x[0]\n",
    "    dt = t[1] - t[0]\n",
    "    k = 2 * np.pi * fftfreq(Nx, d=dx)\n",
    "    \n",
    "    def spectral_derivative(u_snapshot, k):\n",
    "        u_hat = fft(u_snapshot)\n",
    "        du_hat = 1j * k * u_hat\n",
    "        return np.real(ifft(du_hat))\n",
    "    \n",
    "    def spectral_second_derivative(u_snapshot, k):\n",
    "        u_hat = fft(u_snapshot)\n",
    "        d2u_hat = -(k**2) * u_hat\n",
    "        return np.real(ifft(d2u_hat))\n",
    "    \n",
    "    def spectral_fourth_derivative(u_snapshot, k):\n",
    "        u_hat = fft(u_snapshot)\n",
    "        d4u_hat = (k**4) * u_hat\n",
    "        return np.real(ifft(d4u_hat))\n",
    "    \n",
    "    u_t = (u[2:] - u[:-2]) / (2 * dt)\n",
    "    u_mid = u[1:-1]\n",
    "    \n",
    "    Theta = []\n",
    "    ut_flat = []\n",
    "    \n",
    "    for snapshot, ut_snapshot in zip(u_mid, u_t):\n",
    "        ux = spectral_derivative(snapshot, k)\n",
    "        uxx = spectral_second_derivative(snapshot, k)\n",
    "        uxxxx = spectral_fourth_derivative(snapshot, k)\n",
    "        Theta_snapshot = np.vstack([uxx, uxxxx, snapshot * ux]).T\n",
    "        Theta.append(Theta_snapshot)\n",
    "        ut_flat.append(ut_snapshot)\n",
    "    \n",
    "    Theta = np.vstack(Theta)\n",
    "    ut_flat = np.hstack(ut_flat)\n",
    "    \n",
    "    Theta_mean = Theta.mean(axis=0)\n",
    "    Theta_std = Theta.std(axis=0)\n",
    "    Theta_norm = (Theta - Theta_mean) / Theta_std\n",
    "    \n",
    "    huber = HuberRegressor(epsilon=epsilon, fit_intercept=False, max_iter=1000, alpha=1e-4)\n",
    "    huber.fit(Theta_norm, ut_flat)\n",
    "    \n",
    "    residuals = ut_flat - huber.predict(Theta_norm)\n",
    "    threshold = epsilon * np.median(np.abs(residuals))\n",
    "    \n",
    "    # Reshape residuals back to (time, space)\n",
    "    n_time = len(u_mid)\n",
    "    n_space = Nx\n",
    "    residuals_2d = residuals.reshape(n_time, n_space)\n",
    "    \n",
    "    # Mark outliers (per frame)\n",
    "    outlier_map = np.abs(residuals_2d) > threshold\n",
    "    \n",
    "    return outlier_map, residuals_2d\n",
    "\n",
    "print(\"  Computing outliers for ε=1.35...\")\n",
    "outlier_map_135, residuals_135 = detect_outliers_fft(u_varied, x, t, epsilon=1.35)\n",
    "\n",
    "print(\"  Computing outliers for ε=1.0...\")\n",
    "outlier_map_10, residuals_10 = detect_outliers_fft(u_varied, x, t, epsilon=1.0)\n",
    "\n",
    "print(\"    Outlier detection complete\")\n",
    "\n",
    "# ===== Build Comparison DataFrame =====\n",
    "comparison_data = []\n",
    "\n",
    "# FFT Methods\n",
    "for idx, row in baseline_fft.iterrows():\n",
    "    comparison_data.append({\n",
    "        'Derivative': 'FFT', 'Treatment': 'Clean Data' if row['method'] == 'Clean Data' else row['method'],\n",
    "        'a': row['a'], 'b': row['b'], 'c': row['c'],\n",
    "        'rmse': row['rmse'], 'r2': row['r2'],\n",
    "        'abs_error_a': row['abs_error_a'], 'abs_error_b': row['abs_error_b'], 'abs_error_c': row['abs_error_c']\n",
    "    })\n",
    "\n",
    "for idx, row in robust_fft.iterrows():\n",
    "    if row['method'] not in ['Standard Lasso']:\n",
    "        comparison_data.append({\n",
    "            'Derivative': 'FFT', 'Treatment': 'Clean Data' if row['method'] == 'Clean Data' else row['method'],\n",
    "            'a': row['a'], 'b': row['b'], 'c': row['c'],\n",
    "            'rmse': row['rmse'], 'r2': row['r2'],\n",
    "            'abs_error_a': row['abs_error_a'], 'abs_error_b': row['abs_error_b'], 'abs_error_c': row['abs_error_c']\n",
    "        })\n",
    "\n",
    "# Finite Difference Methods\n",
    "for idx, row in baseline_fd.iterrows():\n",
    "    comparison_data.append({\n",
    "        'Derivative': 'Finite Diff', 'Treatment': 'Clean Data' if row['method'] == 'Clean Data' else row['method'],\n",
    "        'a': row['a'], 'b': row['b'], 'c': row['c'],\n",
    "        'rmse': row['rmse'], 'r2': row['r2'],\n",
    "        'abs_error_a': row['abs_error_a'], 'abs_error_b': row['abs_error_b'], 'abs_error_c': row['abs_error_c']\n",
    "    })\n",
    "\n",
    "for idx, row in robust_fd.iterrows():\n",
    "    if row['method'] not in ['Standard Lasso']:\n",
    "        comparison_data.append({\n",
    "            'Derivative': 'Finite Diff', 'Treatment': 'Clean Data' if row['method'] == 'Clean Data' else row['method'],\n",
    "            'a': row['a'], 'b': row['b'], 'c': row['c'],\n",
    "            'rmse': row['rmse'], 'r2': row['r2'],\n",
    "            'abs_error_a': row['abs_error_a'], 'abs_error_b': row['abs_error_b'], 'abs_error_c': row['abs_error_c']\n",
    "        })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "# ===== Extract Data for Plotting =====\n",
    "clean_fft = baseline_fft[baseline_fft['method'] == 'Clean Data'].iloc[0]\n",
    "clean_fd = baseline_fd[baseline_fd['method'] == 'Clean Data'].iloc[0]\n",
    "baseline_fft_notreat = baseline_fft[baseline_fft['method'] == 'Varying IC (No Treatment)'].iloc[0]\n",
    "baseline_fd_notreat = baseline_fd[baseline_fd['method'] == 'Varying IC (No Treatment)'].iloc[0]\n",
    "robust_fft_135 = robust_fft[robust_fft['method'] == 'Robust (ε=1.35)'].iloc[0]\n",
    "robust_fft_10 = robust_fft[robust_fft['method'] == 'Robust (ε=1.0)'].iloc[0]\n",
    "robust_fd_135 = robust_fd[robust_fd['method'] == 'Robust (ε=1.35)'].iloc[0]\n",
    "robust_fd_10 = robust_fd[robust_fd['method'] == 'Robust (ε=1.0)'].iloc[0]\n",
    "\n",
    "# ===== SUMMARY TABLES =====\n",
    "print(\"\\n\\n\" + \"=\"*100)\n",
    "print(\"COMPARISON TABLE 1: ALL METHODS (INCLUDING CLEAN DATA)\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\n{'Derivative':>12} {'Treatment':>25} {'a':>12} {'b':>12} {'c':>12} {'RMSE':>12} {'R²':>10}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "# Sort to show clean first\n",
    "df_sorted = df_comparison.copy()\n",
    "df_sorted['sort_key'] = df_sorted['Treatment'].map({\n",
    "    'Clean Data': 0,\n",
    "    'Varying IC (No Treatment)': 1,\n",
    "    'Robust (ε=1.35)': 2,\n",
    "    'Robust (ε=1.0)': 3\n",
    "})\n",
    "df_sorted = df_sorted.sort_values(['Derivative', 'sort_key'])\n",
    "\n",
    "for _, row in df_sorted.iterrows():\n",
    "    print(f\"{row['Derivative']:>12} {row['Treatment']:>25} {row['a']:>12.6f} {row['b']:>12.6f} \"\n",
    "          f\"{row['c']:>12.6f} {row['rmse']:>12.6f} {row['r2']:>10.6f}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# ===== VISUALIZATION 1: Main Comparison with Outlier Maps =====\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"GENERATING VISUALIZATIONS...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig1 = plt.figure(figsize=(22, 14))\n",
    "\n",
    "# Row 1: Spatiotemporal Comparison (4 plots)\n",
    "ax1 = plt.subplot(3, 4, 1)\n",
    "im1 = plt.imshow(u_clean[:n_frames_viz].T, cmap='RdBu', aspect='auto', origin='lower',\n",
    "                 extent=(0, time_duration, x[0], x[-1]), vmin=-2, vmax=2)\n",
    "plt.xlabel('Time (units)', fontsize=10)\n",
    "plt.ylabel('Space (x)', fontsize=10)\n",
    "plt.title(f'Clean Data (Single Trajectory)\\nTime: 0-{time_duration:.1f} units ({n_frames_viz} frames)', \n",
    "          fontweight='bold', fontsize=10)\n",
    "plt.colorbar(im1, ax=ax1, label='u(x,t)')\n",
    "\n",
    "ax2 = plt.subplot(3, 4, 2)\n",
    "im2 = plt.imshow(u_varied[:n_frames_viz].T, cmap='RdBu', aspect='auto', origin='lower',\n",
    "                 extent=(0, time_duration, x[0], x[-1]), vmin=-2, vmax=2)\n",
    "plt.xlabel('Time (units)', fontsize=10)\n",
    "plt.ylabel('Space (x)', fontsize=10)\n",
    "plt.title(f'Varying IC - No Treatment\\nEnsemble of Trajectories', \n",
    "          fontweight='bold', fontsize=10)\n",
    "plt.colorbar(im2, ax=ax2, label='u(x,t)')\n",
    "\n",
    "# Adjust outlier maps to match visualization frames (1:-1 due to time derivative)\n",
    "outlier_viz_135 = outlier_map_135[:min(n_frames_viz-2, outlier_map_135.shape[0])]\n",
    "outlier_viz_10 = outlier_map_10[:min(n_frames_viz-2, outlier_map_10.shape[0])]\n",
    "\n",
    "ax3 = plt.subplot(3, 4, 3)\n",
    "im3 = plt.imshow(outlier_viz_135.T, cmap='RdYlGn_r', aspect='auto', origin='lower',\n",
    "                 extent=(0, outlier_viz_135.shape[0]*dt, x[0], x[-1]), vmin=0, vmax=1)\n",
    "plt.xlabel('Time (units)', fontsize=10)\n",
    "plt.ylabel('Space (x)', fontsize=10)\n",
    "outlier_pct_135 = np.sum(outlier_viz_135) / outlier_viz_135.size * 100\n",
    "plt.title(f'Outlier Map (ε=1.35)\\n{outlier_pct_135:.1f}% outliers detected', \n",
    "          fontweight='bold', fontsize=10)\n",
    "cbar3 = plt.colorbar(im3, ax=ax3)\n",
    "cbar3.set_ticks([0, 1])\n",
    "cbar3.set_ticklabels(['Normal', 'Outlier'])\n",
    "\n",
    "ax4 = plt.subplot(3, 4, 4)\n",
    "im4 = plt.imshow(outlier_viz_10.T, cmap='RdYlGn_r', aspect='auto', origin='lower',\n",
    "                 extent=(0, outlier_viz_10.shape[0]*dt, x[0], x[-1]), vmin=0, vmax=1)\n",
    "plt.xlabel('Time (units)', fontsize=10)\n",
    "plt.ylabel('Space (x)', fontsize=10)\n",
    "outlier_pct_10 = np.sum(outlier_viz_10) / outlier_viz_10.size * 100\n",
    "plt.title(f'Outlier Map (ε=1.0)\\n{outlier_pct_10:.1f}% outliers detected (More Aggressive)', \n",
    "          fontweight='bold', fontsize=10)\n",
    "cbar4 = plt.colorbar(im4, ax=ax4)\n",
    "cbar4.set_ticks([0, 1])\n",
    "cbar4.set_ticklabels(['Normal', 'Outlier'])\n",
    "\n",
    "# Row 2: Sample Frame and Trajectory Divergence\n",
    "ax5 = plt.subplot(3, 4, 5)\n",
    "plt.plot(x, u_clean[sample_idx], 'g-', label='Clean', linewidth=2.5, alpha=0.8)\n",
    "plt.plot(x, u_varied[sample_idx], 'r--', label='Varying IC', linewidth=2, alpha=0.7)\n",
    "plt.xlabel('x', fontsize=10)\n",
    "plt.ylabel('u(x)', fontsize=10)\n",
    "plt.title(f'Sample Frame at t={sample_time:.1f}\\n(Frame {sample_idx})', fontweight='bold', fontsize=10)\n",
    "plt.legend(fontsize=9)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "ax6 = plt.subplot(3, 4, 6)\n",
    "mean_traj = np.mean(u_varied, axis=0)\n",
    "divergence = np.sqrt(np.mean((u_varied - mean_traj)**2, axis=1))\n",
    "plt.plot(t, divergence, 'b-', linewidth=2)\n",
    "plt.xlabel('Time (units)', fontsize=10)\n",
    "plt.ylabel('Divergence', fontsize=10)\n",
    "plt.title('Trajectory Divergence from Mean\\n(Quantifies IC Uncertainty)', fontweight='bold', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Row 2: Coefficient Recovery - FFT\n",
    "ax7 = plt.subplot(3, 4, 7)\n",
    "methods_fft = ['Clean', 'No Treat', 'ε=1.35', 'ε=1.0']\n",
    "a_vals_fft = [clean_fft['a'], baseline_fft_notreat['a'], robust_fft_135['a'], robust_fft_10['a']]\n",
    "b_vals_fft = [clean_fft['b'], baseline_fft_notreat['b'], robust_fft_135['b'], robust_fft_10['b']]\n",
    "c_vals_fft = [clean_fft['c'], baseline_fft_notreat['c'], robust_fft_135['c'], robust_fft_10['c']]\n",
    "x_pos = np.arange(len(methods_fft))\n",
    "width = 0.25\n",
    "plt.bar(x_pos - width, a_vals_fft, width, label='a (u_xx)', alpha=0.8)\n",
    "plt.bar(x_pos, b_vals_fft, width, label='b (u_xxxx)', alpha=0.8)\n",
    "plt.bar(x_pos + width, c_vals_fft, width, label='c (u*u_x)', alpha=0.8)\n",
    "plt.axhline(y=-1.0, color='red', linestyle='--', linewidth=2, label='True')\n",
    "plt.ylabel('Coefficient Value', fontsize=10)\n",
    "plt.title('FFT: Coefficient Recovery', fontweight='bold', fontsize=10)\n",
    "plt.xticks(x_pos, methods_fft, fontsize=8)\n",
    "plt.legend(fontsize=8)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Row 2: Coefficient Recovery - FD\n",
    "ax8 = plt.subplot(3, 4, 8)\n",
    "a_vals_fd = [clean_fd['a'], baseline_fd_notreat['a'], robust_fd_135['a'], robust_fd_10['a']]\n",
    "b_vals_fd = [clean_fd['b'], baseline_fd_notreat['b'], robust_fd_135['b'], robust_fd_10['b']]\n",
    "c_vals_fd = [clean_fd['c'], baseline_fd_notreat['c'], robust_fd_135['c'], robust_fd_10['c']]\n",
    "plt.bar(x_pos - width, a_vals_fd, width, label='a (u_xx)', alpha=0.8)\n",
    "plt.bar(x_pos, b_vals_fd, width, label='b (u_xxxx)', alpha=0.8)\n",
    "plt.bar(x_pos + width, c_vals_fd, width, label='c ((u²)_x)', alpha=0.8)\n",
    "plt.axhline(y=-1.0, color='red', linestyle='--', linewidth=2)\n",
    "plt.axhline(y=-0.5, color='orange', linestyle='--', linewidth=2, label='True (c)')\n",
    "plt.ylabel('Coefficient Value', fontsize=10)\n",
    "plt.title('Finite Diff: Coefficient Recovery', fontweight='bold', fontsize=10)\n",
    "plt.xticks(x_pos, methods_fft, fontsize=8)\n",
    "plt.legend(fontsize=8)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Row 3: RMSE and R² Comparisons\n",
    "ax9 = plt.subplot(3, 4, 9)\n",
    "rmse_fft = [clean_fft['rmse'], baseline_fft_notreat['rmse'], robust_fft_135['rmse'], robust_fft_10['rmse']]\n",
    "rmse_fd = [clean_fd['rmse'], baseline_fd_notreat['rmse'], robust_fd_135['rmse'], robust_fd_10['rmse']]\n",
    "width2 = 0.35\n",
    "plt.bar(x_pos - width2/2, rmse_fft, width2, label='FFT', alpha=0.8, color='blue')\n",
    "plt.bar(x_pos + width2/2, rmse_fd, width2, label='Finite Diff', alpha=0.8, color='green')\n",
    "plt.ylabel('RMSE', fontsize=10)\n",
    "plt.title('RMSE Comparison (Lower is Better)', fontweight='bold', fontsize=10)\n",
    "plt.xticks(x_pos, methods_fft, fontsize=8)\n",
    "plt.legend(fontsize=8)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.yscale('log')\n",
    "\n",
    "ax10 = plt.subplot(3, 4, 10)\n",
    "r2_fft = [clean_fft['r2'], baseline_fft_notreat['r2'], robust_fft_135['r2'], robust_fft_10['r2']]\n",
    "r2_fd = [clean_fd['r2'], baseline_fd_notreat['r2'], robust_fd_135['r2'], robust_fd_10['r2']]\n",
    "plt.bar(x_pos - width2/2, r2_fft, width2, label='FFT', alpha=0.8, color='blue')\n",
    "plt.bar(x_pos + width2/2, r2_fd, width2, label='Finite Diff', alpha=0.8, color='green')\n",
    "plt.axhline(y=1.0, color='black', linestyle=':', linewidth=1)\n",
    "plt.ylabel('R² Score', fontsize=10)\n",
    "plt.title('R² Comparison (Higher is Better)', fontweight='bold', fontsize=10)\n",
    "plt.xticks(x_pos, methods_fft, fontsize=8)\n",
    "plt.legend(fontsize=8)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Row 3: Absolute Errors\n",
    "ax11 = plt.subplot(3, 4, 11)\n",
    "err_a_fft = [clean_fft['abs_error_a'], baseline_fft_notreat['abs_error_a'], \n",
    "             robust_fft_135['abs_error_a'], robust_fft_10['abs_error_a']]\n",
    "err_b_fft = [clean_fft['abs_error_b'], baseline_fft_notreat['abs_error_b'],\n",
    "             robust_fft_135['abs_error_b'], robust_fft_10['abs_error_b']]\n",
    "err_c_fft = [clean_fft['abs_error_c'], baseline_fft_notreat['abs_error_c'],\n",
    "             robust_fft_135['abs_error_c'], robust_fft_10['abs_error_c']]\n",
    "plt.bar(x_pos - width, err_a_fft, width, label='|Err a|', alpha=0.8)\n",
    "plt.bar(x_pos, err_b_fft, width, label='|Err b|', alpha=0.8)\n",
    "plt.bar(x_pos + width, err_c_fft, width, label='|Err c|', alpha=0.8)\n",
    "plt.ylabel('Absolute Error', fontsize=10)\n",
    "plt.title('FFT: Absolute Coefficient Errors', fontweight='bold', fontsize=10)\n",
    "plt.xticks(x_pos, methods_fft, fontsize=8)\n",
    "plt.legend(fontsize=8)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.yscale('log')\n",
    "\n",
    "ax12 = plt.subplot(3, 4, 12)\n",
    "err_a_fd = [clean_fd['abs_error_a'], baseline_fd_notreat['abs_error_a'],\n",
    "            robust_fd_135['abs_error_a'], robust_fd_10['abs_error_a']]\n",
    "err_b_fd = [clean_fd['abs_error_b'], baseline_fd_notreat['abs_error_b'],\n",
    "            robust_fd_135['abs_error_b'], robust_fd_10['abs_error_b']]\n",
    "err_c_fd = [clean_fd['abs_error_c'], baseline_fd_notreat['abs_error_c'],\n",
    "            robust_fd_135['abs_error_c'], robust_fd_10['abs_error_c']]\n",
    "plt.bar(x_pos - width, err_a_fd, width, label='|Err a|', alpha=0.8)\n",
    "plt.bar(x_pos, err_b_fd, width, label='|Err b|', alpha=0.8)\n",
    "plt.bar(x_pos + width, err_c_fd, width, label='|Err c|', alpha=0.8)\n",
    "plt.ylabel('Absolute Error', fontsize=10)\n",
    "plt.title('Finite Diff: Absolute Coefficient Errors', fontweight='bold', fontsize=10)\n",
    "plt.xticks(x_pos, methods_fft, fontsize=8)\n",
    "plt.legend(fontsize=8)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('varying_ic_visual_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"    Saved: varying_ic_visual_comparison.png\")\n",
    "\n",
    "# ===== VISUALIZATION 2: Enhanced Heatmaps with Clean Data =====\n",
    "fig2, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "methods_all = ['Clean', 'No Treat', 'Robust\\n(ε=1.35)', 'Robust\\n(ε=1.0)']\n",
    "derivatives = ['FFT', 'Finite Diff']\n",
    "\n",
    "# RMSE Heatmap\n",
    "rmse_matrix = np.array([rmse_fft, rmse_fd])\n",
    "sns.heatmap(rmse_matrix, annot=True, fmt='.4f', cmap='RdYlGn_r',\n",
    "            xticklabels=methods_all, yticklabels=derivatives, ax=axes[0,0],\n",
    "            cbar_kws={'label': 'RMSE'})\n",
    "axes[0,0].set_title('RMSE by Method (Including Clean Baseline)', fontweight='bold', fontsize=12)\n",
    "axes[0,0].set_xlabel('Treatment Method')\n",
    "axes[0,0].set_ylabel('Derivative Type')\n",
    "\n",
    "# R² Heatmap\n",
    "r2_matrix = np.array([r2_fft, r2_fd])\n",
    "sns.heatmap(r2_matrix, annot=True, fmt='.4f', cmap='RdYlGn',\n",
    "            xticklabels=methods_all, yticklabels=derivatives, ax=axes[0,1],\n",
    "            cbar_kws={'label': 'R² Score'})\n",
    "axes[0,1].set_title('R² Score by Method (Including Clean Baseline)', fontweight='bold', fontsize=12)\n",
    "axes[0,1].set_xlabel('Treatment Method')\n",
    "axes[0,1].set_ylabel('Derivative Type')\n",
    "\n",
    "# Error a Heatmap\n",
    "err_a_matrix = np.array([err_a_fft, err_a_fd])\n",
    "sns.heatmap(err_a_matrix, annot=True, fmt='.4f', cmap='RdYlGn_r',\n",
    "            xticklabels=methods_all, yticklabels=derivatives, ax=axes[1,0],\n",
    "            cbar_kws={'label': '|Error in a|'})\n",
    "axes[1,0].set_title('Absolute Error in Coefficient a', fontweight='bold', fontsize=12)\n",
    "axes[1,0].set_xlabel('Treatment Method')\n",
    "axes[1,0].set_ylabel('Derivative Type')\n",
    "\n",
    "# Error b Heatmap\n",
    "err_b_matrix = np.array([err_b_fft, err_b_fd])\n",
    "sns.heatmap(err_b_matrix, annot=True, fmt='.4f', cmap='RdYlGn_r',\n",
    "            xticklabels=methods_all, yticklabels=derivatives, ax=axes[1,1],\n",
    "            cbar_kws={'label': '|Error in b|'})\n",
    "axes[1,1].set_title('Absolute Error in Coefficient b', fontweight='bold', fontsize=12)\n",
    "axes[1,1].set_xlabel('Treatment Method')\n",
    "axes[1,1].set_ylabel('Derivative Type')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('varying_ic_heatmap_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"    Saved: varying_ic_heatmap_comparison.png\")\n",
    "\n",
    "# ===== Save Comparison Data =====\n",
    "df_comparison.to_csv('varying_ic_all_methods_comparison.csv', index=False)\n",
    "print(\"    Saved: varying_ic_all_methods_comparison.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"   VARYING IC ENHANCED COMPARISON COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  1. varying_ic_visual_comparison.png - Complete visualization (12 subplots)\")\n",
    "print(\"     - Row 1: Spatiotemporal (Clean, Varying IC, Outlier Maps)\")\n",
    "print(\"     - Row 2: Sample frames, divergence, coefficient recovery\")\n",
    "print(\"     - Row 3: RMSE, R², absolute errors\")\n",
    "print(\"  2. varying_ic_heatmap_comparison.png - Performance heatmaps with clean data\")\n",
    "print(\"  3. varying_ic_all_methods_comparison.csv - Complete data\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n1. Outlier Detection:\")\n",
    "print(f\"   - ε=1.35 detected {outlier_pct_135:.1f}% outliers\")\n",
    "print(f\"   - ε=1.0 detected {outlier_pct_10:.1f}% outliers (more aggressive)\")\n",
    "print(f\"   - Green regions = normal frames, Red regions = outlier frames\")\n",
    "\n",
    "print(f\"\\n2. Spatiotemporal Display:\")\n",
    "print(f\"   - Showing first {n_frames_viz} frames (0 to {time_duration:.1f} time units)\")\n",
    "print(f\"   - Clean vs Varying IC shows visual difference in patterns\")\n",
    "print(f\"   - Outlier maps show which regions are downweighted\")\n",
    "\n",
    "print(f\"\\n3. Performance:\")\n",
    "print(f\"   - Clean FFT: RMSE={clean_fft['rmse']:.6f}, R²={clean_fft['r2']:.6f}\")\n",
    "print(f\"   - Best treated: Check heatmaps for greenest cells\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93602a9",
   "metadata": {},
   "source": [
    "# REAL IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2e7494",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PHASE 4.1: LOAD AND PREPROCESS REAL EXPERIMENTAL IMAGES\n",
    "\n",
    "This script loads the 50-frame TIF image sequence from laser-matter interaction\n",
    "experiments, converts to grayscale, extracts intensity arrays, and performs\n",
    "initial quality checks.\n",
    "\n",
    "Author: SINDy Project\n",
    "Phase: 4.1 - Data Loading\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (18, 10)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 4.1: LOADING REAL EXPERIMENTAL IMAGES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "IMAGE_DIRECTORY = \"Real-Images\"  # Directory containing the 50 TIF files\n",
    "\n",
    "\n",
    "# Output settings\n",
    "SAVE_NPZ = True  # Save processed data to NPZ file\n",
    "SAVE_PLOTS = True  # Save visualization plots\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Image directory: {IMAGE_DIRECTORY}\")\n",
    "print(f\"  Looking for .tif files...\")\n",
    "\n",
    "# ===== STEP 1: FIND AND LOAD IMAGE FILES =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 1: LOCATING IMAGE FILES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Find all TIF files\n",
    "tif_files = sorted(glob.glob(os.path.join(IMAGE_DIRECTORY, \"*.tif\")))\n",
    "\n",
    "if len(tif_files) == 0:\n",
    "    print(f\"  WARNING: No .tif files found in {IMAGE_DIRECTORY}\")\n",
    "    print(f\"   Please ensure images are in the correct directory.\")\n",
    "    print(f\"   Current working directory: {os.getcwd()}\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"\\n Found {len(tif_files)} TIF files\")\n",
    "print(f\"   First file: {os.path.basename(tif_files[0])}\")\n",
    "print(f\"   Last file: {os.path.basename(tif_files[-1])}\")\n",
    "\n",
    "# Extract frame numbers from filenames\n",
    "frame_numbers = []\n",
    "for fname in tif_files:\n",
    "    basename = os.path.basename(fname)\n",
    "    # Try to extract number from filename (e.g., \"0.tif\" -> 0, \"frame_10.tif\" -> 10)\n",
    "    try:\n",
    "        num = int(''.join(filter(str.isdigit, basename.split('.')[0])))\n",
    "        frame_numbers.append(num)\n",
    "    except:\n",
    "        frame_numbers.append(len(frame_numbers))  # Fallback: sequential numbering\n",
    "\n",
    "print(f\"   Frame range: {min(frame_numbers)} to {max(frame_numbers)}\")\n",
    "\n",
    "# ===== STEP 2: LOAD FIRST IMAGE TO GET DIMENSIONS =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: ANALYZING IMAGE PROPERTIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "first_img = Image.open(tif_files[0])\n",
    "first_array = np.array(first_img)\n",
    "\n",
    "print(f\"\\nFirst image properties:\")\n",
    "print(f\"  File: {os.path.basename(tif_files[0])}\")\n",
    "print(f\"  Shape: {first_array.shape}\")\n",
    "print(f\"  Data type: {first_array.dtype}\")\n",
    "print(f\"  Mode: {first_img.mode}\")\n",
    "print(f\"  Value range: [{first_array.min()}, {first_array.max()}]\")\n",
    "\n",
    "# Determine if grayscale conversion needed\n",
    "needs_conversion = len(first_array.shape) == 3\n",
    "if needs_conversion:\n",
    "    print(f\"  → Image has {first_array.shape[2]} channels (needs grayscale conversion)\")\n",
    "else:\n",
    "    print(f\"  → Image is already grayscale\")\n",
    "\n",
    "# ===== STEP 3: LOAD ALL IMAGES =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3: LOADING ALL IMAGES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "images_raw = []\n",
    "images_gray = []\n",
    "\n",
    "print(f\"\\nLoading {len(tif_files)} frames...\")\n",
    "for idx, fname in enumerate(tif_files):\n",
    "    img = Image.open(fname)\n",
    "    img_array = np.array(img)\n",
    "    images_raw.append(img_array)\n",
    "    \n",
    "    # Convert to grayscale if needed\n",
    "    if len(img_array.shape) == 3:\n",
    "        if img_array.shape[2] == 4:  # RGBA\n",
    "            # Average RGB channels (ignore alpha)\n",
    "            gray = np.mean(img_array[:, :, :3], axis=2)\n",
    "        elif img_array.shape[2] == 3:  # RGB\n",
    "            # Average all channels\n",
    "            gray = np.mean(img_array, axis=2)\n",
    "        else:\n",
    "            gray = img_array[:, :, 0]  # Take first channel\n",
    "    else:\n",
    "        gray = img_array\n",
    "    \n",
    "    images_gray.append(gray)\n",
    "    \n",
    "    if (idx + 1) % 10 == 0:\n",
    "        print(f\"  Loaded {idx + 1}/{len(tif_files)} frames...\")\n",
    "\n",
    "print(f\"\\n Successfully loaded all {len(tif_files)} frames\")\n",
    "\n",
    "# Convert to numpy array\n",
    "u_images = np.array(images_gray)  # Shape: (Nt, Ny, Nx)\n",
    "\n",
    "print(f\"\\nProcessed data shape: {u_images.shape}\")\n",
    "print(f\"  Nt (time): {u_images.shape[0]} frames\")\n",
    "print(f\"  Ny (height): {u_images.shape[1]} pixels\")\n",
    "print(f\"  Nx (width): {u_images.shape[2]} pixels\")\n",
    "print(f\"  Data type: {u_images.dtype}\")\n",
    "print(f\"  Memory: {u_images.nbytes / 1e6:.2f} MB\")\n",
    "\n",
    "# ===== STEP 4: DATA QUALITY CHECKS =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 4: DATA QUALITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Compute statistics\n",
    "print(f\"\\nIntensity statistics across all frames:\")\n",
    "print(f\"  Global min: {u_images.min():.2f}\")\n",
    "print(f\"  Global max: {u_images.max():.2f}\")\n",
    "print(f\"  Global mean: {u_images.mean():.2f}\")\n",
    "print(f\"  Global std: {u_images.std():.2f}\")\n",
    "\n",
    "# Per-frame statistics\n",
    "frame_means = np.mean(u_images, axis=(1, 2))\n",
    "frame_stds = np.std(u_images, axis=(1, 2))\n",
    "frame_mins = np.min(u_images, axis=(1, 2))\n",
    "frame_maxs = np.max(u_images, axis=(1, 2))\n",
    "\n",
    "print(f\"\\nPer-frame statistics:\")\n",
    "print(f\"  Mean intensity range: [{frame_means.min():.2f}, {frame_means.max():.2f}]\")\n",
    "print(f\"  Std intensity range: [{frame_stds.min():.2f}, {frame_stds.max():.2f}]\")\n",
    "\n",
    "# Temporal consistency\n",
    "print(f\"\\nTemporal consistency:\")\n",
    "frame_to_frame_diff = np.mean(np.abs(np.diff(u_images, axis=0)))\n",
    "print(f\"  Average frame-to-frame difference: {frame_to_frame_diff:.2f}\")\n",
    "\n",
    "# Frame-to-frame correlation\n",
    "correlations = []\n",
    "for i in range(len(u_images) - 1):\n",
    "    corr = np.corrcoef(u_images[i].flatten(), u_images[i+1].flatten())[0, 1]\n",
    "    correlations.append(corr)\n",
    "mean_correlation = np.mean(correlations)\n",
    "print(f\"  Average frame-to-frame correlation: {mean_correlation:.4f}\")\n",
    "\n",
    "if mean_correlation > 0.8:\n",
    "    print(f\"  → HIGH correlation: Smooth temporal evolution \")\n",
    "elif mean_correlation > 0.5:\n",
    "    print(f\"  → MODERATE correlation: Some temporal variation\")\n",
    "else:\n",
    "    print(f\"  → LOW correlation: High temporal variation or noise \")\n",
    "\n",
    "# ===== STEP 5: NORMALIZATION (OPTIONAL) =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 5: INTENSITY NORMALIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Option 1: Keep as-is (0-255 range)\n",
    "u_images_normalized = u_images.copy()\n",
    "\n",
    "# Option 2: Normalize to [0, 1]\n",
    "# u_images_normalized = (u_images - u_images.min()) / (u_images.max() - u_images.min())\n",
    "\n",
    "# Option 3: Z-score normalization\n",
    "# u_images_normalized = (u_images - u_images.mean()) / u_images.std()\n",
    "\n",
    "print(f\"Normalization: Keeping original intensity range [0, 255]\")\n",
    "print(f\"  (Can be changed in script if needed)\")\n",
    "\n",
    "# ===== STEP 6: SAVE PROCESSED DATA =====\n",
    "if SAVE_NPZ:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 6: SAVING PROCESSED DATA\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    output_file = \"phase4_01_loaded_images.npz\"\n",
    "    np.savez_compressed(output_file,\n",
    "                       u_images=u_images_normalized,\n",
    "                       frame_numbers=np.array(frame_numbers),\n",
    "                       frame_means=frame_means,\n",
    "                       frame_stds=frame_stds,\n",
    "                       shape=u_images.shape)\n",
    "    \n",
    "    print(f\"\\nSaved processed data to: {output_file}\")\n",
    "    print(f\"   Contains:\")\n",
    "    print(f\"     - u_images: {u_images.shape} array\")\n",
    "    print(f\"     - frame_numbers: Frame identifiers\")\n",
    "    print(f\"     - frame_means, frame_stds: Per-frame statistics\")\n",
    "\n",
    "# ===== STEP 7: VISUALIZATION =====\n",
    "if SAVE_PLOTS:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 7: CREATING VISUALIZATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # === Figure 1: Sample frames overview ===\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "    fig.suptitle('Sample Frames from Real Experimental Data', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Show 10 evenly spaced frames\n",
    "    frame_indices = np.linspace(0, len(u_images) - 1, 10, dtype=int)\n",
    "    \n",
    "    for idx, (ax, frame_idx) in enumerate(zip(axes.flat, frame_indices)):\n",
    "        ax.imshow(u_images[frame_idx], cmap='gray', aspect='auto')\n",
    "        ax.set_title(f'Frame {frame_numbers[frame_idx]}')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('phase4_01_sample_frames.png', dpi=150, bbox_inches='tight')\n",
    "    print(f\"   Saved: phase4_01_sample_frames.png\")\n",
    "    \n",
    "    # === Figure 2: Temporal evolution analysis ===\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    fig.suptitle('Temporal Evolution Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Mean intensity over time\n",
    "    axes[0, 0].plot(frame_numbers, frame_means, 'o-', linewidth=2, markersize=4)\n",
    "    axes[0, 0].set_xlabel('Frame Number')\n",
    "    axes[0, 0].set_ylabel('Mean Intensity')\n",
    "    axes[0, 0].set_title('Mean Intensity Evolution')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Std intensity over time\n",
    "    axes[0, 1].plot(frame_numbers, frame_stds, 'o-', linewidth=2, markersize=4, color='orange')\n",
    "    axes[0, 1].set_xlabel('Frame Number')\n",
    "    axes[0, 1].set_ylabel('Std Intensity')\n",
    "    axes[0, 1].set_title('Intensity Variability Evolution')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Frame-to-frame correlation\n",
    "    axes[1, 0].plot(frame_numbers[:-1], correlations, 'o-', linewidth=2, markersize=4, color='green')\n",
    "    axes[1, 0].axhline(mean_correlation, ls='--', color='red', label=f'Mean: {mean_correlation:.3f}')\n",
    "    axes[1, 0].set_xlabel('Frame Number')\n",
    "    axes[1, 0].set_ylabel('Correlation with Next Frame')\n",
    "    axes[1, 0].set_title('Temporal Correlation')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Intensity distribution (all frames combined)\n",
    "    axes[1, 1].hist(u_images.flatten(), bins=100, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    axes[1, 1].set_xlabel('Intensity Value')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].set_title('Overall Intensity Distribution')\n",
    "    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('phase4_01_temporal_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    print(f\"  Saved: phase4_01_temporal_analysis.png\")\n",
    "    \n",
    "    # === Figure 3: Spatiotemporal visualization ===\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    fig.suptitle('Spatiotemporal Structure', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Middle row evolution (1D slice over time)\n",
    "    mid_row = u_images.shape[1] // 2\n",
    "    middle_row_data = u_images[:, mid_row, :]\n",
    "    \n",
    "    axes[0].imshow(middle_row_data, aspect='auto', cmap='gray', \n",
    "                   extent=[0, u_images.shape[2], frame_numbers[-1], frame_numbers[0]])\n",
    "    axes[0].set_xlabel('X Position (pixels)')\n",
    "    axes[0].set_ylabel('Frame Number')\n",
    "    axes[0].set_title('Middle Row Evolution (X-T Diagram)')\n",
    "    \n",
    "    # Middle column evolution\n",
    "    mid_col = u_images.shape[2] // 2\n",
    "    middle_col_data = u_images[:, :, mid_col]\n",
    "    \n",
    "    axes[1].imshow(middle_col_data, aspect='auto', cmap='gray',\n",
    "                   extent=[0, u_images.shape[1], frame_numbers[-1], frame_numbers[0]])\n",
    "    axes[1].set_xlabel('Y Position (pixels)')\n",
    "    axes[1].set_ylabel('Frame Number')\n",
    "    axes[1].set_title('Middle Column Evolution (Y-T Diagram)')\n",
    "    \n",
    "    # Sample spatial profiles\n",
    "    axes[2].set_title('Spatial Profiles (Middle Row)')\n",
    "    sample_frames = np.linspace(0, len(u_images) - 1, 5, dtype=int)\n",
    "    for frame_idx in sample_frames:\n",
    "        axes[2].plot(u_images[frame_idx, mid_row, :], \n",
    "                    label=f'Frame {frame_numbers[frame_idx]}', \n",
    "                    alpha=0.7, linewidth=1.5)\n",
    "    axes[2].set_xlabel('X Position (pixels)')\n",
    "    axes[2].set_ylabel('Intensity')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('phase4_01_spatiotemporal.png', dpi=150, bbox_inches='tight')\n",
    "    print(f\"  Saved: phase4_01_spatiotemporal.png\")\n",
    "\n",
    "# ===== FINAL SUMMARY =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 4.1 COMPLETE - SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n Loaded Data:\")\n",
    "print(f\"  • {len(tif_files)} frames loaded successfully\")\n",
    "print(f\"  • Shape: {u_images.shape} (Nt, Ny, Nx)\")\n",
    "print(f\"  • Intensity range: [{u_images.min():.1f}, {u_images.max():.1f}]\")\n",
    "\n",
    "print(f\"\\nQuality Assessment:\")\n",
    "print(f\"  • Mean frame-to-frame correlation: {mean_correlation:.3f}\")\n",
    "if mean_correlation > 0.7:\n",
    "    quality = \"EXCELLENT - Good temporal continuity\"\n",
    "elif mean_correlation > 0.5:\n",
    "    quality = \"GOOD - Moderate temporal continuity\"\n",
    "else:\n",
    "    quality = \"FAIR - Significant temporal variation\"\n",
    "print(f\"  • Data quality: {quality}\")\n",
    "\n",
    "print(f\"\\n Outputs Generated:\")\n",
    "if SAVE_NPZ:\n",
    "    print(f\"   phase4_01_loaded_images.npz\")\n",
    "if SAVE_PLOTS:\n",
    "    print(f\"   phase4_01_sample_frames.png\")\n",
    "    print(f\"   phase4_01_temporal_analysis.png\")\n",
    "    print(f\"   phase4_01_spatiotemporal.png\")\n",
    "\n",
    "print(f\"\\n Next Steps:\")\n",
    "print(f\"  → Run Phase 4.2: Extract 1D slices for SINDy analysis\")\n",
    "print(f\"  → Choose extraction strategy (single row, averaged rows, multiple slices)\")\n",
    "print(f\"  → Continue with baseline SINDy application (Phase 4.3)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"READY FOR PHASE 4.2! \")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8fcf34",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3820f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PHASE 4.1: LOAD AND PREPROCESS REAL EXPERIMENTAL IMAGES\n",
    "\n",
    "This script loads the 50-frame TIF image sequence from laser-matter interaction\n",
    "experiments, converts to grayscale, extracts intensity arrays, and performs\n",
    "initial quality checks.\n",
    "\n",
    "Author: SINDy Project\n",
    "Phase: 4.1 - Data Loading\n",
    "Version: 2.0 (Improved)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "from scipy import ndimage\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (18, 10)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 4.1: LOADING REAL EXPERIMENTAL IMAGES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "IMAGE_DIRECTORY = \"Real-Images\"  # Directory containing the 50 TIF files\n",
    "NORMALIZATION_METHOD = \"keep\"  # Options: \"keep\", \"minmax\", \"zscore\"\n",
    "GRAYSCALE_METHOD = \"luminosity\"  # Options: \"simple\", \"luminosity\"\n",
    "PIXEL_SIZE_UM = None  # Set to actual pixel size in micrometers if known (e.g., 0.5)\n",
    "TIME_PER_FRAME_S = None  # Set to actual time between frames in seconds if known\n",
    "\n",
    "# Output settings\n",
    "SAVE_NPZ = True  # Save processed data to NPZ file\n",
    "SAVE_PLOTS = True  # Save visualization plots\n",
    "OUTPUT_DIR = \"phase4_real_images\"  # Directory for outputs\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Image directory: {IMAGE_DIRECTORY}\")\n",
    "print(f\"  Normalization method: {NORMALIZATION_METHOD}\")\n",
    "print(f\"  Grayscale conversion: {GRAYSCALE_METHOD}\")\n",
    "print(f\"  Pixel size: {PIXEL_SIZE_UM} µm\" if PIXEL_SIZE_UM else \"  Pixel size: Not specified\")\n",
    "print(f\"  Time per frame: {TIME_PER_FRAME_S} s\" if TIME_PER_FRAME_S else \"  Time per frame: Not specified\")\n",
    "print(f\"  Looking for .tif files...\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ===== STEP 1: FIND AND LOAD IMAGE FILES =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 1: LOCATING IMAGE FILES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Find all TIF files\n",
    "tif_files = sorted(glob.glob(os.path.join(IMAGE_DIRECTORY, \"*.tif\")))\n",
    "\n",
    "if len(tif_files) == 0:\n",
    "    print(f\"\\n      WARNING: No .tif files found in {IMAGE_DIRECTORY}\")\n",
    "    print(f\"     Please ensure images are in the correct directory.\")\n",
    "    print(f\"     Current working directory: {os.getcwd()}\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"\\n   Found {len(tif_files)} TIF files\")\n",
    "print(f\"   First file: {os.path.basename(tif_files[0])}\")\n",
    "print(f\"   Last file: {os.path.basename(tif_files[-1])}\")\n",
    "\n",
    "# Extract frame numbers from filenames (IMPROVED)\n",
    "frame_numbers = []\n",
    "for fname in tif_files:\n",
    "    basename = os.path.basename(fname)\n",
    "    # Try to find the first integer in the filename\n",
    "    match = re.search(r'\\d+', basename)\n",
    "    if match:\n",
    "        frame_numbers.append(int(match.group()))\n",
    "    else:\n",
    "        # Fallback: use index\n",
    "        frame_numbers.append(len(frame_numbers))\n",
    "        print(f\"      WARNING: Could not extract frame number from '{basename}', using index {len(frame_numbers)-1}\")\n",
    "\n",
    "print(f\"   Frame range: {min(frame_numbers)} to {max(frame_numbers)}\")\n",
    "\n",
    "# ===== STEP 2: LOAD FIRST IMAGE TO GET DIMENSIONS =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: ANALYZING IMAGE PROPERTIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "first_img = Image.open(tif_files[0])\n",
    "first_array = np.array(first_img)\n",
    "\n",
    "print(f\"\\nFirst image properties:\")\n",
    "print(f\"  File: {os.path.basename(tif_files[0])}\")\n",
    "print(f\"  Shape: {first_array.shape}\")\n",
    "print(f\"  Data type: {first_array.dtype}\")\n",
    "print(f\"  Mode: {first_img.mode}\")\n",
    "print(f\"  Value range: [{first_array.min()}, {first_array.max()}]\")\n",
    "\n",
    "# Determine if grayscale conversion needed\n",
    "needs_conversion = len(first_array.shape) == 3\n",
    "if needs_conversion:\n",
    "    print(f\"  → Image has {first_array.shape[2]} channels (needs grayscale conversion)\")\n",
    "    print(f\"  → Using {GRAYSCALE_METHOD} method\")\n",
    "else:\n",
    "    print(f\"  → Image is already grayscale\")\n",
    "\n",
    "# Memory estimate (IMPROVED)\n",
    "Nt = len(tif_files)\n",
    "if len(first_array.shape) == 3:\n",
    "    Ny, Nx = first_array.shape[0], first_array.shape[1]\n",
    "else:\n",
    "    Ny, Nx = first_array.shape\n",
    "\n",
    "estimated_memory_gb = (Nt * Ny * Nx * 8) / 1e9  # 8 bytes per float64\n",
    "\n",
    "print(f\"\\nMemory estimate:\")\n",
    "print(f\"  Array size: ({Nt}, {Ny}, {Nx})\")\n",
    "print(f\"  Estimated memory: {estimated_memory_gb:.2f} GB\")\n",
    "\n",
    "if estimated_memory_gb > 4.0:\n",
    "    print(f\"      WARNING: Large dataset detected!\")\n",
    "    print(f\"     Consider processing in chunks or using memory-mapped arrays\")\n",
    "\n",
    "# ===== STEP 3: LOAD ALL IMAGES =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3: LOADING ALL IMAGES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "images_raw = []\n",
    "images_gray = []\n",
    "\n",
    "print(f\"\\nLoading {len(tif_files)} frames...\")\n",
    "for idx, fname in enumerate(tif_files):\n",
    "    img = Image.open(fname)\n",
    "    img_array = np.array(img)\n",
    "    images_raw.append(img_array)\n",
    "    \n",
    "    # Convert to grayscale if needed (IMPROVED)\n",
    "    if len(img_array.shape) == 3:\n",
    "        if GRAYSCALE_METHOD == \"luminosity\":\n",
    "            # Luminosity method (perceptually accurate)\n",
    "            if img_array.shape[2] == 4:  # RGBA\n",
    "                gray = (0.299 * img_array[:, :, 0] + \n",
    "                        0.587 * img_array[:, :, 1] + \n",
    "                        0.114 * img_array[:, :, 2])\n",
    "            elif img_array.shape[2] == 3:  # RGB\n",
    "                gray = (0.299 * img_array[:, :, 0] + \n",
    "                        0.587 * img_array[:, :, 1] + \n",
    "                        0.114 * img_array[:, :, 2])\n",
    "            else:\n",
    "                gray = img_array[:, :, 0]  # Take first channel\n",
    "        else:  # \"simple\" method\n",
    "            # Simple averaging\n",
    "            if img_array.shape[2] == 4:  # RGBA\n",
    "                gray = np.mean(img_array[:, :, :3], axis=2)\n",
    "            elif img_array.shape[2] == 3:  # RGB\n",
    "                gray = np.mean(img_array, axis=2)\n",
    "            else:\n",
    "                gray = img_array[:, :, 0]\n",
    "    else:\n",
    "        gray = img_array\n",
    "    \n",
    "    # Convert to float64 immediately (IMPROVED)\n",
    "    images_gray.append(gray.astype(np.float64))\n",
    "    \n",
    "    if (idx + 1) % 10 == 0:\n",
    "        print(f\"      Loaded {idx + 1}/{len(tif_files)} frames...\")\n",
    "\n",
    "print(f\"\\n   Successfully loaded all {len(tif_files)} frames\")\n",
    "\n",
    "# Convert to numpy array\n",
    "u_images = np.array(images_gray)  # Shape: (Nt, Ny, Nx)\n",
    "\n",
    "print(f\"\\nProcessed data shape: {u_images.shape}\")\n",
    "print(f\"  Nt (time): {u_images.shape[0]} frames\")\n",
    "print(f\"  Ny (height): {u_images.shape[1]} pixels\")\n",
    "print(f\"  Nx (width): {u_images.shape[2]} pixels\")\n",
    "print(f\"  Data type: {u_images.dtype}\")\n",
    "print(f\"  Memory: {u_images.nbytes / 1e6:.2f} MB\")\n",
    "\n",
    "# Physical dimensions (IMPROVED)\n",
    "if PIXEL_SIZE_UM is not None:\n",
    "    width_um = u_images.shape[2] * PIXEL_SIZE_UM\n",
    "    height_um = u_images.shape[1] * PIXEL_SIZE_UM\n",
    "    print(f\"\\nPhysical dimensions:\")\n",
    "    print(f\"  Width: {width_um:.2f} µm ({u_images.shape[2]} pixels)\")\n",
    "    print(f\"  Height: {height_um:.2f} µm ({u_images.shape[1]} pixels)\")\n",
    "\n",
    "if TIME_PER_FRAME_S is not None:\n",
    "    total_time = (u_images.shape[0] - 1) * TIME_PER_FRAME_S\n",
    "    print(f\"\\nTemporal dimensions:\")\n",
    "    print(f\"  Total duration: {total_time:.4f} s\")\n",
    "    print(f\"  Frame rate: {1/TIME_PER_FRAME_S:.2f} fps\")\n",
    "\n",
    "# ===== STEP 4: DATA QUALITY CHECKS =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 4: DATA QUALITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Compute statistics\n",
    "print(f\"\\nIntensity statistics across all frames:\")\n",
    "print(f\"  Global min: {u_images.min():.2f}\")\n",
    "print(f\"  Global max: {u_images.max():.2f}\")\n",
    "print(f\"  Global mean: {u_images.mean():.2f}\")\n",
    "print(f\"  Global std: {u_images.std():.2f}\")\n",
    "\n",
    "# Per-frame statistics\n",
    "frame_means = np.mean(u_images, axis=(1, 2))\n",
    "frame_stds = np.std(u_images, axis=(1, 2))\n",
    "frame_mins = np.min(u_images, axis=(1, 2))\n",
    "frame_maxs = np.max(u_images, axis=(1, 2))\n",
    "\n",
    "print(f\"\\nPer-frame statistics:\")\n",
    "print(f\"  Mean intensity range: [{frame_means.min():.2f}, {frame_means.max():.2f}]\")\n",
    "print(f\"  Std intensity range: [{frame_stds.min():.2f}, {frame_stds.max():.2f}]\")\n",
    "print(f\"  Intensity drift: {frame_means.max() - frame_means.min():.2f} (max - min mean)\")\n",
    "\n",
    "# Temporal consistency\n",
    "print(f\"\\nTemporal consistency:\")\n",
    "frame_to_frame_diff = np.mean(np.abs(np.diff(u_images, axis=0)))\n",
    "print(f\"  Average frame-to-frame difference: {frame_to_frame_diff:.2f}\")\n",
    "\n",
    "# Frame-to-frame correlation\n",
    "correlations = []\n",
    "for i in range(len(u_images) - 1):\n",
    "    corr = np.corrcoef(u_images[i].flatten(), u_images[i+1].flatten())[0, 1]\n",
    "    correlations.append(corr)\n",
    "mean_correlation = np.mean(correlations)\n",
    "print(f\"  Average frame-to-frame correlation: {mean_correlation:.4f}\")\n",
    "\n",
    "if mean_correlation > 0.8:\n",
    "    print(f\"  →    HIGH correlation: Smooth temporal evolution\")\n",
    "elif mean_correlation > 0.5:\n",
    "    print(f\"  →     MODERATE correlation: Some temporal variation\")\n",
    "else:\n",
    "    print(f\"  →     LOW correlation: High temporal variation or noise\")\n",
    "\n",
    "# ===== ADVANCED QUALITY METRICS (NEW) =====\n",
    "print(f\"\\nAdvanced quality checks:\")\n",
    "\n",
    "# 1. Noise estimation (using Donoho-Johnstone estimator)\n",
    "def estimate_noise(image):\n",
    "    \"\"\"Estimate noise using median absolute deviation of Laplacian\"\"\"\n",
    "    H = np.array([[1, -2, 1], [-2, 4, -2], [1, -2, 1]])  # Laplacian kernel\n",
    "    filtered = ndimage.convolve(image, H)\n",
    "    sigma = np.median(np.abs(filtered)) / 0.6745\n",
    "    return sigma\n",
    "\n",
    "noise_estimates = [estimate_noise(frame) for frame in u_images]\n",
    "mean_noise = np.mean(noise_estimates)\n",
    "std_noise = np.std(noise_estimates)\n",
    "print(f\"  Estimated noise level (σ): {mean_noise:.2f} ± {std_noise:.2f}\")\n",
    "\n",
    "# 2. Signal-to-Noise Ratio\n",
    "signal_power = np.var(u_images)  # Signal variance\n",
    "noise_power = mean_noise**2\n",
    "snr_db = 10 * np.log10(signal_power / noise_power)\n",
    "print(f\"  Signal-to-Noise Ratio: {snr_db:.2f} dB\")\n",
    "\n",
    "if snr_db > 20:\n",
    "    print(f\"    →    Excellent SNR (> 20 dB)\")\n",
    "elif snr_db > 10:\n",
    "    print(f\"    →   Moderate SNR (10-20 dB) - Denoising recommended\")\n",
    "else:\n",
    "    print(f\"    →  Low SNR (< 10 dB) - Aggressive denoising needed\")\n",
    "\n",
    "# 3. Check for dead/saturated pixels\n",
    "dead_pixels = np.sum(u_images == 0)\n",
    "saturated_pixels = np.sum(u_images == 255)\n",
    "total_pixels = u_images.size\n",
    "print(f\"  Dead pixels (0): {dead_pixels} ({100*dead_pixels/total_pixels:.3f}%)\")\n",
    "print(f\"  Saturated pixels (255): {saturated_pixels} ({100*saturated_pixels/total_pixels:.3f}%)\")\n",
    "\n",
    "if saturated_pixels > 0.01 * total_pixels:\n",
    "    print(f\"    →      WARNING: > 0.01% saturated pixels detected!\")\n",
    "\n",
    "# 4. Spatial gradient analysis\n",
    "print(f\"\\nSpatial structure:\")\n",
    "sample_frame = u_images[len(u_images)//2]  # Middle frame\n",
    "grad_x = np.gradient(sample_frame, axis=1)\n",
    "grad_y = np.gradient(sample_frame, axis=0)\n",
    "grad_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "mean_gradient = np.mean(grad_magnitude)\n",
    "print(f\"  Mean gradient magnitude (middle frame): {mean_gradient:.2f}\")\n",
    "\n",
    "if mean_gradient > 10:\n",
    "    print(f\"    →    Strong spatial gradients detected (good for 1D slice extraction)\")\n",
    "elif mean_gradient > 5:\n",
    "    print(f\"    →      Moderate spatial gradients\")\n",
    "else:\n",
    "    print(f\"    →     Weak spatial gradients (may affect derivative accuracy)\")\n",
    "\n",
    "# ===== STEP 5: NORMALIZATION =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 5: INTENSITY NORMALIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if NORMALIZATION_METHOD == \"keep\":\n",
    "    u_images_normalized = u_images.copy()\n",
    "    print(f\"   Normalization: Keeping original intensity range [{u_images.min():.2f}, {u_images.max():.2f}]\")\n",
    "    \n",
    "elif NORMALIZATION_METHOD == \"minmax\":\n",
    "    u_min, u_max = u_images.min(), u_images.max()\n",
    "    u_images_normalized = (u_images - u_min) / (u_max - u_min)\n",
    "    print(f\"   Normalization: Min-Max scaling to [0, 1]\")\n",
    "    print(f\"   Original range: [{u_min:.2f}, {u_max:.2f}]\")\n",
    "    print(f\"   New range: [{u_images_normalized.min():.4f}, {u_images_normalized.max():.4f}]\")\n",
    "    \n",
    "elif NORMALIZATION_METHOD == \"zscore\":\n",
    "    u_mean, u_std = u_images.mean(), u_images.std()\n",
    "    u_images_normalized = (u_images - u_mean) / u_std\n",
    "    print(f\"   Normalization: Z-score (zero mean, unit variance)\")\n",
    "    print(f\"   Original mean: {u_mean:.2f}, std: {u_std:.2f}\")\n",
    "    print(f\"   New mean: {u_images_normalized.mean():.6f}, std: {u_images_normalized.std():.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"     WARNING: Unknown normalization method '{NORMALIZATION_METHOD}', keeping original\")\n",
    "    u_images_normalized = u_images.copy()\n",
    "\n",
    "print(f\"\\n Recommendation for SINDy:\")\n",
    "print(f\"   • FFT derivatives: Use 'minmax' or 'zscore' (helps with numerical stability)\")\n",
    "print(f\"   • Finite Difference: 'keep' or 'minmax' work well\")\n",
    "print(f\"   • Current choice: '{NORMALIZATION_METHOD}'\")\n",
    "\n",
    "# ===== STEP 6: SAVE PROCESSED DATA =====\n",
    "if SAVE_NPZ:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 6: SAVING PROCESSED DATA\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    output_file = os.path.join(OUTPUT_DIR, \"phase4_01_loaded_images.npz\")\n",
    "    \n",
    "    # Prepare metadata\n",
    "    metadata = {\n",
    "        'normalization_method': NORMALIZATION_METHOD,\n",
    "        'grayscale_method': GRAYSCALE_METHOD,\n",
    "        'pixel_size_um': PIXEL_SIZE_UM if PIXEL_SIZE_UM else -1,\n",
    "        'time_per_frame_s': TIME_PER_FRAME_S if TIME_PER_FRAME_S else -1,\n",
    "        'mean_noise': mean_noise,\n",
    "        'snr_db': snr_db,\n",
    "        'mean_correlation': mean_correlation\n",
    "    }\n",
    "    \n",
    "    np.savez_compressed(output_file,\n",
    "                       u_images=u_images_normalized,\n",
    "                       u_images_raw=u_images,  # Save both normalized and raw\n",
    "                       frame_numbers=np.array(frame_numbers),\n",
    "                       frame_means=frame_means,\n",
    "                       frame_stds=frame_stds,\n",
    "                       noise_estimates=np.array(noise_estimates),\n",
    "                       correlations=np.array(correlations),\n",
    "                       shape=u_images.shape,\n",
    "                       **metadata)\n",
    "    \n",
    "    print(f\"\\n   Saved processed data to: {output_file}\")\n",
    "    print(f\"   Contains:\")\n",
    "    print(f\"     - u_images: {u_images.shape} array (normalized)\")\n",
    "    print(f\"     - u_images_raw: {u_images.shape} array (original)\")\n",
    "    print(f\"     - frame_numbers: Frame identifiers\")\n",
    "    print(f\"     - frame_means, frame_stds: Per-frame statistics\")\n",
    "    print(f\"     - noise_estimates: Per-frame noise levels\")\n",
    "    print(f\"     - correlations: Frame-to-frame correlations\")\n",
    "    print(f\"     - metadata: Processing parameters\")\n",
    "\n",
    "# ===== STEP 7: VISUALIZATION =====\n",
    "if SAVE_PLOTS:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 7: CREATING VISUALIZATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # === Figure 1: Sample frames overview ===\n",
    "    print(\"\\n  Creating Figure 1: Sample frames...\")\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "    fig.suptitle('Sample Frames from Real Experimental Data', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Show 10 evenly spaced frames\n",
    "    frame_indices = np.linspace(0, len(u_images) - 1, 10, dtype=int)\n",
    "    \n",
    "    for idx, (ax, frame_idx) in enumerate(zip(axes.flat, frame_indices)):\n",
    "        im = ax.imshow(u_images_normalized[frame_idx], cmap='gray', aspect='auto')\n",
    "        ax.set_title(f'Frame {frame_numbers[frame_idx]}\\nMean: {frame_means[frame_idx]:.1f}', \n",
    "                     fontsize=10)\n",
    "        ax.axis('off')\n",
    "        # Add colorbar to last plot\n",
    "        if idx == 9:\n",
    "            plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_file = os.path.join(OUTPUT_DIR, 'phase4_01_sample_frames.png')\n",
    "    plt.savefig(plot_file, dpi=150, bbox_inches='tight')\n",
    "    print(f\"      Saved: {plot_file}\")\n",
    "    plt.close()\n",
    "    \n",
    "    # === Figure 2: Temporal evolution analysis ===\n",
    "    print(\"  Creating Figure 2: Temporal evolution...\")\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('Temporal Evolution Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Mean intensity over time\n",
    "    axes[0, 0].plot(frame_numbers, frame_means, 'o-', linewidth=2, markersize=4)\n",
    "    axes[0, 0].set_xlabel('Frame Number')\n",
    "    axes[0, 0].set_ylabel('Mean Intensity')\n",
    "    axes[0, 0].set_title('Mean Intensity Evolution')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Std intensity over time\n",
    "    axes[0, 1].plot(frame_numbers, frame_stds, 'o-', linewidth=2, markersize=4, color='orange')\n",
    "    axes[0, 1].set_xlabel('Frame Number')\n",
    "    axes[0, 1].set_ylabel('Std Intensity')\n",
    "    axes[0, 1].set_title('Intensity Variability Evolution')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Noise estimates over time (NEW)\n",
    "    axes[0, 2].plot(frame_numbers, noise_estimates, 'o-', linewidth=2, markersize=4, color='red')\n",
    "    axes[0, 2].axhline(mean_noise, ls='--', color='black', label=f'Mean: {mean_noise:.2f}')\n",
    "    axes[0, 2].set_xlabel('Frame Number')\n",
    "    axes[0, 2].set_ylabel('Estimated Noise (σ)')\n",
    "    axes[0, 2].set_title('Noise Level Evolution')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Frame-to-frame correlation\n",
    "    axes[1, 0].plot(frame_numbers[:-1], correlations, 'o-', linewidth=2, markersize=4, color='green')\n",
    "    axes[1, 0].axhline(mean_correlation, ls='--', color='red', label=f'Mean: {mean_correlation:.3f}')\n",
    "    axes[1, 0].set_xlabel('Frame Number')\n",
    "    axes[1, 0].set_ylabel('Correlation with Next Frame')\n",
    "    axes[1, 0].set_title('Temporal Correlation')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Intensity distribution (all frames combined)\n",
    "    axes[1, 1].hist(u_images_normalized.flatten(), bins=100, color='steelblue', \n",
    "                    alpha=0.7, edgecolor='black')\n",
    "    axes[1, 1].set_xlabel('Intensity Value')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].set_title('Overall Intensity Distribution')\n",
    "    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Intensity range evolution (NEW)\n",
    "    axes[1, 2].fill_between(frame_numbers, frame_mins, frame_maxs, alpha=0.3, label='Min-Max Range')\n",
    "    axes[1, 2].plot(frame_numbers, frame_means, 'k-', linewidth=2, label='Mean')\n",
    "    axes[1, 2].set_xlabel('Frame Number')\n",
    "    axes[1, 2].set_ylabel('Intensity')\n",
    "    axes[1, 2].set_title('Intensity Range Evolution')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_file = os.path.join(OUTPUT_DIR, 'phase4_01_temporal_analysis.png')\n",
    "    plt.savefig(plot_file, dpi=150, bbox_inches='tight')\n",
    "    print(f\"      Saved: {plot_file}\")\n",
    "    plt.close()\n",
    "    \n",
    "    # === Figure 3: Spatiotemporal visualization ===\n",
    "    print(\"  Creating Figure 3: Spatiotemporal structure...\")\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Spatiotemporal Structure', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Middle row evolution (1D slice over time)\n",
    "    mid_row = u_images.shape[1] // 2\n",
    "    middle_row_data = u_images_normalized[:, mid_row, :]\n",
    "    \n",
    "    im0 = axes[0, 0].imshow(middle_row_data, aspect='auto', cmap='gray', \n",
    "                   extent=[0, u_images.shape[2], frame_numbers[-1], frame_numbers[0]])\n",
    "    axes[0, 0].set_xlabel('X Position (pixels)')\n",
    "    axes[0, 0].set_ylabel('Frame Number')\n",
    "    axes[0, 0].set_title('Middle Row Evolution (X-T Diagram)')\n",
    "    plt.colorbar(im0, ax=axes[0, 0], label='Intensity')\n",
    "    \n",
    "    # Middle column evolution\n",
    "    mid_col = u_images.shape[2] // 2\n",
    "    middle_col_data = u_images_normalized[:, :, mid_col]\n",
    "    \n",
    "    im1 = axes[0, 1].imshow(middle_col_data, aspect='auto', cmap='gray',\n",
    "                   extent=[0, u_images.shape[1], frame_numbers[-1], frame_numbers[0]])\n",
    "    axes[0, 1].set_xlabel('Y Position (pixels)')\n",
    "    axes[0, 1].set_ylabel('Frame Number')\n",
    "    axes[0, 1].set_title('Middle Column Evolution (Y-T Diagram)')\n",
    "    plt.colorbar(im1, ax=axes[0, 1], label='Intensity')\n",
    "    \n",
    "    # Sample spatial profiles (middle row)\n",
    "    axes[1, 0].set_title('Spatial Profiles (Middle Row)')\n",
    "    sample_frames = np.linspace(0, len(u_images) - 1, 5, dtype=int)\n",
    "    for frame_idx in sample_frames:\n",
    "        axes[1, 0].plot(u_images_normalized[frame_idx, mid_row, :], \n",
    "                    label=f'Frame {frame_numbers[frame_idx]}', \n",
    "                    alpha=0.7, linewidth=1.5)\n",
    "    axes[1, 0].set_xlabel('X Position (pixels)')\n",
    "    axes[1, 0].set_ylabel('Intensity')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gradient magnitude of middle frame (NEW)\n",
    "    mid_frame_idx = len(u_images) // 2\n",
    "    grad_x = np.gradient(u_images_normalized[mid_frame_idx], axis=1)\n",
    "    grad_y = np.gradient(u_images_normalized[mid_frame_idx], axis=0)\n",
    "    grad_mag = np.sqrt(grad_x**2 + grad_y**2)\n",
    "    \n",
    "    im2 = axes[1, 1].imshow(grad_mag, aspect='auto', cmap='hot')\n",
    "    axes[1, 1].set_title(f'Gradient Magnitude (Frame {frame_numbers[mid_frame_idx]})')\n",
    "    axes[1, 1].set_xlabel('X Position (pixels)')\n",
    "    axes[1, 1].set_ylabel('Y Position (pixels)')\n",
    "    plt.colorbar(im2, ax=axes[1, 1], label='|∇u|')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_file = os.path.join(OUTPUT_DIR, 'phase4_01_spatiotemporal.png')\n",
    "    plt.savefig(plot_file, dpi=150, bbox_inches='tight')\n",
    "    print(f\"  ✓ Saved: {plot_file}\")\n",
    "    plt.close()\n",
    "    \n",
    "    # === Figure 4: Quality metrics summary (NEW) ===\n",
    "    print(\"  Creating Figure 4: Quality metrics...\")\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Data Quality Metrics Summary', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # SNR visualization\n",
    "    axes[0, 0].bar(['Signal\\nPower', 'Noise\\nPower', 'SNR (dB)'], \n",
    "                   [signal_power, noise_power, snr_db],\n",
    "                   color=['green', 'red', 'blue'], alpha=0.7)\n",
    "    axes[0, 0].set_ylabel('Value')\n",
    "    axes[0, 0].set_title(f'Signal-to-Noise Analysis\\nSNR = {snr_db:.2f} dB')\n",
    "    axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Correlation histogram\n",
    "    axes[0, 1].hist(correlations, bins=30, color='green', alpha=0.7, edgecolor='black')\n",
    "    axes[0, 1].axvline(mean_correlation, color='red', linestyle='--', \n",
    "                       linewidth=2, label=f'Mean: {mean_correlation:.3f}')\n",
    "    axes[0, 1].set_xlabel('Correlation Coefficient')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].set_title('Distribution of Frame-to-Frame Correlations')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Noise histogram\n",
    "    axes[1, 0].hist(noise_estimates, bins=30, color='orange', alpha=0.7, edgecolor='black')\n",
    "    axes[1, 0].axvline(mean_noise, color='red', linestyle='--', \n",
    "                       linewidth=2, label=f'Mean: {mean_noise:.2f}')\n",
    "    axes[1, 0].set_xlabel('Estimated Noise (σ)')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Distribution of Noise Estimates')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Quality summary text\n",
    "    axes[1, 1].axis('off')\n",
    "    quality_text = f\"\"\"\n",
    "    DATA QUALITY SUMMARY\n",
    "    {'='*40}\n",
    "    \n",
    "    Temporal Quality:\n",
    "      • Mean correlation: {mean_correlation:.4f}\n",
    "      • Status: {' EXCELLENT' if mean_correlation > 0.8 else 'MODERATE' if mean_correlation > 0.5 else ' POOR'}\n",
    "    \n",
    "    Noise Characteristics:\n",
    "      • Noise level (σ): {mean_noise:.2f} ± {std_noise:.2f}\n",
    "      • SNR: {snr_db:.2f} dB\n",
    "      • Status: {'EXCELLENT' if snr_db > 20 else ' MODERATE' if snr_db > 10 else 'LOW'}\n",
    "    \n",
    "    Spatial Structure:\n",
    "      • Mean gradient: {mean_gradient:.2f}\n",
    "      • Status: {' STRONG' if mean_gradient > 10 else ' MODERATE' if mean_gradient > 5 else 'WEAK'}\n",
    "    \n",
    "    Data Integrity:\n",
    "      • Dead pixels: {100*dead_pixels/total_pixels:.3f}%\n",
    "      • Saturated pixels: {100*saturated_pixels/total_pixels:.3f}%\n",
    "      • Status: {' GOOD' if saturated_pixels < 0.01*total_pixels else ' CHECK SATURATION'}\n",
    "    \n",
    "    Recommendation:\n",
    "      {' Data suitable for SINDy analysis' if mean_correlation > 0.7 and snr_db > 10 else '    Consider preprocessing before SINDy'}\n",
    "    \"\"\"\n",
    "    axes[1, 1].text(0.1, 0.5, quality_text, fontsize=10, family='monospace',\n",
    "                    verticalalignment='center', transform=axes[1, 1].transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_file = os.path.join(OUTPUT_DIR, 'phase4_01_quality_metrics.png')\n",
    "    plt.savefig(plot_file, dpi=150, bbox_inches='tight')\n",
    "    print(f\"      Saved: {plot_file}\")\n",
    "    plt.close()\n",
    "\n",
    "# ===== FINAL SUMMARY =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 4.1 COMPLETE - SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n Loaded Data:\")\n",
    "print(f\"   • {len(tif_files)} frames loaded successfully\")\n",
    "print(f\"   • Shape: {u_images.shape} (Nt, Ny, Nx)\")\n",
    "print(f\"   • Intensity range: [{u_images.min():.1f}, {u_images.max():.1f}]\")\n",
    "print(f\"   • Normalized range: [{u_images_normalized.min():.4f}, {u_images_normalized.max():.4f}]\")\n",
    "\n",
    "print(f\"\\n Quality Assessment:\")\n",
    "print(f\"   • Mean frame-to-frame correlation: {mean_correlation:.3f}\")\n",
    "print(f\"   • Signal-to-Noise Ratio: {snr_db:.2f} dB\")\n",
    "print(f\"   • Mean noise level: {mean_noise:.2f}\")\n",
    "print(f\"   • Mean spatial gradient: {mean_gradient:.2f}\")\n",
    "\n",
    "# Overall quality rating\n",
    "quality_score = 0\n",
    "if mean_correlation > 0.7:\n",
    "    quality_score += 1\n",
    "if snr_db > 10:\n",
    "    quality_score += 1\n",
    "if mean_gradient > 5:\n",
    "    quality_score += 1\n",
    "if saturated_pixels < 0.01 * total_pixels:\n",
    "    quality_score += 1\n",
    "\n",
    "quality_ratings = {\n",
    "    4: \" EXCELLENT - Data ready for SINDy\",\n",
    "    3: \" GOOD - Minor preprocessing may help\",\n",
    "    2: \"  FAIR - Preprocessing recommended\",\n",
    "    1: \"  POOR - Significant preprocessing needed\",\n",
    "    0: \" CRITICAL - Data quality issues detected\"\n",
    "}\n",
    "\n",
    "print(f\"   • Overall quality: {quality_ratings[quality_score]}\")\n",
    "\n",
    "print(f\"\\nOutputs Generated:\")\n",
    "if SAVE_NPZ:\n",
    "    print(f\"    {os.path.join(OUTPUT_DIR, 'phase4_01_loaded_images.npz')}\")\n",
    "if SAVE_PLOTS:\n",
    "    print(f\"    {os.path.join(OUTPUT_DIR, 'phase4_01_sample_frames.png')}\")\n",
    "    print(f\"    {os.path.join(OUTPUT_DIR, 'phase4_01_temporal_analysis.png')}\")\n",
    "    print(f\"    {os.path.join(OUTPUT_DIR, 'phase4_01_spatiotemporal.png')}\")\n",
    "    print(f\"    {os.path.join(OUTPUT_DIR, 'phase4_01_quality_metrics.png')}\")\n",
    "\n",
    "print(f\"\\n Next Steps:\")\n",
    "print(f\"   → Run Phase 4.2: Extract 1D slices for SINDy analysis\")\n",
    "print(f\"   → Recommended extraction strategy: {'Averaged rows' if snr_db < 15 else 'Single middle row'}\")\n",
    "if snr_db < 10:\n",
    "    print(f\"   →   Consider aggressive denoising (Phase 4.5) before SINDy\")\n",
    "if mean_correlation < 0.5:\n",
    "    print(f\"   →   Check for registration issues (Phase 4.6)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" READY FOR PHASE 4.2!    \")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc40e66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('phase4_real_images/phase4_01_loaded_images.npz')\n",
    "u_images = data['u_images']  # (51, 2052, 2560)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1aa29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use frames 10-45 (skip weak early frames and potential outliers)\n",
    "frame_start = 10\n",
    "frame_end = 45\n",
    "u_selected = u_images[frame_start:frame_end+1, :, :]  # (36, 2052, 2560)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd045ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interior region (avoid boundaries)\n",
    "y_start, y_end = 800, 1200  # Central 400 rows\n",
    "x_start, x_end = 250, 2300  # Avoid edge artifacts\n",
    "\n",
    "u_interior = u_selected[:, y_start:y_end, x_start:x_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a754966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average rows to reduce noise\n",
    "u_1d = np.mean(u_interior, axis=1)  # (36, 2050) - time × space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41195615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import detrend\n",
    "u_1d_detrended = detrend(u_1d, axis=0, type='linear')  # Remove temporal drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b24fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nt, Nx = u_1d_detrended.shape\n",
    "t = np.arange(Nt)  # Time indices (will scale if TIME_PER_FRAME known)\n",
    "x = np.arange(Nx)  # Spatial indices\n",
    "\n",
    "# Create u(x,t) array\n",
    "u_real = u_1d_detrended  # This is your SINDy input!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd6ecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatiotemporal diagram\n",
    "plt.imshow(u_real, aspect='auto', cmap='gray')\n",
    "plt.xlabel('X Position (pixels)')\n",
    "plt.ylabel('Frame Number')\n",
    "plt.title('Extracted 1D Spatiotemporal Data (Averaged Rows)')\n",
    "plt.colorbar(label='Intensity')\n",
    "plt.savefig('phase4_02_extracted_1d_slices.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dcdee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('phase4_real_images/phase4_02_extracted_1d_slices.npz',\n",
    "                   u_real=u_real,\n",
    "                   t=t,\n",
    "                   x=x,\n",
    "                   Nt=Nt,\n",
    "                   Nx=Nx,\n",
    "                   frame_start=frame_start,\n",
    "                   frame_end=frame_end,\n",
    "                   y_region=(y_start, y_end),\n",
    "                   x_region=(x_start, x_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8180f42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PHASE 4.2B: COMPREHENSIVE VISUALIZATION & QUALITY METRICS\n",
    "FOR EXTRACTED 1D SPATIOTEMPORAL DATA\n",
    "\n",
    "This script performs detailed analysis of the extracted 1D slices to assess\n",
    "data quality, identify potential issues, and validate readiness for SINDy.\n",
    "\n",
    "Author: SINDy Project\n",
    "Phase: 4.2B - Data Quality Assessment\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal, stats\n",
    "from scipy.fft import fft, fftfreq\n",
    "import os\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (18, 12)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 4.2B: QUALITY METRICS FOR EXTRACTED 1D DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "INPUT_FILE = \"phase4_real_images/phase4_02_extracted_1d_slices.npz\"\n",
    "OUTPUT_DIR = \"phase4_real_images\"\n",
    "SAVE_PLOTS = True\n",
    "\n",
    "# ===== STEP 1: LOAD EXTRACTED DATA =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 1: LOADING EXTRACTED DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "data = np.load(INPUT_FILE)\n",
    "u_real = data['u_real']  # (Nt, Nx)\n",
    "t = data['t']\n",
    "x = data['x']\n",
    "Nt, Nx = u_real.shape\n",
    "\n",
    "print(f\"\\n✅ Loaded extracted 1D spatiotemporal data:\")\n",
    "print(f\"   Shape: {u_real.shape} (Nt={Nt}, Nx={Nx})\")\n",
    "print(f\"   Time range: {t[0]} to {t[-1]} ({len(t)} frames)\")\n",
    "print(f\"   Spatial range: {x[0]} to {x[-1]} ({len(x)} pixels)\")\n",
    "print(f\"   Intensity range: [{u_real.min():.2f}, {u_real.max():.2f}]\")\n",
    "print(f\"   Mean: {u_real.mean():.4f}, Std: {u_real.std():.4f}\")\n",
    "\n",
    "# ===== STEP 2: BASIC STATISTICS =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: STATISTICAL PROPERTIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Overall statistics\n",
    "print(f\"\\n📊 Overall Statistics:\")\n",
    "print(f\"   Min: {u_real.min():.4f}\")\n",
    "print(f\"   Max: {u_real.max():.4f}\")\n",
    "print(f\"   Mean: {u_real.mean():.4f}\")\n",
    "print(f\"   Median: {np.median(u_real):.4f}\")\n",
    "print(f\"   Std: {u_real.std():.4f}\")\n",
    "print(f\"   Range: {u_real.max() - u_real.min():.4f}\")\n",
    "\n",
    "# Temporal statistics\n",
    "temporal_means = np.mean(u_real, axis=1)\n",
    "temporal_stds = np.std(u_real, axis=1)\n",
    "temporal_mins = np.min(u_real, axis=1)\n",
    "temporal_maxs = np.max(u_real, axis=1)\n",
    "\n",
    "print(f\"\\n⏱️  Temporal Statistics:\")\n",
    "print(f\"   Mean over time: {temporal_means.mean():.4f} ± {temporal_means.std():.4f}\")\n",
    "print(f\"   Std over time: {temporal_stds.mean():.4f} ± {temporal_stds.std():.4f}\")\n",
    "print(f\"   Temporal drift: {temporal_means.max() - temporal_means.min():.4f}\")\n",
    "\n",
    "# Spatial statistics\n",
    "spatial_means = np.mean(u_real, axis=0)\n",
    "spatial_stds = np.std(u_real, axis=0)\n",
    "spatial_mins = np.min(u_real, axis=0)\n",
    "spatial_maxs = np.max(u_real, axis=0)\n",
    "\n",
    "print(f\"\\n📏 Spatial Statistics:\")\n",
    "print(f\"   Mean over space: {spatial_means.mean():.4f} ± {spatial_means.std():.4f}\")\n",
    "print(f\"   Std over space: {spatial_stds.mean():.4f} ± {spatial_stds.std():.4f}\")\n",
    "print(f\"   Spatial variation: {spatial_means.max() - spatial_means.min():.4f}\")\n",
    "\n",
    "# ===== STEP 3: NOISE ESTIMATION =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3: NOISE CHARACTERIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def estimate_noise_mad(data):\n",
    "    \"\"\"Estimate noise using Median Absolute Deviation\"\"\"\n",
    "    # Apply Laplacian for high-frequency noise extraction\n",
    "    laplacian = np.array([[1, -2, 1]])\n",
    "    noise_estimate = []\n",
    "    for row in data:\n",
    "        filtered = np.convolve(row, laplacian[0], mode='same')\n",
    "        sigma = np.median(np.abs(filtered)) / 0.6745\n",
    "        noise_estimate.append(sigma)\n",
    "    return np.array(noise_estimate)\n",
    "\n",
    "noise_per_frame = estimate_noise_mad(u_real)\n",
    "mean_noise = np.mean(noise_per_frame)\n",
    "std_noise = np.std(noise_per_frame)\n",
    "\n",
    "print(f\"\\n🔊 Noise Estimation (MAD method):\")\n",
    "print(f\"   Mean noise level: {mean_noise:.4f} ± {std_noise:.4f}\")\n",
    "print(f\"   Min noise: {noise_per_frame.min():.4f} (Frame {noise_per_frame.argmin()})\")\n",
    "print(f\"   Max noise: {noise_per_frame.max():.4f} (Frame {noise_per_frame.argmax()})\")\n",
    "\n",
    "# Signal-to-Noise Ratio\n",
    "signal_power = np.var(u_real)\n",
    "noise_power = mean_noise**2\n",
    "snr_db = 10 * np.log10(signal_power / noise_power)\n",
    "\n",
    "print(f\"\\n📡 Signal-to-Noise Ratio:\")\n",
    "print(f\"   Signal variance: {signal_power:.4f}\")\n",
    "print(f\"   Noise variance: {noise_power:.4f}\")\n",
    "print(f\"   SNR: {snr_db:.2f} dB\")\n",
    "\n",
    "if snr_db > 20:\n",
    "    print(f\"   → ✅ Excellent SNR (> 20 dB) - Ready for SINDy!\")\n",
    "elif snr_db > 10:\n",
    "    print(f\"   → ⚠️  Moderate SNR (10-20 dB) - Denoising recommended\")\n",
    "else:\n",
    "    print(f\"   → ❌ Low SNR (< 10 dB) - Denoising REQUIRED\")\n",
    "\n",
    "# ===== STEP 4: TEMPORAL CONSISTENCY =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 4: TEMPORAL CONSISTENCY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Frame-to-frame differences\n",
    "frame_diffs = np.diff(u_real, axis=0)\n",
    "mean_frame_diff = np.mean(np.abs(frame_diffs))\n",
    "max_frame_diff = np.max(np.abs(frame_diffs))\n",
    "\n",
    "print(f\"\\n📊 Frame-to-Frame Changes:\")\n",
    "print(f\"   Mean absolute difference: {mean_frame_diff:.4f}\")\n",
    "print(f\"   Max absolute difference: {max_frame_diff:.4f}\")\n",
    "print(f\"   Relative change: {mean_frame_diff/u_real.std():.4f} (as fraction of std)\")\n",
    "\n",
    "# Temporal correlation\n",
    "correlations = []\n",
    "for i in range(Nt - 1):\n",
    "    corr = np.corrcoef(u_real[i, :], u_real[i+1, :])[0, 1]\n",
    "    correlations.append(corr)\n",
    "correlations = np.array(correlations)\n",
    "mean_correlation = np.mean(correlations)\n",
    "\n",
    "print(f\"\\n🔗 Temporal Correlation:\")\n",
    "print(f\"   Mean correlation: {mean_correlation:.4f}\")\n",
    "print(f\"   Min correlation: {correlations.min():.4f} (Frame {correlations.argmin()} → {correlations.argmin()+1})\")\n",
    "print(f\"   Max correlation: {correlations.max():.4f} (Frame {correlations.argmax()} → {correlations.argmax()+1})\")\n",
    "\n",
    "if mean_correlation > 0.8:\n",
    "    print(f\"   → ✅ High correlation - Smooth evolution\")\n",
    "elif mean_correlation > 0.6:\n",
    "    print(f\"   → ⚠️  Moderate correlation - Some variation\")\n",
    "else:\n",
    "    print(f\"   → ❌ Low correlation - Check for discontinuities\")\n",
    "\n",
    "# Detect sudden jumps\n",
    "correlation_drops = np.where(correlations < 0.5)[0]\n",
    "if len(correlation_drops) > 0:\n",
    "    print(f\"\\n   ⚠️  WARNING: Low correlation detected at frames:\")\n",
    "    for idx in correlation_drops:\n",
    "        print(f\"      Frame {idx} → {idx+1}: correlation = {correlations[idx]:.3f}\")\n",
    "\n",
    "# ===== STEP 5: SPATIAL STRUCTURE ANALYSIS =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 5: SPATIAL STRUCTURE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Spatial gradients\n",
    "spatial_gradients = np.gradient(u_real, axis=1)\n",
    "mean_gradient = np.mean(np.abs(spatial_gradients))\n",
    "max_gradient = np.max(np.abs(spatial_gradients))\n",
    "\n",
    "print(f\"\\n∇ Spatial Gradients:\")\n",
    "print(f\"   Mean |∇u|: {mean_gradient:.4f}\")\n",
    "print(f\"   Max |∇u|: {max_gradient:.4f}\")\n",
    "\n",
    "if mean_gradient > 0.1:\n",
    "    print(f\"   → ✅ Strong spatial structure (good for derivatives)\")\n",
    "elif mean_gradient > 0.05:\n",
    "    print(f\"   → ⚠️  Moderate spatial structure\")\n",
    "else:\n",
    "    print(f\"   → ❌ Weak spatial structure (derivatives may be noisy)\")\n",
    "\n",
    "# Dominant wavelength estimation (FFT)\n",
    "print(f\"\\n🌊 Wavelength Analysis (using FFT):\")\n",
    "# Average over time to get mean spatial profile\n",
    "mean_spatial_profile = np.mean(u_real, axis=0)\n",
    "# Remove mean and apply window\n",
    "windowed = (mean_spatial_profile - mean_spatial_profile.mean()) * np.hanning(len(mean_spatial_profile))\n",
    "# FFT\n",
    "fft_vals = np.abs(fft(windowed))\n",
    "freqs = fftfreq(len(windowed), d=1.0)  # d=1 pixel\n",
    "# Only positive frequencies\n",
    "pos_freqs = freqs[:len(freqs)//2]\n",
    "pos_fft = fft_vals[:len(fft_vals)//2]\n",
    "# Find dominant frequency\n",
    "dominant_idx = np.argmax(pos_fft[1:]) + 1  # Skip DC component\n",
    "dominant_freq = pos_freqs[dominant_idx]\n",
    "dominant_wavelength = 1 / dominant_freq if dominant_freq > 0 else np.inf\n",
    "\n",
    "print(f\"   Dominant spatial frequency: {dominant_freq:.6f} cycles/pixel\")\n",
    "print(f\"   Dominant wavelength: {dominant_wavelength:.2f} pixels\")\n",
    "\n",
    "# Find all significant peaks\n",
    "peak_threshold = 0.3 * pos_fft[1:].max()\n",
    "peaks, _ = signal.find_peaks(pos_fft[1:], height=peak_threshold)\n",
    "if len(peaks) > 0:\n",
    "    print(f\"   Number of significant peaks: {len(peaks)}\")\n",
    "    # Compute wavelengths separately to avoid f-string formatting issues\n",
    "    peak_wavelengths = [1/pos_freqs[p+1] for p in peaks[:5]]\n",
    "    peak_wavelengths_str = ', '.join([f'{w:.1f}' for w in peak_wavelengths])\n",
    "    print(f\"   Peak wavelengths: [{peak_wavelengths_str}] pixels (top 5)\")\n",
    "    \n",
    "# ===== STEP 6: DISTRIBUTION ANALYSIS =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 6: INTENSITY DISTRIBUTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for normality\n",
    "_, p_value = stats.normaltest(u_real.flatten())\n",
    "is_normal = p_value > 0.05\n",
    "\n",
    "print(f\"\\n📈 Distribution Properties:\")\n",
    "print(f\"   Skewness: {stats.skew(u_real.flatten()):.4f}\")\n",
    "print(f\"   Kurtosis: {stats.kurtosis(u_real.flatten()):.4f}\")\n",
    "print(f\"   Normality test p-value: {p_value:.4e}\")\n",
    "if is_normal:\n",
    "    print(f\"   → ✅ Distribution is approximately normal\")\n",
    "else:\n",
    "    print(f\"   → ⚠️  Distribution is non-normal (may indicate artifacts)\")\n",
    "\n",
    "# Outlier detection\n",
    "q1, q3 = np.percentile(u_real.flatten(), [25, 75])\n",
    "iqr = q3 - q1\n",
    "outlier_threshold_low = q1 - 3 * iqr\n",
    "outlier_threshold_high = q3 + 3 * iqr\n",
    "outliers = (u_real < outlier_threshold_low) | (u_real > outlier_threshold_high)\n",
    "outlier_count = np.sum(outliers)\n",
    "outlier_percentage = 100 * outlier_count / u_real.size\n",
    "\n",
    "print(f\"\\n🎯 Outlier Detection (3×IQR method):\")\n",
    "print(f\"   IQR: {iqr:.4f}\")\n",
    "print(f\"   Outlier thresholds: [{outlier_threshold_low:.2f}, {outlier_threshold_high:.2f}]\")\n",
    "print(f\"   Outliers found: {outlier_count} ({outlier_percentage:.3f}%)\")\n",
    "\n",
    "if outlier_percentage < 0.1:\n",
    "    print(f\"   → ✅ Very few outliers\")\n",
    "elif outlier_percentage < 1.0:\n",
    "    print(f\"   → ⚠️  Some outliers present\")\n",
    "else:\n",
    "    print(f\"   → ❌ Many outliers - investigate data quality\")\n",
    "\n",
    "# ===== STEP 7: STATIONARITY CHECK =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 7: STATIONARITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Split data into chunks and compare statistics\n",
    "n_chunks = 4\n",
    "chunk_size = Nt // n_chunks\n",
    "chunk_means = []\n",
    "chunk_stds = []\n",
    "\n",
    "for i in range(n_chunks):\n",
    "    start = i * chunk_size\n",
    "    end = (i + 1) * chunk_size if i < n_chunks - 1 else Nt\n",
    "    chunk = u_real[start:end, :]\n",
    "    chunk_means.append(chunk.mean())\n",
    "    chunk_stds.append(chunk.std())\n",
    "\n",
    "mean_drift = max(chunk_means) - min(chunk_means)\n",
    "std_drift = max(chunk_stds) - min(chunk_stds)\n",
    "\n",
    "print(f\"\\n📊 Temporal Stationarity (split into {n_chunks} chunks):\")\n",
    "print(f\"   Chunk means: {[f'{m:.3f}' for m in chunk_means]}\")\n",
    "print(f\"   Chunk stds: {[f'{s:.3f}' for s in chunk_stds]}\")\n",
    "print(f\"   Mean drift: {mean_drift:.4f}\")\n",
    "print(f\"   Std drift: {std_drift:.4f}\")\n",
    "\n",
    "if mean_drift < 0.1 * u_real.std():\n",
    "    print(f\"   → ✅ Mean is stationary (drift < 10% of std)\")\n",
    "else:\n",
    "    print(f\"   → ⚠️  Significant mean drift detected\")\n",
    "\n",
    "if std_drift < 0.2 * u_real.std():\n",
    "    print(f\"   → ✅ Variance is stationary\")\n",
    "else:\n",
    "    print(f\"   → ⚠️  Variance changes over time\")\n",
    "\n",
    "# ===== STEP 8: DERIVATIVE READINESS =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 8: DERIVATIVE COMPUTATION READINESS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Estimate derivative accuracy by computing test derivatives\n",
    "print(f\"\\n🧮 Testing Derivative Computation:\")\n",
    "\n",
    "# Temporal derivative (simple finite difference)\n",
    "dt = 1.0  # Frame spacing\n",
    "u_t_test = np.diff(u_real, axis=0) / dt\n",
    "u_t_noise_estimate = estimate_noise_mad(u_t_test).mean()\n",
    "print(f\"   Temporal derivative u_t:\")\n",
    "print(f\"      Mean |u_t|: {np.mean(np.abs(u_t_test)):.4f}\")\n",
    "print(f\"      Estimated noise in u_t: {u_t_noise_estimate:.4f}\")\n",
    "\n",
    "# Spatial first derivative (central difference)\n",
    "dx = 1.0  # Pixel spacing\n",
    "u_x_test = np.gradient(u_real, axis=1) / dx\n",
    "u_x_noise_estimate = estimate_noise_mad(u_x_test).mean()\n",
    "print(f\"   Spatial derivative u_x:\")\n",
    "print(f\"      Mean |u_x|: {np.mean(np.abs(u_x_test)):.4f}\")\n",
    "print(f\"      Estimated noise in u_x: {u_x_noise_estimate:.4f}\")\n",
    "\n",
    "# Spatial second derivative\n",
    "u_xx_test = np.gradient(u_x_test, axis=1) / dx\n",
    "u_xx_noise_estimate = estimate_noise_mad(u_xx_test).mean()\n",
    "print(f\"   Spatial derivative u_xx:\")\n",
    "print(f\"      Mean |u_xx|: {np.mean(np.abs(u_xx_test)):.4f}\")\n",
    "print(f\"      Estimated noise in u_xx: {u_xx_noise_estimate:.4f}\")\n",
    "\n",
    "# SNR for derivatives\n",
    "u_t_snr = 10 * np.log10(np.var(u_t_test) / (u_t_noise_estimate**2))\n",
    "u_x_snr = 10 * np.log10(np.var(u_x_test) / (u_x_noise_estimate**2))\n",
    "u_xx_snr = 10 * np.log10(np.var(u_xx_test) / (u_xx_noise_estimate**2))\n",
    "\n",
    "print(f\"\\n📡 Derivative SNR Estimates:\")\n",
    "print(f\"   u_t SNR: {u_t_snr:.2f} dB\")\n",
    "print(f\"   u_x SNR: {u_x_snr:.2f} dB\")\n",
    "print(f\"   u_xx SNR: {u_xx_snr:.2f} dB\")\n",
    "\n",
    "if u_t_snr > 10 and u_x_snr > 10 and u_xx_snr > 10:\n",
    "    print(f\"   → ✅ All derivatives have acceptable SNR (> 10 dB)\")\n",
    "elif u_t_snr > 5 and u_x_snr > 5 and u_xx_snr > 5:\n",
    "    print(f\"   → ⚠️  Moderate derivative SNR (5-10 dB) - Denoising recommended\")\n",
    "else:\n",
    "    print(f\"   → ❌ Low derivative SNR (< 5 dB) - Denoising REQUIRED before SINDy\")\n",
    "\n",
    "# ===== COMPREHENSIVE QUALITY SCORE =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OVERALL DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "quality_score = 0\n",
    "max_score = 8\n",
    "\n",
    "# Criterion 1: SNR\n",
    "if snr_db > 20:\n",
    "    quality_score += 2\n",
    "    snr_status = \"✅ Excellent\"\n",
    "elif snr_db > 10:\n",
    "    quality_score += 1\n",
    "    snr_status = \"⚠️  Moderate\"\n",
    "else:\n",
    "    snr_status = \"❌ Poor\"\n",
    "\n",
    "# Criterion 2: Temporal correlation\n",
    "if mean_correlation > 0.8:\n",
    "    quality_score += 1\n",
    "    corr_status = \"✅ High\"\n",
    "elif mean_correlation > 0.6:\n",
    "    quality_score += 0.5\n",
    "    corr_status = \"⚠️  Moderate\"\n",
    "else:\n",
    "    corr_status = \"❌ Low\"\n",
    "\n",
    "# Criterion 3: Spatial gradients\n",
    "if mean_gradient > 0.1:\n",
    "    quality_score += 1\n",
    "    grad_status = \"✅ Strong\"\n",
    "elif mean_gradient > 0.05:\n",
    "    quality_score += 0.5\n",
    "    grad_status = \"⚠️  Moderate\"\n",
    "else:\n",
    "    grad_status = \"❌ Weak\"\n",
    "\n",
    "# Criterion 4: Outliers\n",
    "if outlier_percentage < 0.1:\n",
    "    quality_score += 1\n",
    "    outlier_status = \"✅ Minimal\"\n",
    "elif outlier_percentage < 1.0:\n",
    "    quality_score += 0.5\n",
    "    outlier_status = \"⚠️  Some\"\n",
    "else:\n",
    "    outlier_status = \"❌ Many\"\n",
    "\n",
    "# Criterion 5: Stationarity\n",
    "if mean_drift < 0.1 * u_real.std():\n",
    "    quality_score += 1\n",
    "    station_status = \"✅ Stationary\"\n",
    "else:\n",
    "    station_status = \"⚠️  Non-stationary\"\n",
    "\n",
    "# Criterion 6: Derivative SNR\n",
    "if u_t_snr > 10 and u_x_snr > 10:\n",
    "    quality_score += 2\n",
    "    deriv_status = \"✅ Good\"\n",
    "elif u_t_snr > 5 and u_x_snr > 5:\n",
    "    quality_score += 1\n",
    "    deriv_status = \"⚠️  Marginal\"\n",
    "else:\n",
    "    deriv_status = \"❌ Poor\"\n",
    "\n",
    "print(f\"\\n📊 Quality Criteria Summary:\")\n",
    "print(f\"   1. SNR: {snr_status} ({snr_db:.2f} dB)\")\n",
    "print(f\"   2. Temporal correlation: {corr_status} ({mean_correlation:.3f})\")\n",
    "print(f\"   3. Spatial gradients: {grad_status} ({mean_gradient:.4f})\")\n",
    "print(f\"   4. Outliers: {outlier_status} ({outlier_percentage:.2f}%)\")\n",
    "print(f\"   5. Stationarity: {station_status}\")\n",
    "print(f\"   6. Derivative SNR: {deriv_status}\")\n",
    "\n",
    "print(f\"\\n🎯 OVERALL QUALITY SCORE: {quality_score}/{max_score}\")\n",
    "\n",
    "if quality_score >= 7:\n",
    "    overall_assessment = \"✅ EXCELLENT - Ready for SINDy!\"\n",
    "    recommendation = \"Proceed directly to Phase 4.3 (Baseline SINDy)\"\n",
    "elif quality_score >= 5:\n",
    "    overall_assessment = \"✅ GOOD - SINDy should work with minor preprocessing\"\n",
    "    recommendation = \"Proceed to Phase 4.3, but keep Phase 4.5 (denoising) ready\"\n",
    "elif quality_score >= 3:\n",
    "    overall_assessment = \"⚠️  FAIR - Preprocessing recommended\"\n",
    "    recommendation = \"Apply denoising (Phase 4.5) before Phase 4.3\"\n",
    "else:\n",
    "    overall_assessment = \"❌ POOR - Significant preprocessing needed\"\n",
    "    recommendation = \"Apply aggressive denoising (Phase 4.5) and consider robust regression (Phase 4.7)\"\n",
    "\n",
    "print(f\"\\n📋 Assessment: {overall_assessment}\")\n",
    "print(f\"💡 Recommendation: {recommendation}\")\n",
    "\n",
    "# ===== STEP 9: COMPREHENSIVE VISUALIZATIONS =====\n",
    "if SAVE_PLOTS:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 9: GENERATING COMPREHENSIVE VISUALIZATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # === FIGURE 1: Basic Properties ===\n",
    "    print(\"\\n  Creating Figure 1: Basic properties...\")\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # Main spatiotemporal plot\n",
    "    ax1 = fig.add_subplot(gs[0:2, 0:2])\n",
    "    im1 = ax1.imshow(u_real, aspect='auto', cmap='seismic', \n",
    "                     extent=[x[0], x[-1], t[-1], t[0]], vmin=-30, vmax=30)\n",
    "    ax1.set_xlabel('X Position (pixels)', fontsize=12)\n",
    "    ax1.set_ylabel('Frame Number', fontsize=12)\n",
    "    ax1.set_title('Extracted 1D Spatiotemporal Data', fontsize=14, fontweight='bold')\n",
    "    plt.colorbar(im1, ax=ax1, label='Intensity')\n",
    "    \n",
    "    # Temporal mean evolution\n",
    "    ax2 = fig.add_subplot(gs[0, 2])\n",
    "    ax2.plot(t, temporal_means, 'b-', linewidth=2)\n",
    "    ax2.fill_between(t, temporal_means - temporal_stds, temporal_means + temporal_stds, \n",
    "                      alpha=0.3, color='blue')\n",
    "    ax2.set_xlabel('Frame Number', fontsize=10)\n",
    "    ax2.set_ylabel('Mean Intensity', fontsize=10)\n",
    "    ax2.set_title('Temporal Mean ± Std', fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Temporal std evolution\n",
    "    ax3 = fig.add_subplot(gs[0, 3])\n",
    "    ax3.plot(t, temporal_stds, 'r-', linewidth=2)\n",
    "    ax3.set_xlabel('Frame Number', fontsize=10)\n",
    "    ax3.set_ylabel('Std Intensity', fontsize=10)\n",
    "    ax3.set_title('Temporal Variability', fontsize=11)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Spatial mean profile\n",
    "    ax4 = fig.add_subplot(gs[1, 2])\n",
    "    ax4.plot(x, spatial_means, 'g-', linewidth=2)\n",
    "    ax4.fill_between(x, spatial_means - spatial_stds, spatial_means + spatial_stds,\n",
    "                      alpha=0.3, color='green')\n",
    "    ax4.set_xlabel('X Position (pixels)', fontsize=10)\n",
    "    ax4.set_ylabel('Mean Intensity', fontsize=10)\n",
    "    ax4.set_title('Spatial Mean ± Std', fontsize=11)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Spatial std profile\n",
    "    ax5 = fig.add_subplot(gs[1, 3])\n",
    "    ax5.plot(x, spatial_stds, 'orange', linewidth=2)\n",
    "    ax5.set_xlabel('X Position (pixels)', fontsize=10)\n",
    "    ax5.set_ylabel('Std Intensity', fontsize=10)\n",
    "    ax5.set_title('Spatial Variability', fontsize=11)\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Intensity distribution\n",
    "    ax6 = fig.add_subplot(gs[2, 0])\n",
    "    ax6.hist(u_real.flatten(), bins=100, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    ax6.axvline(u_real.mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "    ax6.axvline(np.median(u_real), color='green', linestyle='--', linewidth=2, label='Median')\n",
    "    ax6.set_xlabel('Intensity', fontsize=10)\n",
    "    ax6.set_ylabel('Frequency', fontsize=10)\n",
    "    ax6.set_title('Intensity Distribution', fontsize=11)\n",
    "    ax6.legend()\n",
    "    ax6.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Correlation evolution\n",
    "    ax7 = fig.add_subplot(gs[2, 1])\n",
    "    ax7.plot(t[:-1], correlations, 'purple', linewidth=2)\n",
    "    ax7.axhline(mean_correlation, color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Mean: {mean_correlation:.3f}')\n",
    "    ax7.set_xlabel('Frame Number', fontsize=10)\n",
    "    ax7.set_ylabel('Correlation', fontsize=10)\n",
    "    ax7.set_title('Frame-to-Frame Correlation', fontsize=11)\n",
    "    ax7.legend()\n",
    "    ax7.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Noise level evolution\n",
    "    ax8 = fig.add_subplot(gs[2, 2])\n",
    "    ax8.plot(t, noise_per_frame, 'brown', linewidth=2)\n",
    "    ax8.axhline(mean_noise, color='black', linestyle='--', linewidth=2,\n",
    "                label=f'Mean: {mean_noise:.3f}')\n",
    "    ax8.set_xlabel('Frame Number', fontsize=10)\n",
    "    ax8.set_ylabel('Noise Level (σ)', fontsize=10)\n",
    "    ax8.set_title('Estimated Noise Evolution', fontsize=11)\n",
    "    ax8.legend()\n",
    "    ax8.grid(True, alpha=0.3)\n",
    "    \n",
    "    # FFT power spectrum\n",
    "    ax9 = fig.add_subplot(gs[2, 3])\n",
    "    ax9.semilogy(pos_freqs[1:100], pos_fft[1:100], 'navy', linewidth=2)\n",
    "    if len(peaks) > 0:\n",
    "        for peak in peaks[:3]:\n",
    "            ax9.axvline(pos_freqs[peak+1], color='red', linestyle='--', alpha=0.5)\n",
    "    ax9.set_xlabel('Spatial Frequency (cycles/pixel)', fontsize=10)\n",
    "    ax9.set_ylabel('Power', fontsize=10)\n",
    "    ax9.set_title('Spatial Power Spectrum', fontsize=11)\n",
    "    ax9.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Comprehensive Quality Metrics - Extracted 1D Data', \n",
    "                 fontsize=16, fontweight='bold', y=0.995)\n",
    "    \n",
    "    plot_file = os.path.join(OUTPUT_DIR, 'phase4_02b_comprehensive_metrics.png')\n",
    "    plt.savefig(plot_file, dpi=150, bbox_inches='tight')\n",
    "    print(f\"  ✓ Saved: {plot_file}\")\n",
    "    plt.close()\n",
    "    \n",
    "    # === FIGURE 2: Sample Frames ===\n",
    "    print(\"  Creating Figure 2: Sample spatial profiles...\")\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Sample Spatial Profiles Over Time', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    sample_indices = np.linspace(0, Nt-1, 9, dtype=int)\n",
    "    \n",
    "    for idx, (ax, frame_idx) in enumerate(zip(axes.flat, sample_indices)):\n",
    "        ax.plot(x, u_real[frame_idx, :], linewidth=1.5, color='navy')\n",
    "        ax.axhline(0, color='red', linestyle='--', alpha=0.5, linewidth=1)\n",
    "        ax.set_xlabel('X Position (pixels)', fontsize=9)\n",
    "        ax.set_ylabel('Intensity', fontsize=9)\n",
    "        ax.set_title(f'Frame {frame_idx} (t={t[frame_idx]})', fontsize=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim([u_real.min()-5, u_real.max()+5])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_file = os.path.join(OUTPUT_DIR, 'phase4_02b_sample_profiles.png')\n",
    "    plt.savefig(plot_file, dpi=150, bbox_inches='tight')\n",
    "    print(f\"  ✓ Saved: {plot_file}\")\n",
    "    plt.close()\n",
    "    \n",
    "    # === FIGURE 3: Derivative Analysis ===\n",
    "    print(\"  Creating Figure 3: Derivative quality check...\")\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('Derivative Quality Assessment', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # u_t\n",
    "    im0 = axes[0, 0].imshow(u_t_test, aspect='auto', cmap='RdBu_r', \n",
    "                            extent=[x[0], x[-1], t[-2], t[0]])\n",
    "    axes[0, 0].set_title('Temporal Derivative (u_t)')\n",
    "    axes[0, 0].set_xlabel('X Position')\n",
    "    axes[0, 0].set_ylabel('Frame Number')\n",
    "    plt.colorbar(im0, ax=axes[0, 0])\n",
    "    \n",
    "    axes[1, 0].hist(u_t_test.flatten(), bins=100, color='red', alpha=0.7)\n",
    "    axes[1, 0].set_title(f'u_t Distribution (SNR: {u_t_snr:.1f} dB)')\n",
    "    axes[1, 0].set_xlabel('u_t')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # u_x\n",
    "    im1 = axes[0, 1].imshow(u_x_test, aspect='auto', cmap='RdBu_r',\n",
    "                            extent=[x[0], x[-1], t[-1], t[0]])\n",
    "    axes[0, 1].set_title('Spatial Derivative (u_x)')\n",
    "    axes[0, 1].set_xlabel('X Position')\n",
    "    axes[0, 1].set_ylabel('Frame Number')\n",
    "    plt.colorbar(im1, ax=axes[0, 1])\n",
    "    \n",
    "    axes[1, 1].hist(u_x_test.flatten(), bins=100, color='green', alpha=0.7)\n",
    "    axes[1, 1].set_title(f'u_x Distribution (SNR: {u_x_snr:.1f} dB)')\n",
    "    axes[1, 1].set_xlabel('u_x')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # u_xx\n",
    "    im2 = axes[0, 2].imshow(u_xx_test, aspect='auto', cmap='RdBu_r',\n",
    "                            extent=[x[0], x[-1], t[-1], t[0]])\n",
    "    axes[0, 2].set_title('Second Derivative (u_xx)')\n",
    "    axes[0, 2].set_xlabel('X Position')\n",
    "    axes[0, 2].set_ylabel('Frame Number')\n",
    "    plt.colorbar(im2, ax=axes[0, 2])\n",
    "    \n",
    "    axes[1, 2].hist(u_xx_test.flatten(), bins=100, color='blue', alpha=0.7)\n",
    "    axes[1, 2].set_title(f'u_xx Distribution (SNR: {u_xx_snr:.1f} dB)')\n",
    "    axes[1, 2].set_xlabel('u_xx')\n",
    "    axes[1, 2].set_ylabel('Frequency')\n",
    "    axes[1, 2].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_file = os.path.join(OUTPUT_DIR, 'phase4_02b_derivative_quality.png')\n",
    "    plt.savefig(plot_file, dpi=150, bbox_inches='tight')\n",
    "    print(f\"  ✓ Saved: {plot_file}\")\n",
    "    plt.close()\n",
    "    \n",
    "    # === FIGURE 4: Quality Summary Dashboard ===\n",
    "    print(\"  Creating Figure 4: Quality dashboard...\")\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # Quality scores radar/bar chart\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    criteria = ['SNR', 'Correlation', 'Gradients', 'Outliers', 'Stationarity', 'Deriv SNR']\n",
    "    scores_norm = [\n",
    "        min(snr_db/20, 1),\n",
    "        mean_correlation,\n",
    "        min(mean_gradient/0.1, 1),\n",
    "        1 - min(outlier_percentage/1.0, 1),\n",
    "        1 - min(mean_drift/(0.1*u_real.std()), 1),\n",
    "        min(min(u_t_snr, u_x_snr)/10, 1)\n",
    "    ]\n",
    "    colors_bar = ['green' if s > 0.8 else 'orange' if s > 0.5 else 'red' for s in scores_norm]\n",
    "    ax1.barh(criteria, scores_norm, color=colors_bar, alpha=0.7)\n",
    "    ax1.set_xlim([0, 1])\n",
    "    ax1.set_xlabel('Normalized Score', fontsize=10)\n",
    "    ax1.set_title('Quality Criteria Scores', fontsize=12, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Overall assessment text\n",
    "    ax2 = fig.add_subplot(gs[0, 1:])\n",
    "    ax2.axis('off')\n",
    "    summary_text = f\"\"\"\n",
    "DATA QUALITY SUMMARY\n",
    "{'='*60}\n",
    "\n",
    "Overall Score: {quality_score}/{max_score}\n",
    "Assessment: {overall_assessment}\n",
    "\n",
    "Key Metrics:\n",
    "  • Signal-to-Noise Ratio: {snr_db:.2f} dB\n",
    "  • Temporal Correlation: {mean_correlation:.4f}\n",
    "  • Spatial Gradient: {mean_gradient:.4f}\n",
    "  • Noise Level: {mean_noise:.4f}\n",
    "  • Outlier Percentage: {outlier_percentage:.3f}%\n",
    "  \n",
    "Data Properties:\n",
    "  • Shape: {u_real.shape} (Nt × Nx)\n",
    "  • Intensity Range: [{u_real.min():.2f}, {u_real.max():.2f}]\n",
    "  • Mean: {u_real.mean():.4f}, Std: {u_real.std():.4f}\n",
    "  • Dominant Wavelength: {dominant_wavelength:.1f} pixels\n",
    "\n",
    "Derivative Readiness:\n",
    "  • u_t SNR: {u_t_snr:.2f} dB\n",
    "  • u_x SNR: {u_x_snr:.2f} dB\n",
    "  • u_xx SNR: {u_xx_snr:.2f} dB\n",
    "\n",
    "RECOMMENDATION:\n",
    "{recommendation}\n",
    "    \"\"\"\n",
    "    ax2.text(0.05, 0.95, summary_text, fontsize=10, family='monospace',\n",
    "             verticalalignment='top', transform=ax2.transAxes,\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "    \n",
    "    # Temporal stability\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    chunk_x = np.arange(len(chunk_means))\n",
    "    ax3.plot(chunk_x, chunk_means, 'o-', linewidth=2, markersize=8, label='Mean', color='blue')\n",
    "    ax3_twin = ax3.twinx()\n",
    "    ax3_twin.plot(chunk_x, chunk_stds, 's-', linewidth=2, markersize=8, label='Std', color='red')\n",
    "    ax3.set_xlabel('Temporal Chunk', fontsize=10)\n",
    "    ax3.set_ylabel('Mean', fontsize=10, color='blue')\n",
    "    ax3_twin.set_ylabel('Std', fontsize=10, color='red')\n",
    "    ax3.set_title('Temporal Stationarity Check', fontsize=11)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.legend(loc='upper left')\n",
    "    ax3_twin.legend(loc='upper right')\n",
    "    \n",
    "    # Correlation heatmap (small subset)\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    # Compute correlation matrix for subset of frames\n",
    "    subset_size = min(20, Nt)\n",
    "    subset_indices = np.linspace(0, Nt-1, subset_size, dtype=int)\n",
    "    corr_matrix = np.corrcoef(u_real[subset_indices, :])\n",
    "    im = ax4.imshow(corr_matrix, cmap='RdYlGn', vmin=0, vmax=1, aspect='auto')\n",
    "    ax4.set_xlabel('Frame Index', fontsize=10)\n",
    "    ax4.set_ylabel('Frame Index', fontsize=10)\n",
    "    ax4.set_title('Frame Correlation Matrix', fontsize=11)\n",
    "    plt.colorbar(im, ax=ax4, label='Correlation')\n",
    "    \n",
    "    # Noise distribution\n",
    "    ax5 = fig.add_subplot(gs[1, 2])\n",
    "    ax5.plot(t, noise_per_frame, 'o-', linewidth=1.5, markersize=4, color='brown')\n",
    "    ax5.axhline(mean_noise, color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Mean: {mean_noise:.3f}')\n",
    "    ax5.fill_between(t, mean_noise - std_noise, mean_noise + std_noise,\n",
    "                      alpha=0.3, color='red', label=f'±1σ: {std_noise:.3f}')\n",
    "    ax5.set_xlabel('Frame Number', fontsize=10)\n",
    "    ax5.set_ylabel('Noise Level', fontsize=10)\n",
    "    ax5.set_title('Noise Consistency', fontsize=11)\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Quality Dashboard', fontsize=16, fontweight='bold', y=0.98)\n",
    "    \n",
    "    plot_file = os.path.join(OUTPUT_DIR, 'phase4_02b_quality_dashboard.png')\n",
    "    plt.savefig(plot_file, dpi=150, bbox_inches='tight')\n",
    "    print(f\"  ✓ Saved: {plot_file}\")\n",
    "    plt.close()\n",
    "\n",
    "# ===== FINAL SUMMARY =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 4.2B COMPLETE - QUALITY ASSESSMENT SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n✅ Analysis Complete:\")\n",
    "print(f\"   • Input: {INPUT_FILE}\")\n",
    "print(f\"   • Data shape: {u_real.shape} (Nt={Nt}, Nx={Nx})\")\n",
    "print(f\"   • Quality score: {quality_score}/{max_score}\")\n",
    "print(f\"   • Assessment: {overall_assessment}\")\n",
    "\n",
    "if SAVE_PLOTS:\n",
    "    print(f\"\\n📁 Generated Visualizations:\")\n",
    "    print(f\"   ✓ phase4_02b_comprehensive_metrics.png\")\n",
    "    print(f\"   ✓ phase4_02b_sample_profiles.png\")\n",
    "    print(f\"   ✓ phase4_02b_derivative_quality.png\")\n",
    "    print(f\"   ✓ phase4_02b_quality_dashboard.png\")\n",
    "\n",
    "print(f\"\\n🎯 Next Steps:\")\n",
    "print(f\"   {recommendation}\")\n",
    "\n",
    "if quality_score >= 5:\n",
    "    print(f\"\\n✅ DATA IS READY FOR PHASE 4.3 (BASELINE SINDY)\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  PREPROCESSING RECOMMENDED BEFORE PHASE 4.3\")\n",
    "    print(f\"   Consider applying Phase 4.5 (Denoising) first\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"🚀 READY TO PROCEED!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994462de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557efa79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8170d9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PHASE 4.1: BASELINE 2D SINDY (NO PREPROCESSING)\n",
    "\n",
    "Input: Raw 2D images (51, 2052, 2560)\n",
    "Output: Baseline coefficient recovery metrics\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Load raw images\n",
    "IMAGE_DIRECTORY = \"Real-Images\" \n",
    "images_raw = load_tiff_images(IMAGE_DIRECTORY)  # (51, 2052, 2560)\n",
    "\n",
    "# Spatially subsample (reduce computational cost)\n",
    "subsample_factor = 8\n",
    "images_subsampled = images_raw[:, ::subsample_factor, ::subsample_factor]\n",
    "# New shape: (51, 257, 320)\n",
    "\n",
    "# Apply 2D SINDy\n",
    "coeffs_baseline = apply_2d_sindy_fft(images_subsampled)\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    'method': 'baseline_no_preprocessing',\n",
    "    'coefficients': coeffs_baseline,\n",
    "    'rmse': compute_rmse(coeffs_baseline),\n",
    "    'r2': compute_r2(coeffs_baseline)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e312e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PHASE 4.1: BASELINE 2D SINDY (NO PREPROCESSING)\n",
    "\n",
    "Goal: Establish baseline coefficient recovery from raw 2D images\n",
    "Input: ZIP file containing TIFF images\n",
    "Output: Baseline metrics, visualizations, CSV results\n",
    "\n",
    "Author: SINDy Real Image Processing Project\n",
    "Date: Phase 4.1\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import os\n",
    "from sklearn.linear_model import Lasso\n",
    "from tifffile import imread\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (18, 12)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 4.1: BASELINE 2D SINDY - NO PREPROCESSING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "ZIP_FILE = \"path/to/your/Real_Images.zip\"  # UPDATE THIS PATH\n",
    "OUTPUT_DIR = \"phase4_results\"\n",
    "SUBSAMPLE_FACTOR = 8  # Reduce from (2052, 2560) to (257, 320)\n",
    "SINDY_ALPHA = 1e-4  # Lasso regularization parameter\n",
    "SAVE_PLOTS = True\n",
    "\n",
    "# True coefficients for 2D Kuramoto-Sivashinsky equation\n",
    "# u_t = a*u_xx + b*u_yy + c*u_xxxx + d*u_yyyy + e*u*u_x + f*u*u_y\n",
    "TRUE_COEFFS = np.array([-1.0, -1.0, -1.0, -1.0, -1.0, -1.0])\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ===== STEP 1: LOAD IMAGES FROM ZIP =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 1: LOADING IMAGES FROM ZIP FILE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def load_images_from_zip(zip_path):\n",
    "    \"\"\"\n",
    "    Load TIFF images from a ZIP file\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    zip_path : str\n",
    "        Path to ZIP file containing TIFF images\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    images : ndarray\n",
    "        Array of shape (Nt, H, W) containing all images\n",
    "    filenames : list\n",
    "        List of image filenames\n",
    "    \"\"\"\n",
    "    print(f\"\\n📦 Opening ZIP file: {zip_path}\")\n",
    "    \n",
    "    images = []\n",
    "    filenames = []\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        # Get all TIFF files in ZIP\n",
    "        tiff_files = [f for f in zip_ref.namelist() \n",
    "                      if f.lower().endswith(('.tif', '.tiff')) \n",
    "                      and not f.startswith('__MACOSX')]\n",
    "        \n",
    "        tiff_files = sorted(tiff_files)\n",
    "        \n",
    "        print(f\"   Found {len(tiff_files)} TIFF files\")\n",
    "        \n",
    "        # Extract to temporary directory and load\n",
    "        temp_dir = \"temp_extracted_images\"\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "        \n",
    "        for idx, tiff_file in enumerate(tiff_files):\n",
    "            # Extract file\n",
    "            zip_ref.extract(tiff_file, temp_dir)\n",
    "            \n",
    "            # Load image\n",
    "            img_path = os.path.join(temp_dir, tiff_file)\n",
    "            img = imread(img_path)\n",
    "            \n",
    "            images.append(img)\n",
    "            filenames.append(os.path.basename(tiff_file))\n",
    "            \n",
    "            if (idx + 1) % 10 == 0:\n",
    "                print(f\"   Loaded {idx + 1}/{len(tiff_files)} images...\")\n",
    "        \n",
    "        print(f\"   ✓ All images loaded successfully\")\n",
    "        \n",
    "        # Clean up temporary directory\n",
    "        import shutil\n",
    "        shutil.rmtree(temp_dir)\n",
    "    \n",
    "    images = np.array(images)\n",
    "    return images, filenames\n",
    "\n",
    "# Load images\n",
    "try:\n",
    "    images_raw, filenames = load_images_from_zip(ZIP_FILE)\n",
    "    Nt, H, W = images_raw.shape\n",
    "    \n",
    "    print(f\"\\n✅ Loaded raw images:\")\n",
    "    print(f\"   Number of frames (Nt): {Nt}\")\n",
    "    print(f\"   Image height (H): {H}\")\n",
    "    print(f\"   Image width (W): {W}\")\n",
    "    print(f\"   Total shape: {images_raw.shape}\")\n",
    "    print(f\"   Data type: {images_raw.dtype}\")\n",
    "    print(f\"   Intensity range: [{images_raw.min()}, {images_raw.max()}]\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"\\n❌ ERROR: ZIP file not found at: {ZIP_FILE}\")\n",
    "    print(f\"   Please update the ZIP_FILE path in the configuration section\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ ERROR loading images: {str(e)}\")\n",
    "    exit(1)\n",
    "\n",
    "# ===== STEP 2: SPATIAL SUBSAMPLING =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: SPATIAL SUBSAMPLING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n🔽 Subsampling by factor: {SUBSAMPLE_FACTOR}\")\n",
    "print(f\"   Original shape: ({Nt}, {H}, {W})\")\n",
    "\n",
    "images_subsampled = images_raw[:, ::SUBSAMPLE_FACTOR, ::SUBSAMPLE_FACTOR]\n",
    "Nt_sub, H_sub, W_sub = images_subsampled.shape\n",
    "\n",
    "print(f\"   Subsampled shape: ({Nt_sub}, {H_sub}, {W_sub})\")\n",
    "print(f\"   Reduction: {H*W} → {H_sub*W_sub} pixels per frame\")\n",
    "print(f\"   Total points for SINDy: {Nt_sub * H_sub * W_sub:,}\")\n",
    "\n",
    "# Normalize to zero mean (important for SINDy)\n",
    "images_normalized = images_subsampled - images_subsampled.mean()\n",
    "print(f\"\\n   ✓ Normalized to zero mean\")\n",
    "print(f\"   New intensity range: [{images_normalized.min():.2f}, {images_normalized.max():.2f}]\")\n",
    "\n",
    "# ===== STEP 3: COMPUTE 2D SPATIAL DERIVATIVES (FFT) =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3: COMPUTING 2D SPATIAL DERIVATIVES (FFT)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def compute_2d_fft_derivatives(u):\n",
    "    \"\"\"\n",
    "    Compute 2D spatial derivatives using FFT\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    u : ndarray\n",
    "        3D array of shape (Nt, Ny, Nx)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    derivatives : dict\n",
    "        Dictionary containing u_x, u_y, u_xx, u_yy, u_xxxx, u_yyyy\n",
    "    \"\"\"\n",
    "    Nt, Ny, Nx = u.shape\n",
    "    \n",
    "    print(f\"\\n🧮 Computing spatial derivatives for shape: ({Nt}, {Ny}, {Nx})\")\n",
    "    \n",
    "    # Wavenumbers (assuming dx = dy = 1.0 pixel)\n",
    "    ky = 2 * np.pi * np.fft.fftfreq(Ny, d=1.0)\n",
    "    kx = 2 * np.pi * np.fft.fftfreq(Nx, d=1.0)\n",
    "    \n",
    "    # Create 2D meshgrid\n",
    "    KY, KX = np.meshgrid(ky, kx, indexing='ij')\n",
    "    \n",
    "    print(f\"   Wavenumber ranges:\")\n",
    "    print(f\"      kx: [{kx.min():.4f}, {kx.max():.4f}]\")\n",
    "    print(f\"      ky: [{ky.min():.4f}, {ky.max():.4f}]\")\n",
    "    \n",
    "    # FFT of all frames\n",
    "    print(f\"   Computing 2D FFT for all {Nt} frames...\")\n",
    "    u_fft = np.fft.fft2(u, axes=(1, 2))\n",
    "    \n",
    "    # First derivatives\n",
    "    print(f\"   Computing first derivatives (u_x, u_y)...\")\n",
    "    u_x_fft = 1j * KX[np.newaxis, :, :] * u_fft\n",
    "    u_y_fft = 1j * KY[np.newaxis, :, :] * u_fft\n",
    "    \n",
    "    u_x = np.fft.ifft2(u_x_fft, axes=(1, 2)).real\n",
    "    u_y = np.fft.ifft2(u_y_fft, axes=(1, 2)).real\n",
    "    \n",
    "    # Second derivatives\n",
    "    print(f\"   Computing second derivatives (u_xx, u_yy)...\")\n",
    "    u_xx_fft = -(KX[np.newaxis, :, :]**2) * u_fft\n",
    "    u_yy_fft = -(KY[np.newaxis, :, :]**2) * u_fft\n",
    "    \n",
    "    u_xx = np.fft.ifft2(u_xx_fft, axes=(1, 2)).real\n",
    "    u_yy = np.fft.ifft2(u_yy_fft, axes=(1, 2)).real\n",
    "    \n",
    "    # Fourth derivatives\n",
    "    print(f\"   Computing fourth derivatives (u_xxxx, u_yyyy)...\")\n",
    "    u_xxxx_fft = (KX[np.newaxis, :, :]**4) * u_fft\n",
    "    u_yyyy_fft = (KY[np.newaxis, :, :]**4) * u_fft\n",
    "    \n",
    "    u_xxxx = np.fft.ifft2(u_xxxx_fft, axes=(1, 2)).real\n",
    "    u_yyyy = np.fft.ifft2(u_yyyy_fft, axes=(1, 2)).real\n",
    "    \n",
    "    print(f\"   ✓ All spatial derivatives computed\")\n",
    "    \n",
    "    derivatives = {\n",
    "        'u_x': u_x,\n",
    "        'u_y': u_y,\n",
    "        'u_xx': u_xx,\n",
    "        'u_yy': u_yy,\n",
    "        'u_xxxx': u_xxxx,\n",
    "        'u_yyyy': u_yyyy\n",
    "    }\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\n   Derivative statistics:\")\n",
    "    for name, deriv in derivatives.items():\n",
    "        print(f\"      {name:8s}: mean={deriv.mean():8.4f}, std={deriv.std():8.4f}, \"\n",
    "              f\"range=[{deriv.min():8.2f}, {deriv.max():8.2f}]\")\n",
    "    \n",
    "    return derivatives\n",
    "\n",
    "derivatives = compute_2d_fft_derivatives(images_normalized)\n",
    "\n",
    "# ===== STEP 4: COMPUTE TEMPORAL DERIVATIVE =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 4: COMPUTING TEMPORAL DERIVATIVE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n⏱️  Computing u_t using np.gradient...\")\n",
    "u_t = np.gradient(images_normalized, axis=0)\n",
    "\n",
    "print(f\"   u_t shape: {u_t.shape}\")\n",
    "print(f\"   u_t statistics:\")\n",
    "print(f\"      mean: {u_t.mean():.4f}\")\n",
    "print(f\"      std: {u_t.std():.4f}\")\n",
    "print(f\"      range: [{u_t.min():.2f}, {u_t.max():.2f}]\")\n",
    "\n",
    "# ===== STEP 5: BUILD LIBRARY MATRIX =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 5: BUILDING LIBRARY MATRIX (THETA)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n📚 Building library for 2D Kuramoto-Sivashinsky equation:\")\n",
    "print(f\"   u_t = a*u_xx + b*u_yy + c*u_xxxx + d*u_yyyy + e*u*u_x + f*u*u_y\")\n",
    "\n",
    "# Extract derivatives\n",
    "u = images_normalized\n",
    "u_x = derivatives['u_x']\n",
    "u_y = derivatives['u_y']\n",
    "u_xx = derivatives['u_xx']\n",
    "u_yy = derivatives['u_yy']\n",
    "u_xxxx = derivatives['u_xxxx']\n",
    "u_yyyy = derivatives['u_yyyy']\n",
    "\n",
    "# Build library matrix\n",
    "Theta = np.column_stack([\n",
    "    u_xx.flatten(),      # Column 0: u_xx\n",
    "    u_yy.flatten(),      # Column 1: u_yy\n",
    "    u_xxxx.flatten(),    # Column 2: u_xxxx\n",
    "    u_yyyy.flatten(),    # Column 3: u_yyyy\n",
    "    (u * u_x).flatten(), # Column 4: u*u_x\n",
    "    (u * u_y).flatten()  # Column 5: u*u_y\n",
    "])\n",
    "\n",
    "print(f\"\\n   Library matrix (Theta) shape: {Theta.shape}\")\n",
    "print(f\"   Number of data points: {Theta.shape[0]:,}\")\n",
    "print(f\"   Number of features: {Theta.shape[1]}\")\n",
    "print(f\"   Feature names: ['u_xx', 'u_yy', 'u_xxxx', 'u_yyyy', 'u*u_x', 'u*u_y']\")\n",
    "\n",
    "# Target vector\n",
    "y = u_t.flatten()\n",
    "print(f\"\\n   Target vector (u_t) shape: {y.shape}\")\n",
    "\n",
    "# Check for NaN/Inf\n",
    "if np.any(np.isnan(Theta)) or np.any(np.isinf(Theta)):\n",
    "    print(f\"\\n   ⚠️  WARNING: NaN or Inf detected in library matrix!\")\n",
    "    print(f\"      NaN count: {np.isnan(Theta).sum()}\")\n",
    "    print(f\"      Inf count: {np.isinf(Theta).sum()}\")\n",
    "else:\n",
    "    print(f\"\\n   ✓ No NaN or Inf values detected\")\n",
    "\n",
    "# ===== STEP 6: APPLY SINDY (LASSO REGRESSION) =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 6: APPLYING SINDY (LASSO REGRESSION)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n🔍 Fitting Lasso model with alpha={SINDY_ALPHA}\")\n",
    "\n",
    "lasso = Lasso(alpha=SINDY_ALPHA, max_iter=10000, tol=1e-4)\n",
    "lasso.fit(Theta, y)\n",
    "\n",
    "coeffs_baseline = lasso.coef_\n",
    "r2_baseline = lasso.score(Theta, y)\n",
    "\n",
    "print(f\"\\n   ✓ Model fitted successfully\")\n",
    "print(f\"   R² score: {r2_baseline:.6f}\")\n",
    "print(f\"   Number of iterations: {lasso.n_iter_}\")\n",
    "\n",
    "# ===== STEP 7: EVALUATE RESULTS =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 7: EVALUATING COEFFICIENT RECOVERY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Compute errors\n",
    "coeff_errors = np.abs(coeffs_baseline - TRUE_COEFFS)\n",
    "rmse = np.sqrt(np.mean((coeffs_baseline - TRUE_COEFFS)**2))\n",
    "mae = np.mean(coeff_errors)\n",
    "max_error = np.max(coeff_errors)\n",
    "\n",
    "print(f\"\\n📊 Coefficient Recovery Results:\")\n",
    "print(f\"\\n   {'Feature':<12} {'True':<10} {'Recovered':<12} {'Error':<10} {'Rel Error (%)':<15}\")\n",
    "print(f\"   {'-'*60}\")\n",
    "\n",
    "feature_names = ['u_xx', 'u_yy', 'u_xxxx', 'u_yyyy', 'u*u_x', 'u*u_y']\n",
    "for i, name in enumerate(feature_names):\n",
    "    true_val = TRUE_COEFFS[i]\n",
    "    rec_val = coeffs_baseline[i]\n",
    "    error = coeff_errors[i]\n",
    "    rel_error = (error / np.abs(true_val)) * 100 if true_val != 0 else np.inf\n",
    "    \n",
    "    print(f\"   {name:<12} {true_val:<10.4f} {rec_val:<12.4f} {error:<10.4f} {rel_error:<15.2f}\")\n",
    "\n",
    "print(f\"\\n   Overall Metrics:\")\n",
    "print(f\"      RMSE: {rmse:.6f}\")\n",
    "print(f\"      MAE:  {mae:.6f}\")\n",
    "print(f\"      Max Error: {max_error:.6f}\")\n",
    "print(f\"      R² Score: {r2_baseline:.6f}\")\n",
    "\n",
    "# Prediction\n",
    "y_pred = lasso.predict(Theta)\n",
    "prediction_rmse = np.sqrt(np.mean((y - y_pred)**2))\n",
    "print(f\"      Prediction RMSE: {prediction_rmse:.6f}\")\n",
    "\n",
    "# ===== STEP 8: SAVE RESULTS =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 8: SAVING RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = {\n",
    "    'method': 'baseline_no_preprocessing',\n",
    "    'subsample_factor': SUBSAMPLE_FACTOR,\n",
    "    'alpha': SINDY_ALPHA,\n",
    "    'r2_score': r2_baseline,\n",
    "    'rmse': rmse,\n",
    "    'mae': mae,\n",
    "    'max_error': max_error,\n",
    "    'prediction_rmse': prediction_rmse,\n",
    "    'coeff_u_xx': coeffs_baseline[0],\n",
    "    'coeff_u_yy': coeffs_baseline[1],\n",
    "    'coeff_u_xxxx': coeffs_baseline[2],\n",
    "    'coeff_u_yyyy': coeffs_baseline[3],\n",
    "    'coeff_u_ux': coeffs_baseline[4],\n",
    "    'coeff_u_uy': coeffs_baseline[5],\n",
    "    'error_u_xx': coeff_errors[0],\n",
    "    'error_u_yy': coeff_errors[1],\n",
    "    'error_u_xxxx': coeff_errors[2],\n",
    "    'error_u_yyyy': coeff_errors[3],\n",
    "    'error_u_ux': coeff_errors[4],\n",
    "    'error_u_uy': coeff_errors[5]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame([results])\n",
    "csv_file = os.path.join(OUTPUT_DIR, 'phase4_01_baseline_results.csv')\n",
    "results_df.to_csv(csv_file, index=False)\n",
    "print(f\"\\n   ✓ Results saved to: {csv_file}\")\n",
    "\n",
    "# Save detailed arrays\n",
    "npz_file = os.path.join(OUTPUT_DIR, 'phase4_01_baseline_data.npz')\n",
    "np.savez(npz_file,\n",
    "         images_raw=images_raw,\n",
    "         images_subsampled=images_normalized,\n",
    "         coefficients=coeffs_baseline,\n",
    "         true_coefficients=TRUE_COEFFS,\n",
    "         u_t=u_t,\n",
    "         **derivatives)\n",
    "print(f\"   ✓ Data arrays saved to: {npz_file}\")\n",
    "\n",
    "# ===== STEP 9: VISUALIZATIONS =====\n",
    "if SAVE_PLOTS:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 9: GENERATING VISUALIZATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # === FIGURE 1: Coefficient Recovery ===\n",
    "    print(\"\\n   Creating Figure 1: Coefficient recovery comparison...\")\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('Phase 4.1: Baseline Coefficient Recovery (No Preprocessing)', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i, (ax, name) in enumerate(zip(axes.flat, feature_names)):\n",
    "        x = ['True', 'Recovered']\n",
    "        y = [TRUE_COEFFS[i], coeffs_baseline[i]]\n",
    "        colors = ['green', 'red' if coeff_errors[i] > 0.1 else 'orange']\n",
    "        \n",
    "        bars = ax.bar(x, y, color=colors, alpha=0.7, edgecolor='black')\n",
    "        ax.axhline(TRUE_COEFFS[i], color='green', linestyle='--', \n",
    "                   linewidth=2, label='True Value')\n",
    "        ax.set_ylabel('Coefficient Value', fontsize=10)\n",
    "        ax.set_title(f'{name}\\nError: {coeff_errors[i]:.4f} '\n",
    "                     f'({(coeff_errors[i]/np.abs(TRUE_COEFFS[i])*100):.1f}%)',\n",
    "                     fontsize=11, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, val in zip(bars, y):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{val:.4f}',\n",
    "                   ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_file = os.path.join(OUTPUT_DIR, 'phase4_01_coefficient_recovery.png')\n",
    "    plt.savefig(plot_file, dpi=150, bbox_inches='tight')\n",
    "    print(f\"   ✓ Saved: {plot_file}\")\n",
    "    plt.close()\n",
    "    \n",
    "    # === FIGURE 2: Sample Images and Derivatives ===\n",
    "    print(\"   Creating Figure 2: Sample images and derivatives...\")\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(20, 12))\n",
    "    fig.suptitle('Phase 4.1: Sample Frames and Computed Derivatives', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Show 3 sample frames\n",
    "    sample_indices = [0, Nt//2, Nt-1]\n",
    "    \n",
    "    for row, idx in enumerate(sample_indices):\n",
    "        # Original image\n",
    "        im0 = axes[row, 0].imshow(images_normalized[idx], cmap='gray', aspect='auto')\n",
    "        axes[row, 0].set_title(f'Frame {idx}\\nOriginal', fontsize=10)\n",
    "        axes[row, 0].axis('off')\n",
    "        plt.colorbar(im0, ax=axes[row, 0])\n",
    "        \n",
    "        # u_xx\n",
    "        im1 = axes[row, 1].imshow(u_xx[idx], cmap='RdBu_r', aspect='auto')\n",
    "        axes[row, 1].set_title(f'u_xx', fontsize=10)\n",
    "        axes[row, 1].axis('off')\n",
    "        plt.colorbar(im1, ax=axes[row, 1])\n",
    "        \n",
    "        # u_xxxx\n",
    "        im2 = axes[row, 2].imshow(u_xxxx[idx], cmap='RdBu_r', aspect='auto')\n",
    "        axes[row, 2].set_title(f'u_xxxx', fontsize=10)\n",
    "        axes[row, 2].axis('off')\n",
    "        plt.colorbar(im2, ax=axes[row, 2])\n",
    "        \n",
    "        # u*u_x (nonlinear term)\n",
    "        im3 = axes[row, 3].imshow((u*u_x)[idx], cmap='RdBu_r', aspect='auto')\n",
    "        axes[row, 3].set_title(f'u*u_x', fontsize=10)\n",
    "        axes[row, 3].axis('off')\n",
    "        plt.colorbar(im3, ax=axes[row, 3])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_file = os.path.join(OUTPUT_DIR, 'phase4_01_sample_derivatives.png')\n",
    "    plt.savefig(plot_file, dpi=150, bbox_inches='tight')\n",
    "    print(f\"   ✓ Saved: {plot_file}\")\n",
    "    plt.close()\n",
    "    \n",
    "    # === FIGURE 3: Prediction Quality ===\n",
    "    print(\"   Creating Figure 3: Prediction quality assessment...\")\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    fig.suptitle('Phase 4.1: SINDy Prediction Quality', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Scatter plot: True vs Predicted\n",
    "    axes[0].scatter(y, y_pred, alpha=0.1, s=1)\n",
    "    axes[0].plot([y.min(), y.max()], [y.min(), y.max()], \n",
    "                 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    axes[0].set_xlabel('True u_t', fontsize=11)\n",
    "    axes[0].set_ylabel('Predicted u_t', fontsize=11)\n",
    "    axes[0].set_title(f'True vs Predicted\\nR² = {r2_baseline:.4f}', fontsize=12)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residuals\n",
    "    residuals = y - y_pred\n",
    "    axes[1].hist(residuals, bins=100, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    axes[1].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[1].set_xlabel('Residual (True - Predicted)', fontsize=11)\n",
    "    axes[1].set_ylabel('Frequency', fontsize=11)\n",
    "    axes[1].set_title(f'Residual Distribution\\nRMSE = {prediction_rmse:.6f}', fontsize=12)\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Sample temporal predictions\n",
    "    sample_idx = H_sub // 2\n",
    "    sample_row = slice(sample_idx, sample_idx + 1)\n",
    "    u_t_sample = u_t[:, sample_row, :].flatten()\n",
    "    y_pred_sample = y_pred.reshape(u_t.shape)[:, sample_row, :].flatten()\n",
    "    \n",
    "    axes[2].plot(u_t_sample[:500], 'b-', linewidth=1.5, label='True u_t', alpha=0.7)\n",
    "    axes[2].plot(y_pred_sample[:500], 'r--', linewidth=1.5, label='Predicted u_t', alpha=0.7)\n",
    "    axes[2].set_xlabel('Spatial Point', fontsize=11)\n",
    "    axes[2].set_ylabel('u_t', fontsize=11)\n",
    "    axes[2].set_title('Sample Temporal Derivative', fontsize=12)\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_file = os.path.join(OUTPUT_DIR, 'phase4_01_prediction_quality.png')\n",
    "    plt.savefig(plot_file, dpi=150, bbox_inches='tight')\n",
    "    print(f\"   ✓ Saved: {plot_file}\")\n",
    "    plt.close()\n",
    "    \n",
    "    # === FIGURE 4: Summary Dashboard ===\n",
    "    print(\"   Creating Figure 4: Summary dashboard...\")\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # Coefficient comparison\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    x_pos = np.arange(len(feature_names))\n",
    "    width = 0.35\n",
    "    ax1.bar(x_pos - width/2, TRUE_COEFFS, width, label='True', \n",
    "            color='green', alpha=0.7, edgecolor='black')\n",
    "    ax1.bar(x_pos + width/2, coeffs_baseline, width, label='Recovered',\n",
    "            color='red', alpha=0.7, edgecolor='black')\n",
    "    ax1.set_xlabel('Coefficient', fontsize=11)\n",
    "    ax1.set_ylabel('Value', fontsize=11)\n",
    "    ax1.set_title('Coefficient Comparison', fontsize=12, fontweight='bold')\n",
    "    ax1.set_xticks(x_pos)\n",
    "    ax1.set_xticklabels(feature_names, rotation=45, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    ax1.axhline(0, color='black', linewidth=0.5)\n",
    "    \n",
    "    # Error bars\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    colors_err = ['green' if e < 0.05 else 'orange' if e < 0.1 else 'red' \n",
    "                  for e in coeff_errors]\n",
    "    bars = ax2.barh(feature_names, coeff_errors, color=colors_err, \n",
    "                     alpha=0.7, edgecolor='black')\n",
    "    ax2.set_xlabel('Absolute Error', fontsize=11)\n",
    "    ax2.set_title('Coefficient Errors', fontsize=12, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Add error values on bars\n",
    "    for bar, err in zip(bars, coeff_errors):\n",
    "        width = bar.get_width()\n",
    "        ax2.text(width, bar.get_y() + bar.get_height()/2,\n",
    "                f' {err:.4f}',\n",
    "                ha='left', va='center', fontsize=9)\n",
    "    \n",
    "    # Summary text\n",
    "    ax3 = fig.add_subplot(gs[1, :])\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "PHASE 4.1: BASELINE 2D SINDY RESULTS (NO PREPROCESSING)\n",
    "{'='*80}\n",
    "\n",
    "INPUT DATA:\n",
    "  • Original images: {Nt} frames of {H} × {W} pixels\n",
    "  • Subsampled: {Nt_sub} frames of {H_sub} × {W_sub} pixels\n",
    "  • Subsample factor: {SUBSAMPLE_FACTOR}\n",
    "  • Total data points: {Nt_sub * H_sub * W_sub:,}\n",
    "\n",
    "SINDY CONFIGURATION:\n",
    "  • Regularization (α): {SINDY_ALPHA}\n",
    "  • Method: Lasso regression\n",
    "  • Library features: 6 (u_xx, u_yy, u_xxxx, u_yyyy, u*u_x, u*u_y)\n",
    "\n",
    "COEFFICIENT RECOVERY:\n",
    "  • RMSE: {rmse:.6f}\n",
    "  • MAE:  {mae:.6f}\n",
    "  • Max Error: {max_error:.6f}\n",
    "  • R² Score: {r2_baseline:.6f}\n",
    "\n",
    "PREDICTION QUALITY:\n",
    "  • Prediction RMSE: {prediction_rmse:.6f}\n",
    "  • R² Score: {r2_baseline:.6f}\n",
    "\n",
    "ASSESSMENT:\n",
    "  {'✅ GOOD' if rmse < 0.1 else '⚠️  MODERATE' if rmse < 0.3 else '❌ POOR'} - Baseline coefficient recovery\n",
    "  {'✅ GOOD' if r2_baseline > 0.8 else '⚠️  MODERATE' if r2_baseline > 0.5 else '❌ POOR'} - Model fit quality\n",
    "\n",
    "NEXT STEPS:\n",
    "  → Phase 4.2: Apply registration methods\n",
    "  → Phase 4.3: Apply denoising methods\n",
    "  → Compare improvements over baseline\n",
    "    \"\"\"\n",
    "    \n",
    "    ax3.text(0.05, 0.95, summary_text, fontsize=10, family='monospace',\n",
    "             verticalalignment='top', transform=ax3.transAxes,\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "    \n",
    "    plt.suptitle('Phase 4.1: Baseline Summary Dashboard', \n",
    "                 fontsize=16, fontweight='bold', y=0.98)\n",
    "    \n",
    "    plot_file = os.path.join(OUTPUT_DIR, 'phase4_01_summary_dashboard.png')\n",
    "    plt.savefig(plot_file, dpi=150, bbox_inches='tight')\n",
    "    print(f\"   ✓ Saved: {plot_file}\")\n",
    "    plt.close()\n",
    "\n",
    "# ===== FINAL SUMMARY =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 4.1 COMPLETE - BASELINE 2D SINDY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n✅ Successfully completed baseline analysis:\")\n",
    "print(f\"   • Loaded {Nt} images from ZIP file\")\n",
    "print(f\"   • Subsampled to ({Nt_sub}, {H_sub}, {W_sub})\")\n",
    "print(f\"   • Computed 2D spatial derivatives (FFT)\")\n",
    "print(f\"   • Applied SINDy with Lasso regression\")\n",
    "print(f\"   • Coefficient RMSE: {rmse:.6f}\")\n",
    "print(f\"   • Model R² score: {r2_baseline:.6f}\")\n",
    "\n",
    "print(f\"\\n📁 Output files saved to: {OUTPUT_DIR}/\")\n",
    "print(f\"   ✓ phase4_01_baseline_results.csv\")\n",
    "print(f\"   ✓ phase4_01_baseline_data.npz\")\n",
    "\n",
    "if SAVE_PLOTS:\n",
    "    print(f\"   ✓ phase4_01_coefficient_recovery.png\")\n",
    "    print(f\"   ✓ phase4_01_sample_derivatives.png\")\n",
    "    print(f\"   ✓ phase4_01_prediction_quality.png\")\n",
    "    print(f\"   ✓ phase4_01_summary_dashboard.png\")\n",
    "\n",
    "assessment = \"GOOD\" if rmse < 0.1 else \"MODERATE\" if rmse < 0.3 else \"POOR\"\n",
    "print(f\"\\n🎯 Overall Assessment: {assessment}\")\n",
    "\n",
    "if rmse < 0.1:\n",
    "    print(f\"   ✅ Baseline performance is excellent!\")\n",
    "    print(f\"   → Registration and denoising may provide marginal improvements\")\n",
    "elif rmse < 0.3:\n",
    "    print(f\"   ⚠️  Baseline performance is moderate\")\n",
    "    print(f\"   → Registration and denoising should improve results significantly\")\n",
    "else:\n",
    "    print(f\"   ❌ Baseline performance is poor\")\n",
    "    print(f\"   → Preprocessing (registration + denoising) is ESSENTIAL\")\n",
    "\n",
    "print(f\"\\n🚀 Ready for Phase 4.2: Registration Methods\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6b88e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PHASE 4.1: BASELINE 2D SINDY (NO PREPROCESSING)\n",
    "\n",
    "Goal: Establish baseline coefficient recovery from raw 2D images\n",
    "Input: ZIP file containing TIFF images\n",
    "Output: Baseline metrics, visualizations, CSV results\n",
    "\n",
    "Author: SINDy Real Image Processing Project\n",
    "Date: Phase 4.1\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import os\n",
    "from sklearn.linear_model import Lasso\n",
    "from tifffile import imread\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (18, 12)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 4.1: BASELINE 2D SINDY - NO PREPROCESSING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "ZIP_FILE = \"path/to/your/Real_Images.zip\"  # UPDATE THIS PATH\n",
    "OUTPUT_DIR = \"phase4_results\"\n",
    "SUBSAMPLE_FACTOR = 8  # Reduce from (2052, 2560) to (257, 320)\n",
    "SINDY_ALPHA = 1e-4  # Lasso regularization parameter\n",
    "SAVE_PLOTS = True\n",
    "\n",
    "# True coefficients for 2D Kuramoto-Sivashinsky equation\n",
    "# u_t = a*u_xx + b*u_yy + c*u_xxxx + d*u_yyyy + e*u*u_x + f*u*u_y\n",
    "TRUE_COEFFS = np.array([-1.0, -1.0, -1.0, -1.0, -1.0, -1.0])\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ===== STEP 1: LOAD IMAGES FROM ZIP =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 1: LOADING IMAGES FROM ZIP FILE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def load_images_from_zip(zip_path):\n",
    "    \"\"\"\n",
    "    Load TIFF images from a ZIP file\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    zip_path : str\n",
    "        Path to ZIP file containing TIFF images\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    images : ndarray\n",
    "        Array of shape (Nt, H, W) containing all images\n",
    "    filenames : list\n",
    "        List of image filenames\n",
    "    \"\"\"\n",
    "    print(f\"\\n📦 Opening ZIP file: {zip_path}\")\n",
    "    \n",
    "    images = []\n",
    "    filenames = []\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        # Get all TIFF files in ZIP\n",
    "        tiff_files = [f for f in zip_ref.namelist() \n",
    "                      if f.lower().endswith(('.tif', '.tiff')) \n",
    "                      and not f.startswith('__MACOSX')]\n",
    "        \n",
    "        tiff_files = sorted(tiff_files)\n",
    "        \n",
    "        print(f\"   Found {len(tiff_files)} TIFF files\")\n",
    "        \n",
    "        # Extract to temporary directory and load\n",
    "        temp_dir = \"temp_extracted_images\"\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "        \n",
    "        for idx, tiff_file in enumerate(tiff_files):\n",
    "            # Extract file\n",
    "            zip_ref.extract(tiff_file, temp_dir)\n",
    "            \n",
    "            # Load image\n",
    "            img_path = os.path.join(temp_dir, tiff_file)\n",
    "            img = imread(img_path)\n",
    "            \n",
    "            images.append(img)\n",
    "            filenames.append(os.path.basename(tiff_file))\n",
    "            \n",
    "            if (idx + 1) % 10 == 0:\n",
    "                print(f\"   Loaded {idx + 1}/{len(tiff_files)} images...\")\n",
    "        \n",
    "        print(f\"   ✓ All images loaded successfully\")\n",
    "        \n",
    "        # Clean up temporary directory\n",
    "        import shutil\n",
    "        shutil.rmtree(temp_dir)\n",
    "    \n",
    "    images = np.array(images)\n",
    "    return images, filenames\n",
    "\n",
    "# Load images\n",
    "try:\n",
    "    images_raw, filenames = load_images_from_zip(ZIP_FILE)\n",
    "    Nt, H, W = images_raw.shape\n",
    "    \n",
    "    print(f\"\\n✅ Loaded raw images:\")\n",
    "    print(f\"   Number of frames (Nt): {Nt}\")\n",
    "    print(f\"   Image height (H): {H}\")\n",
    "    print(f\"   Image width (W): {W}\")\n",
    "    print(f\"   Total shape: {images_raw.shape}\")\n",
    "    print(f\"   Data type: {images_raw.dtype}\")\n",
    "    print(f\"   Intensity range: [{images_raw.min()}, {images_raw.max()}]\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"\\n❌ ERROR: ZIP file not found at: {ZIP_FILE}\")\n",
    "    print(f\"   Please update the ZIP_FILE path in the configuration section\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ ERROR loading images: {str(e)}\")\n",
    "    exit(1)\n",
    "\n",
    "# ===== STEP 2: SPATIAL SUBSAMPLING =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: SPATIAL SUBSAMPLING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n🔽 Subsampling by factor: {SUBSAMPLE_FACTOR}\")\n",
    "print(f\"   Original shape: ({Nt}, {H}, {W})\")\n",
    "\n",
    "images_subsampled = images_raw[:, ::SUBSAMPLE_FACTOR, ::SUBSAMPLE_FACTOR]\n",
    "Nt_sub, H_sub, W_sub = images_subsampled.shape\n",
    "\n",
    "print(f\"   Subsampled shape: ({Nt_sub}, {H_sub}, {W_sub})\")\n",
    "print(f\"   Reduction: {H*W} → {H_sub*W_sub} pixels per frame\")\n",
    "print(f\"   Total points for SINDy: {Nt_sub * H_sub * W_sub:,}\")\n",
    "\n",
    "# Normalize to zero mean (important for SINDy)\n",
    "images_normalized = images_subsampled - images_subsampled.mean()\n",
    "print(f\"\\n   ✓ Normalized to zero mean\")\n",
    "print(f\"   New intensity range: [{images_normalized.min():.2f}, {images_normalized.max():.2f}]\")\n",
    "\n",
    "# ===== STEP 3: COMPUTE 2D SPATIAL DERIVATIVES (FFT) =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3: COMPUTING 2D SPATIAL DERIVATIVES (FFT)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def compute_2d_fft_derivatives(u):\n",
    "    \"\"\"\n",
    "    Compute 2D spatial derivatives using FFT\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    u : ndarray\n",
    "        3D array of shape (Nt, Ny, Nx)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    derivatives : dict\n",
    "        Dictionary containing u_x, u_y, u_xx, u_yy, u_xxxx, u_yyyy\n",
    "    \"\"\"\n",
    "    Nt, Ny, Nx = u.shape\n",
    "    \n",
    "    print(f\"\\n🧮 Computing spatial derivatives for shape: ({Nt}, {Ny}, {Nx})\")\n",
    "    \n",
    "    # Wavenumbers (assuming dx = dy = 1.0 pixel)\n",
    "    ky = 2 * np.pi * np.fft.fftfreq(Ny, d=1.0)\n",
    "    kx = 2 * np.pi * np.fft.fftfreq(Nx, d=1.0)\n",
    "    \n",
    "    # Create 2D meshgrid\n",
    "    KY, KX = np.meshgrid(ky, kx, indexing='ij')\n",
    "    \n",
    "    print(f\"   Wavenumber ranges:\")\n",
    "    print(f\"      kx: [{kx.min():.4f}, {kx.max():.4f}]\")\n",
    "    print(f\"      ky: [{ky.min():.4f}, {ky.max():.4f}]\")\n",
    "    \n",
    "    # FFT of all frames\n",
    "    print(f\"   Computing 2D FFT for all {Nt} frames...\")\n",
    "    u_fft = np.fft.fft2(u, axes=(1, 2))\n",
    "    \n",
    "    # First derivatives\n",
    "    print(f\"   Computing first derivatives (u_x, u_y)...\")\n",
    "    u_x_fft = 1j * KX[np.newaxis, :, :] * u_fft\n",
    "    u_y_fft = 1j * KY[np.newaxis, :, :] * u_fft\n",
    "    \n",
    "    u_x = np.fft.ifft2(u_x_fft, axes=(1, 2)).real\n",
    "    u_y = np.fft.ifft2(u_y_fft, axes=(1, 2)).real\n",
    "    \n",
    "    # Second derivatives\n",
    "    print(f\"   Computing second derivatives (u_xx, u_yy)...\")\n",
    "    u_xx_fft = -(KX[np.newaxis, :, :]**2) * u_fft\n",
    "    u_yy_fft = -(KY[np.newaxis, :, :]**2) * u_fft\n",
    "    \n",
    "    u_xx = np.fft.ifft2(u_xx_fft, axes=(1, 2)).real\n",
    "    u_yy = np.fft.ifft2(u_yy_fft, axes=(1, 2)).real\n",
    "    \n",
    "    # Fourth derivatives\n",
    "    print(f\"   Computing fourth derivatives (u_xxxx, u_yyyy)...\")\n",
    "    u_xxxx_fft = (KX[np.newaxis, :, :]**4) * u_fft\n",
    "    u_yyyy_fft = (KY[np.newaxis, :, :]**4) * u_fft\n",
    "    \n",
    "    u_xxxx = np.fft.ifft2(u_xxxx_fft, axes=(1, 2)).real\n",
    "    u_yyyy = np.fft.ifft2(u_yyyy_fft, axes=(1, 2)).real\n",
    "    \n",
    "    print(f\"   ✓ All spatial derivatives computed\")\n",
    "    \n",
    "    derivatives = {\n",
    "        'u_x': u_x,\n",
    "        'u_y': u_y,\n",
    "        'u_xx': u_xx,\n",
    "        'u_yy': u_yy,\n",
    "        'u_xxxx': u_xxxx,\n",
    "        'u_yyyy': u_yyyy\n",
    "    }\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\n   Derivative statistics:\")\n",
    "    for name, deriv in derivatives.items():\n",
    "        print(f\"      {name:8s}: mean={deriv.mean():8.4f}, std={deriv.std():8.4f}, \"\n",
    "              f\"range=[{deriv.min():8.2f}, {deriv.max():8.2f}]\")\n",
    "    \n",
    "    return derivatives\n",
    "\n",
    "derivatives = compute_2d_fft_derivatives(images_normalized)\n",
    "\n",
    "# ===== STEP 4: COMPUTE TEMPORAL DERIVATIVE =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 4: COMPUTING TEMPORAL DERIVATIVE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n⏱️  Computing u_t using np.gradient...\")\n",
    "u_t = np.gradient(images_normalized, axis=0)\n",
    "\n",
    "print(f\"   u_t shape: {u_t.shape}\")\n",
    "print(f\"   u_t statistics:\")\n",
    "print(f\"      mean: {u_t.mean():.4f}\")\n",
    "print(f\"      std: {u_t.std():.4f}\")\n",
    "print(f\"      range: [{u_t.min():.2f}, {u_t.max():.2f}]\")\n",
    "\n",
    "# ===== STEP 5: BUILD LIBRARY MATRIX =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 5: BUILDING LIBRARY MATRIX (THETA)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n📚 Building library for 2D Kuramoto-Sivashinsky equation:\")\n",
    "print(f\"   u_t = a*u_xx + b*u_yy + c*u_xxxx + d*u_yyyy + e*u*u_x + f*u*u_y\")\n",
    "\n",
    "# Extract derivatives\n",
    "u = images_normalized\n",
    "u_x = derivatives['u_x']\n",
    "u_y = derivatives['u_y']\n",
    "u_xx = derivatives['u_xx']\n",
    "u_yy = derivatives['u_yy']\n",
    "u_xxxx = derivatives['u_xxxx']\n",
    "u_yyyy = derivatives['u_yyyy']\n",
    "\n",
    "# Build library matrix\n",
    "Theta = np.column_stack([\n",
    "    u_xx.flatten(),      # Column 0: u_xx\n",
    "    u_yy.flatten(),      # Column 1: u_yy\n",
    "    u_xxxx.flatten(),    # Column 2: u_xxxx\n",
    "    u_yyyy.flatten(),    # Column 3: u_yyyy\n",
    "    (u * u_x).flatten(), # Column 4: u*u_x\n",
    "    (u * u_y).flatten()  # Column 5: u*u_y\n",
    "])\n",
    "\n",
    "print(f\"\\n   Library matrix (Theta) shape: {Theta.shape}\")\n",
    "print(f\"   Number of data points: {Theta.shape[0]:,}\")\n",
    "print(f\"   Number of features: {Theta.shape[1]}\")\n",
    "print(f\"   Feature names: ['u_xx', 'u_yy', 'u_xxxx', 'u_yyyy', 'u*u_x', 'u*u_y']\")\n",
    "\n",
    "# Target vector\n",
    "y = u_t.flatten()\n",
    "print(f\"\\n   Target vector (u_t) shape: {y.shape}\")\n",
    "\n",
    "# Check for NaN/Inf\n",
    "if np.any(np.isnan(Theta)) or np.any(np.isinf(Theta)):\n",
    "    print(f\"\\n   ⚠️  WARNING: NaN or Inf detected in library matrix!\")\n",
    "    print(f\"      NaN count: {np.isnan(Theta).sum()}\")\n",
    "    print(f\"      Inf count: {np.isinf(Theta).sum()}\")\n",
    "else:\n",
    "    print(f\"\\n   ✓ No NaN or Inf values detected\")\n",
    "\n",
    "# ===== STEP 6: APPLY SINDY (LASSO REGRESSION) =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 6: APPLYING SINDY (LASSO REGRESSION)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n🔍 Fitting Lasso model with alpha={SINDY_ALPHA}\")\n",
    "\n",
    "lasso = Lasso(alpha=SINDY_ALPHA, max_iter=10000, tol=1e-4)\n",
    "lasso.fit(Theta, y)\n",
    "\n",
    "coeffs_baseline = lasso.coef_\n",
    "r2_baseline = lasso.score(Theta, y)\n",
    "\n",
    "print(f\"\\n   ✓ Model fitted successfully\")\n",
    "print(f\"   R² score: {r2_baseline:.6f}\")\n",
    "print(f\"   Number of iterations: {lasso.n_iter_}\")\n",
    "\n",
    "# ===== STEP 7: EVALUATE RESULTS =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 7: EVALUATING COEFFICIENT RECOVERY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Compute errors\n",
    "coeff_errors = np.abs(coeffs_baseline - TRUE_COEFFS)\n",
    "rmse = np.sqrt(np.mean((coeffs_baseline - TRUE_COEFFS)**2))\n",
    "mae = np.mean(coeff_errors)\n",
    "max_error = np.max(coeff_errors)\n",
    "\n",
    "print(f\"\\n📊 Coefficient Recovery Results:\")\n",
    "print(f\"\\n   {'Feature':<12} {'True':<10} {'Recovered':<12} {'Error':<10} {'Rel Error (%)':<15}\")\n",
    "print(f\"   {'-'*60}\")\n",
    "\n",
    "feature_names = ['u_xx', 'u_yy', 'u_xxxx', 'u_yyyy', 'u*u_x', 'u*u_y']\n",
    "for i, name in enumerate(feature_names):\n",
    "    true_val = TRUE_COEFFS[i]\n",
    "    rec_val = coeffs_baseline[i]\n",
    "    error = coeff_errors[i]\n",
    "    rel_error = (error / np.abs(true_val)) * 100 if true_val != 0 else np.inf\n",
    "    \n",
    "    print(f\"   {name:<12} {true_val:<10.4f} {rec_val:<12.4f} {error:<10.4f} {rel_error:<15.2f}\")\n",
    "\n",
    "print(f\"\\n   Overall Metrics:\")\n",
    "print(f\"      RMSE: {rmse:.6f}\")\n",
    "print(f\"      MAE:  {mae:.6f}\")\n",
    "print(f\"      Max Error: {max_error:.6f}\")\n",
    "print(f\"      R² Score: {r2_baseline:.6f}\")\n",
    "\n",
    "# Prediction\n",
    "y_pred = lasso.predict(Theta)\n",
    "prediction_rmse = np.sqrt(np.mean((y - y_pred)**2))\n",
    "print(f\"      Prediction RMSE: {prediction_rmse:.6f}\")\n",
    "\n",
    "# ===== STEP 8: SAVE RESULTS =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 8: SAVING RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = {\n",
    "    'method': 'baseline_no_preprocessing',\n",
    "    'subsample_factor': SUBSAMPLE_FACTOR,\n",
    "    'alpha': SINDY_ALPHA,\n",
    "    'r2_score': r2_baseline,\n",
    "    'rmse': rmse,\n",
    "    'mae': mae,\n",
    "    'max_error': max_error,\n",
    "    'prediction_rmse': prediction_rmse,\n",
    "    'coeff_u_xx': coeffs_baseline[0],\n",
    "    'coeff_u_yy': coeffs_baseline[1],\n",
    "    'coeff_u_xxxx': coeffs_baseline[2],\n",
    "    'coeff_u_yyyy': coeffs_baseline[3],\n",
    "    'coeff_u_ux': coeffs_baseline[4],\n",
    "    'coeff_u_uy': coeffs_baseline[5],\n",
    "    'error_u_xx': coeff_errors[0],\n",
    "    'error_u_yy': coeff_errors[1],\n",
    "    'error_u_xxxx': coeff_errors[2],\n",
    "    'error_u_yyyy': coeff_errors[3],\n",
    "    'error_u_ux': coeff_errors[4],\n",
    "    'error_u_uy': coeff_errors[5]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame([results])\n",
    "csv_file = os.path.join(OUTPUT_DIR, 'phase4_01_baseline_results.csv')\n",
    "results_df.to_csv(csv_file, index=False)\n",
    "print(f\"\\n   ✓ Results saved to: {csv_file}\")\n",
    "\n",
    "# Save detailed arrays\n",
    "npz_file = os.path.join(OUTPUT_DIR, 'phase4_01_baseline_data.npz')\n",
    "np.savez(npz_file,\n",
    "         images_raw=images_raw,\n",
    "         images_subsampled=images_normalized,\n",
    "         coefficients=coeffs_baseline,\n",
    "         true_coefficients=TRUE_COEFFS,\n",
    "         u_t=u_t,\n",
    "         **derivatives)\n",
    "print(f\"   ✓ Data arrays saved to: {npz_file}\")\n",
    "\n",
    "# ===== STEP 9: VISUALIZATIONS =====\n",
    "if SAVE_PLOTS:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 9: GENERATING VISUALIZATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # === FIGURE 1: Coefficient Recovery ===\n",
    "    print(\"\\n   Creating Figure 1: Coefficient recovery comparison...\")\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('Phase 4.1: Baseline Coefficient Recovery (No Preprocessing)', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i, (ax, name) in enumerate(zip(axes.flat, feature_names)):\n",
    "        x_labels = ['True', 'Recovered']\n",
    "        y_vals = [TRUE_COEFFS[i], coeffs_baseline[i]]\n",
    "        colors = ['green', 'red' if coeff_errors[i] > 0.1 else 'orange']\n",
    "        \n",
    "        bars = ax.bar(x_labels, y_vals, color=colors, alpha=0.7, edgecolor='black')\n",
    "        ax.axhline(TRUE_COEFFS[i], color='green', linestyle='--', \n",
    "                   linewidth=2, label='True Value')\n",
    "        ax.set_ylabel('Coefficient Value', fontsize=10)\n",
    "        ax.set_title(f'{name}\\nError: {coeff_errors[i]:.4f} '\n",
    "                     f'({(coeff_errors[i]/np.abs(TRUE_COEFFS[i])*100):.1f}%)',\n",
    "                     fontsize=11, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, val in zip(bars, y_vals):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{val:.4f}',\n",
    "                   ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_file = os.path.join(OUTPUT_DIR, 'phase4_01_coefficient_recovery.png')\n",
    "    plt.savefig(plot_file, dpi=150, bbox_inches='tight')\n",
    "    print(f\"   ✓ Saved: {plot_file}\")\n",
    "    plt.close()\n",
    "    \n",
    "    # === FIGURE 2: Sample Images and Derivatives ===\n",
    "    print(\"   Creating Figure 2: Sample images and derivatives...\")\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(20, 12))\n",
    "    fig.suptitle('Phase 4.1: Sample Frames and Computed Derivatives', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Show 3 sample frames\n",
    "    sample_indices = [0, Nt_sub//2, Nt_sub-1]\n",
    "    \n",
    "    for row, idx in enumerate(sample_indices):\n",
    "        # Original image\n",
    "        im0 = axes[row, 0].imshow(images_normalized[idx], cmap='gray', aspect='auto')\n",
    "        axes[row, 0].set_title(f'Frame {idx}\\nOriginal', fontsize=10)\n",
    "        axes[row, 0].axis('off')\n",
    "        plt.colorbar(im0, ax=axes[row, 0])\n",
    "        \n",
    "        # u_xx\n",
    "        im1 = axes[row, 1].imshow(u_xx[idx], cmap='RdBu_r', aspect='auto')\n",
    "        axes[row, 1].set_title(f'u_xx', fontsize=10)\n",
    "        axes[row, 1].axis('off')\n",
    "        plt.colorbar(im1, ax=axes[row, 1])\n",
    "        \n",
    "        # u_xxxx\n",
    "        im2 = axes[row, 2].imshow(u_xxxx[idx], cmap='RdBu_r', aspect='auto')\n",
    "        axes[row, 2].set_title(f'u_xxxx', fontsize=10)\n",
    "        axes[row, 2].axis('off')\n",
    "        plt.colorbar(im2, ax=axes[row, 2])\n",
    "        \n",
    "        # u*u_x (nonlinear term)\n",
    "        im3 = axes[row, 3].imshow((u*u_x)[idx], cmap='RdBu_r', aspect='auto')\n",
    "        axes[row, 3].set_title(f'u*u_x', fontsize=10)\n",
    "        axes[row, 3].axis('off')\n",
    "        plt.colorbar(im3, ax=axes[row, 3])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_file = os.path.join(OUTPUT_DIR, 'phase4_01_sample_derivatives.png')\n",
    "    plt.savefig(plot_file, dpi=150, bbox_inches='tight')\n",
    "    print(f\"   ✓ Saved: {plot_file}\")\n",
    "    plt.close()\n",
    "    \n",
    "    # === FIGURE 3: Prediction Quality ===\n",
    "    print(\"   Creating Figure 3: Prediction quality assessment...\")\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    fig.suptitle('Phase 4.1: SINDy Prediction Quality', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Scatter plot: True vs Predicted\n",
    "    axes[0].scatter(y, y_pred, alpha=0.1, s=1)\n",
    "    axes[0].plot([y.min(), y.max()], [y.min(), y.max()], \n",
    "                 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    axes[0].set_xlabel('True u_t', fontsize=11)\n",
    "    axes[0].set_ylabel('Predicted u_t', fontsize=11)\n",
    "    axes[0].set_title(f'True vs Predicted\\nR² = {r2_baseline:.4f}', fontsize=12)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residuals\n",
    "    residuals = y - y_pred\n",
    "    axes[1].hist(residuals, bins=100, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    axes[1].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[1].set_xlabel('Residual (True - Predicted)', fontsize=11)\n",
    "    axes[1].set_ylabel('Frequency', fontsize=11)\n",
    "    axes[1].set_title(f'Residual Distribution\\nRMSE = {prediction_rmse:.6f}', fontsize=12)\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Sample temporal predictions\n",
    "    sample_idx = H_sub // 2\n",
    "    sample_row = slice(sample_idx, sample_idx + 1)\n",
    "    u_t_sample = u_t[:, sample_row, :].flatten()\n",
    "    y_pred_sample = y_pred.reshape(u_t.shape)[:, sample_row, :].flatten()\n",
    "    \n",
    "    axes[2].plot(u_t_sample[:500], 'b-', linewidth=1.5, label='True u_t', alpha=0.7)\n",
    "    axes[2].plot(y_pred_sample[:500], 'r--', linewidth=1.5, label='Predicted u_t', alpha=0.7)\n",
    "    axes[2].set_xlabel('Spatial Point', fontsize=11)\n",
    "    axes[2].set_ylabel('u_t', fontsize=11)\n",
    "    axes[2].set_title('Sample Temporal Derivative', fontsize=12)\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_file = os.path.join(OUTPUT_DIR, 'phase4_01_prediction_quality.png')\n",
    "    plt.savefig(plot_file, dpi=150, bbox_inches='tight')\n",
    "    print(f\"   ✓ Saved: {plot_file}\")\n",
    "    plt.close()\n",
    "    \n",
    "    # === FIGURE 4: Summary Dashboard ===\n",
    "    print(\"   Creating Figure 4: Summary dashboard...\")\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # Coefficient comparison\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    x_pos = np.arange(len(feature_names))\n",
    "    bar_width = 0.35\n",
    "    ax1.bar(x_pos - bar_width/2, TRUE_COEFFS, bar_width, label='True', \n",
    "            color='green', alpha=0.7, edgecolor='black')\n",
    "    ax1.bar(x_pos + bar_width/2, coeffs_baseline, bar_width, label='Recovered',\n",
    "            color='red', alpha=0.7, edgecolor='black')\n",
    "    ax1.set_xlabel('Coefficient', fontsize=11)\n",
    "    ax1.set_ylabel('Value', fontsize=11)\n",
    "    ax1.set_title('Coefficient Comparison', fontsize=12, fontweight='bold')\n",
    "    ax1.set_xticks(x_pos)\n",
    "    ax1.set_xticklabels(feature_names, rotation=45, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    ax1.axhline(0, color='black', linewidth=0.5)\n",
    "    \n",
    "    # Error bars\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    colors_err = ['green' if e < 0.05 else 'orange' if e < 0.1 else 'red' \n",
    "                  for e in coeff_errors]\n",
    "    bars = ax2.barh(feature_names, coeff_errors, color=colors_err, \n",
    "                     alpha=0.7, edgecolor='black')\n",
    "    ax2.set_xlabel('Absolute Error', fontsize=11)\n",
    "    ax2.set_title('Coefficient Errors', fontsize=12, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Add error values on bars\n",
    "    for bar, err in zip(bars, coeff_errors):\n",
    "        bar_w = bar.get_width()\n",
    "        ax2.text(bar_w, bar.get_y() + bar.get_height()/2,\n",
    "                f' {err:.4f}',\n",
    "                ha='left', va='center', fontsize=9)\n",
    "    \n",
    "    # Summary text\n",
    "    ax3 = fig.add_subplot(gs[1, :])\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "PHASE 4.1: BASELINE 2D SINDY RESULTS (NO PREPROCESSING)\n",
    "{'='*80}\n",
    "\n",
    "INPUT DATA:\n",
    "  • Original images: {Nt} frames of {H} × {W} pixels\n",
    "  • Subsampled: {Nt_sub} frames of {H_sub} × {W_sub} pixels\n",
    "  • Subsample factor: {SUBSAMPLE_FACTOR}\n",
    "  • Total data points: {Nt_sub * H_sub * W_sub:,}\n",
    "\n",
    "SINDY CONFIGURATION:\n",
    "  • Regularization (α): {SINDY_ALPHA}\n",
    "  • Method: Lasso regression\n",
    "  • Library features: 6 (u_xx, u_yy, u_xxxx, u_yyyy, u*u_x, u*u_y)\n",
    "\n",
    "COEFFICIENT RECOVERY:\n",
    "  • RMSE: {rmse:.6f}\n",
    "  • MAE:  {mae:.6f}\n",
    "  • Max Error: {max_error:.6f}\n",
    "  • R² Score: {r2_baseline:.6f}\n",
    "\n",
    "PREDICTION QUALITY:\n",
    "  • Prediction RMSE: {prediction_rmse:.6f}\n",
    "  • R² Score: {r2_baseline:.6f}\n",
    "\n",
    "ASSESSMENT:\n",
    "  {'✅ GOOD' if rmse < 0.1 else '⚠️  MODERATE' if rmse < 0.3 else '❌ POOR'} - Baseline coefficient recovery\n",
    "  {'✅ GOOD' if r2_baseline > 0.8 else '⚠️  MODERATE' if r2_baseline > 0.5 else '❌ POOR'} - Model fit quality\n",
    "\n",
    "NEXT STEPS:\n",
    "  → Phase 4.2: Apply registration methods\n",
    "  → Phase 4.3: Apply denoising methods\n",
    "  → Compare improvements over baseline\n",
    "    \"\"\"\n",
    "    \n",
    "    ax3.text(0.05, 0.95, summary_text, fontsize=10, family='monospace',\n",
    "             verticalalignment='top', transform=ax3.transAxes,\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "    \n",
    "    plt.suptitle('Phase 4.1: Baseline Summary Dashboard', \n",
    "                 fontsize=16, fontweight='bold', y=0.98)\n",
    "    \n",
    "    plot_file = os.path.join(OUTPUT_DIR, 'phase4_01_summary_dashboard.png')\n",
    "    plt.savefig(plot_file, dpi=150, bbox_inches='tight')\n",
    "    print(f\"   ✓ Saved: {plot_file}\")\n",
    "    plt.close()\n",
    "\n",
    "# ===== FINAL SUMMARY =====\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 4.1 COMPLETE - BASELINE 2D SINDY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n✅ Successfully completed baseline analysis:\")\n",
    "print(f\"   • Loaded {Nt} images from ZIP file\")\n",
    "print(f\"   • Subsampled to ({Nt_sub}, {H_sub}, {W_sub})\")\n",
    "print(f\"   • Computed 2D spatial derivatives (FFT)\")\n",
    "print(f\"   • Applied SINDy with Lasso regression\")\n",
    "print(f\"   • Coefficient RMSE: {rmse:.6f}\")\n",
    "print(f\"   • Model R² score: {r2_baseline:.6f}\")\n",
    "\n",
    "print(f\"\\n📁 Output files saved to: {OUTPUT_DIR}/\")\n",
    "print(f\"   ✓ phase4_01_baseline_results.csv\")\n",
    "print(f\"   ✓ phase4_01_baseline_data.npz\")\n",
    "\n",
    "if SAVE_PLOTS:\n",
    "    print(f\"   ✓ phase4_01_coefficient_recovery.png\")\n",
    "    print(f\"   ✓ phase4_01_sample_derivatives.png\")\n",
    "    print(f\"   ✓ phase4_01_prediction_quality.png\")\n",
    "    print(f\"   ✓ phase4_01_summary_dashboard.png\")\n",
    "\n",
    "assessment = \"GOOD\" if rmse < 0.1 else \"MODERATE\" if rmse < 0.3 else \"POOR\"\n",
    "print(f\"\\n🎯 Overall Assessment: {assessment}\")\n",
    "\n",
    "if rmse < 0.1:\n",
    "    print(f\"   ✅ Baseline performance is excellent!\")\n",
    "    print(f\"   → Registration and denoising may provide marginal improvements\")\n",
    "elif rmse < 0.3:\n",
    "    print(f\"   ⚠️  Baseline performance is moderate\")\n",
    "    print(f\"   → Registration and denoising should improve results significantly\")\n",
    "else:\n",
    "    print(f\"   ❌ Baseline performance is poor\")\n",
    "    print(f\"   → Preprocessing (registration + denoising) is ESSENTIAL\")\n",
    "\n",
    "print(f\"\\n🚀 Ready for Phase 4.2: Registration Methods\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8222ab9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f59646",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}